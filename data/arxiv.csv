id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2202.08118v1,Smart contracts,2022-02-16T14:53:18Z,2022-02-16T14:53:18Z,"Smart Cities, Smart Libraries and Smart Knowledge Managers: Ushering in
  the neo-Knowledge Society","  The emergence of smart cities as a specific concept is not very old. In
simple terms, it refers to cities which are sustainable and driven
predominantly by their Information and Communication Technology (ICT)
infrastructure. Smart libraries and smart knowledge managers, alongside its
other smart component-entities, are vital for their emergence, sustenance and
progress. The paper attempts at deducing a symbiosis amongst smart cities,
smart libraries and smart knowledge managers. It further elaborates on how
these will usher in the neo-knowledge society, and the opportunities it'll
offer vis-\`a-vis Library and Information Science (LIS). Finally, it concludes
on an optimistic note, mentioning possible future research activities in this
regard.
",['Mayukh Bagchi']
http://arxiv.org/abs/2309.12344v1,Smart contracts,2023-08-25T19:23:33Z,2023-08-25T19:23:33Z,"Exploring IoT in Smart Cities: Practices, Challenges and Way Forward","  The rise of Internet of things (IoT) technology has revolutionized urban
living, offering immense potential for smart cities in which smart home, smart
infrastructure, and smart industry are essential aspects that contribute to the
development of intelligent urban ecosystems. The integration of smart home
technology raises concerns regarding data privacy and security, while smart
infrastructure implementation demands robust networking and interoperability
solutions. Simultaneously, deploying IoT in industrial settings faces
challenges related to scalability, standardization, and data management. This
research paper offers a systematic literature review of published research in
the field of IoT in smart cities including 55 relevant primary studies that
have been published in reputable journals and conferences. This extensive
literature review explores and evaluates various aspects of smart home, smart
infrastructure, and smart industry and the challenges like security and
privacy, smart sensors, interoperability and standardization. We provide a
unified perspective, as we seek to enhance the efficiency and effectiveness of
smart cities while overcoming security concerns. It then explores their
potential for collective integration and impact on the development of smart
cities. Furthermore, this study addresses the challenges associated with each
component individually and explores their combined impact on enhancing urban
efficiency and sustainability. Through a comprehensive analysis of security
concerns, this research successfully integrates these IoT components in a
unified approach, presenting a holistic framework for building smart cities of
the future. Integrating smart home, smart infrastructure, and smart industry,
this research highlights the significance of an integrated approach in
developing smart cities.
","['Kashif Ishaq', 'Syed Shah Farooq']"
http://arxiv.org/abs/1807.08165v1,Smart contracts,2018-07-21T14:57:49Z,2018-07-21T14:57:49Z,"On Computational Infraestruture Requirements to Smart and Autonomic
  Cities Framework","  Smart cities are an actual trend being pursued by research that,
fundamentally, tries to improve city's management on behalf of a better human
quality of live. This paper proposes a new autonomic complementary approach for
smart cities management. It is argued that smart city management systems with
autonomic characteristics will improve and facilitate management
functionalities in general. A framework is also presented as use case
considering specific application scenarios like smart-health, smart-grid,
smart-environment and smart-streets.
","['Romildo Bezerra', 'Flavia Nascimento', 'Joberto Martins']"
http://arxiv.org/abs/1804.01242v1,Smart contracts,2018-04-04T05:27:09Z,2018-04-04T05:27:09Z,A Smart Home Gateway Platform for Data Collection and Awareness,"  Smart homes have attracted much attention due to the expanding of
Internet-of-Things (IoT) and smart devices. In this paper, we propose a smart
gateway platform for data collection and awareness in smart home networks. A
smart gateway will replace the traditional network gateway to connect the home
network and the Internet. A smart home network supports different types of
smart devices, such as in home IoT devices, smart phones, smart electric
appliances, etc. A traditional network gateway is not capable of providing
quality-of-service measurement, user behavioral analytics, or network
optimization. Such tasks are traditionally performed with measurement agents
such as optical splitters or network probes deployed in the core network. Our
proposed platform is a lightweight plug-in for the smart gateway to accomplish
data collection, awareness and reporting. While the smart gateway is able to
adjust the control policy for data collection and awareness locally, a
cloud-based controller is also included for more refined control policy
updates. Furthermore, we propose a multi-dimensional awareness framework to
achieve accurate data awareness at the smart gateway. The efficiency of data
collection and accuracy of data awareness of the proposed platform is
demonstrated based on the tests using actual data traffic from a large number
of smart home users.
","['Pan Wang', 'Feng Ye', 'Xuejiao Chen']"
http://arxiv.org/abs/2209.10300v1,Smart contracts,2022-09-21T12:14:46Z,2022-09-21T12:14:46Z,5G-Enabled Smart Manufacturing -- A booklet by 5G-SMART,"  In this booklet the most important learnings and key results of 5G-SMART in
the area of smart manufacturing are summarized.
","['Leefke Grosjean', 'Krister Landernäs', 'Berna Sayrac', 'Ognjen Dobrijevic', 'Niels König', 'Davit Harutyunyan', 'Dhruvin Patel', 'Jose F. Monserrat', 'Joachim Sachs']"
http://arxiv.org/abs/2109.05581v1,Smart contracts,2021-09-12T18:33:24Z,2021-09-12T18:33:24Z,Data Analytics for Smart cities: Challenges and Promises,"  The explosion of advancements in artificial intelligence, sensor
technologies, and wireless communication activates ubiquitous sensing through
distributed sensors. These sensors are various domains of networks that lead us
to smart systems in healthcare, transportation, environment, and other relevant
branches/networks. Having collaborative interaction among the smart systems
connects end-user devices to each other which enables achieving a new
integrated entity called Smart Cities. The goal of this study is to provide a
comprehensive survey of data analytics in smart cities. In this paper, we aim
to focus on one of the smart cities important branches, namely Smart Mobility,
and its positive ample impact on the smart cities decision-making process.
Intelligent decision-making systems in smart mobility offer many advantages
such as saving energy, relaying city traffic, and more importantly, reducing
air pollution by offering real-time useful information and imperative
knowledge. Making a decision in smart cities in time is challenging due to
various and high dimensional factors and parameters, which are not frequently
collected. In this paper, we first address current challenges in smart cities
and provide an overview of potential solutions to these challenges. Then, we
offer a framework of these solutions, called universal smart cities decision
making, with three main sections of data capturing, data analysis, and decision
making to optimize the smart mobility within smart cities. With this framework,
we elaborate on fundamental concepts of big data, machine learning, and deep
leaning algorithms that have been applied to smart cities and discuss the role
of these algorithms in decision making for smart mobility in smart cities.
","['Farid Ghareh Mohammadi', 'Farzan Shenavarmasouleh', 'M. Hadi Amini', 'Hamid R. Arabnia']"
http://arxiv.org/abs/1711.09184v1,Smart contracts,2017-11-25T04:06:24Z,2017-11-25T04:06:24Z,A Formal Specification Framework for Smart Grid Components,"  Smart grid can be considered as the next step in the evolution of power
systems. It comprises of different entities and objects ranging from smart
appliances, smart meters, generators, smart storages, and more. One key problem
in modeling smart grid is that while currently there is a considerable focus on
the practical aspects of smart grid, there are very few modeling attempts and
even lesser attempts at formalization. To the best of our knowledge, among
other formal methods, formal specification has previously not been applied in
the domain of smart grid. In this paper, we attempt to bridge this gap by
presenting a novel approach to modeling smart grid components using a formal
specification approach. We use a state-based formal specification language
namely Z (pronounced as `Zed') since we believe Z is particularly suited for
modeling smart grid components.We demonstrate the application of Z on key smart
grid components. The presented formal specification can be considered as first
steps towards modeling of smart grid using a Software Engineering formalism. It
also demonstrates how complex systems, such as the smart grid, can be modeled
elegantly using formal specification.
","['Waseem Akram', 'Muaz A. Niazi']"
http://arxiv.org/abs/1909.02914v1,Smart contracts,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2405.06930v1,Smart contracts,2024-05-11T06:26:07Z,2024-05-11T06:26:07Z,"Extended Reality for Smart Built Environments Design: Smart Lighting
  Design Testbed","  Smart Built Environment is an eco-system of `connected' and `smart' Internet
of Things (IoT) devices that are embedded in a built environment. Smart
lighting is an important category of smart IoT devices that has recently
attracted research interest, particularly for residential areas. In this paper,
we present an extended reality based smart lighting design testbed that can
generate design prototypes based on the functionality of the physical
environment. The emphasis is on designing a smart lighting system in a
controlled residential environment, with some evaluation of well-being and
comfort.
","['Elham Mohammadrezaei', 'Denis Gracanin']"
http://arxiv.org/abs/1610.06855v1,Smart contracts,2016-10-21T16:50:11Z,2016-10-21T16:50:11Z,Big Data: Perspektiven fuer Smart Grids und Smart Buildings,"  This paper gives a short survey of recent trends in the emerging field of big
data. It explains the definitions and useful methods. In addition, application
fields of smart buildings and smart grids are discussed.
",['Ralf Mikut']
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2206.02760v1,Smart contracts,2022-06-06T17:37:51Z,2022-06-06T17:37:51Z,Blockchain for the Cybersecurity of Smart City Applications,"  Cybersecurity is an inherent characteristic that should be addressed before
the large deployment of smart city applications. Recently, Blockchain appears
as a promising technology to provide several cybersecurity aspects of smart
city applications. This paper provides a comprehensive review of the existing
blockchain-based solutions for the cybersecurity of the main smart city
applications, namely smart healthcare, smart transportation, smart agriculture,
supply chain management, smart grid, and smart homes. We describe the existing
solutions and we discuss their merits and limits. Moreover, we define the
security requirements of each smart city application and we give a mapping of
the studied solutions to these defined requirements. Additionally, future
directions are given. We believe that the present survey is a good starting
point for every researcher in the fields of cybersecurity, blockchain, and
smart cities.
","['Omar Cheikhrouhou', 'Ichrak Amdouni', 'Khaleel Mershad', 'Maryem Ammi', 'Tuan Nguyen Gia']"
http://arxiv.org/abs/2207.04424v1,Smart contracts,2022-07-10T09:22:10Z,2022-07-10T09:22:10Z,"An Overview of Cyber Threats, Attacks, and Countermeasures on the
  Primary Domains of Smart Cities","  A smart city is a place where existing facilities and services are enhanced
by digital technology to benefit people and companies. The most critical
infrastructures in this city are interconnected. Increased data exchange across
municipal domains aims to manage the essential assets, leading to more
automation in city governance and optimization of the dynamic offered services.
However, no clear guideline or standard exists for modeling these data flows.
As a result, operators, municipalities, policymakers, manufac-turers, solution
providers, and vendors are forced to accept systems with limited scalability
and varying needs. Nonetheless, it is critical to raise awareness about smart
city cybersecurity and implement suitable measures to safeguard citizens'
privacy and security because the cyber threats seem to be well-organized,
diverse, and sophisticated. This study aims to present an overview of cyber
threats, attacks, and countermeasures on the primary domains of smart cities
(smart government, smart mobility, smart environment, smart living, smart
healthcare, smart economy, and smart people) to present information extracted
from state-of-the-art to policymakers to perceive the critical situation and,
at the same time, to be a valuable resource for the scientific community.
","['Vasiliki Demertzi', 'Stavros Demertzis', 'Konstantinos Demertzis']"
http://arxiv.org/abs/2402.00568v1,Smart contracts,2024-02-01T13:01:47Z,2024-02-01T13:01:47Z,Secure Supervised Learning-Based Smart Home Authentication Framework,"  The Smart home possesses the capability of facilitating home services to
their users with the systematic advance in The Internet of Things (IoT) and
information and communication technologies (ICT) in recent decades. The home
service offered by the smart devices helps the users in utilize maximized level
of comfort for the objective of improving life quality. As the user and smart
devices communicate through an insecure channel, the smart home environment is
prone to security and privacy problems. A secure authentication protocol needs
to be established between the smart devices and the user, such that a situation
for device authentication can be made feasible in smart home environments. Most
of the existing smart home authentication protocols were identified to fail in
facilitating a secure mutual authentication and increases the possibility of
lunching the attacks of session key disclosure, impersonation and stolen smart
device. In this paper, Secure Supervised Learning-based Smart Home
Authentication Framework (SSL-SHAF) is proposed as are liable mutual
authentication that can be contextually imposed for better security. The formal
analysis of the proposed SSL-SHAF confirmed better resistance against session
key disclosure, impersonation and stolen smart device attacks. The results of
SSL-SHAF confirmed minimized computational costs and security compared to the
baseline protocols considered for investigation.
","['K. Swapna Sudha', 'N. Jeyanthi', 'Celestine Iwendi']"
http://arxiv.org/abs/1112.1158v1,Smart contracts,2011-12-06T04:55:32Z,2011-12-06T04:55:32Z,"Wireless Communications and Networking Technologies for Smart Grid:
  Paradigms and Challenges","  Smart grid, regarded as the next generation power grid, uses two-way flows of
electricity and information to create a widely distributed automated energy
delivery network. In this work we present our vision on smart grid from the
perspective of wireless communications and networking technologies. We present
wireless communication and networking paradigms for four typical scenarios in
the future smart grid and also point out the research challenges of the
wireless communication and networking technologies used in smart grid
","['Xi Fang', 'Dejun Yang', 'Guoliang Xue']"
http://arxiv.org/abs/1807.03111v1,Smart contracts,2018-06-13T10:03:44Z,2018-06-13T10:03:44Z,"A Framework for Detecting and Translating User Behavior from Smart Meter
  Data","  The European adoption of smart electricity meters triggers the developments
of new value-added service for smart energy and optimal consumption. Recently,
several algorithms and tools have been built to analyze smart meter's data.
This paper introduces an open framework and prototypes for detecting and
presenting user behavior from its smart meter power consumption data. The
framework aims at presenting the detected user behavior in natural language
reports. In order to validate the proposed framework, an experiment has been
performed and the results have been presented.
","['Egon Kidmose', 'Emad Ebeid', 'Rune Hylsberg Jacobsen']"
http://arxiv.org/abs/2101.06519v1,Smart contracts,2021-01-16T20:46:43Z,2021-01-16T20:46:43Z,"Intrusion Detection Systems for Smart Home IoT Devices: Experimental
  Comparison Study","  Smart homes are one of the most promising applications of the emerging
Internet of Things (IoT) technology. With the growing number of IoT related
devices such as smart thermostats, smart fridges, smart speaker, smart light
bulbs and smart locks, smart homes promise to make our lives easier and more
comfortable. However, the increased deployment of such smart devices brings an
increase in potential security risks and home privacy breaches. In order to
overcome such risks, Intrusion Detection Systems are presented as pertinent
tools that can provide network-level protection for smart devices deployed in
home environments. These systems monitor the network activities of the smart
home-connected de-vices and focus on alerting suspicious or malicious activity.
They also can deal with detected abnormal activities by hindering the impostors
in accessing the victim devices. However, the employment of such systems in the
context of a smart home can be challenging due to the devices hardware
limitations, which may restrict their ability to counter the existing and
emerging attack vectors. Therefore, this paper proposes an experimental
comparison between the widely used open-source NIDSs namely Snort, Suricata and
Bro IDS to find the most appropriate one for smart homes in term of detection
accuracy and resources consumption including CP and memory utilization.
Experimental Results show that Suricata is the best performing NIDS for smart
homes
","['Faisal Alsakran', 'Gueltoum Bendiab', 'Stavros Shiaeles', 'Nicholas Kolokotronis']"
http://arxiv.org/abs/2304.12851v1,Smart contracts,2023-04-25T14:22:48Z,2023-04-25T14:22:48Z,Towards Smart Education through the Internet of Things: A Review,"  IoT is a fundamental enabling technology for creating smart spaces, which can
assist the effective face-to-face and online education systems. The transition
to smart education (integrating IoT and AI into the education system) is
appealing, which has a concrete impact on learners' engagement, motivation,
attendance, and deep learning. Traditional education faces many challenges,
including administration, pedagogy, assessment, and classroom supervision.
Recent developments in ICT (e.g., IoT, AI and 5G, etc.) have yielded lots of
smart solutions for various aspects of life; however, smart solutions are not
well integrated into the education system. In particular, the COVID-19 pandemic
situation had further emphasized the adoption of new smart solutions in
education. This study reviews the related studies and addresses the (i)
problems in the traditional education system with possible solutions, (ii) the
transition towards smart education, and (iii) research challenges in the
transition to smart education (i.e, computational and social resistance).
Considering these studies, smart solutions (e.g., smart pedagogy, smart
assessment, smart classroom, smart administration, etc.) are introduced to the
problems of the traditional system. This exploratory study opens new trends for
scholars and the market to integrate ICT, IoT, and AI into smart education.
","['Afzal Badshah', 'Anwar Ghani', 'Ali Daud', 'Ateeqa Jalal', 'Muhammad Bilal', 'Jon Crowcroft']"
http://arxiv.org/abs/1109.4474v1,Smart contracts,2011-09-21T04:33:49Z,2011-09-21T04:33:49Z,Smart Grid Information Security (IS) Functional Requirement,"  It is important to implement safe smart grid environment to enhance people's
lives and livelihoods. This paper provides information on smart grid IS
functional requirement by illustrating some discussion points to the sixteen
identified requirements. This paper introduces the smart grid potential hazards
that can be referred as a triggering factor to improve the system and security
of the entire grid. The background of smart information infrastructure and the
needs for smart grid IS is described with the adoption of hermeneutic circle as
methodology. Grid information technology and security-s session discusses that
grid provides the chance of a simple and transparent access to different
information sources. In addition, the transformation between traditional versus
smart grid networking trend and the IS importance on the communication field
reflects the criticality of grid IS functional requirement identification is
introduces. The smart grid IS functional requirements described in this paper
are general and can be adopted or modified to suit any smart grid system. This
paper has tutorial contents where some related backgrounds were provided,
especially for networking community, covering the cyber security requirement of
smart grid information infrastructure.
","['Amy Poh Ai Ling', 'Mukaidono Masao']"
http://arxiv.org/abs/1706.07363v1,Smart contracts,2017-06-22T15:19:16Z,2017-06-22T15:19:16Z,Smart Wireless Communication is the Cornerstone of Smart Infrastructures,"  Emerging smart infrastructures, such as Smart City, Smart Grid, Smart Health,
and Smart Transportation, need smart wireless connectivity. However, the
requirements of these smart infrastructures cannot be met with today's wireless
networks. A new wireless infrastructure is needed to meet unprecedented needs
in terms of agility, reliability, security, scalability, and partnerships.
  We are at the beginning of a revolution in how we live with technology,
resulting from a convergence of machine learning (ML), the Internet-of-Things
(IoT), and robotics. A smart infrastructure monitors and processes a vast
amount of data, collected from a dense and wide distribution of heterogeneous
sensors (e.g., the IoT), as well as from web applications like social media. In
real time, using machine learning, patterns and relationships in the data over
space, time, and application can be detected and predictions can be made; on
the basis of these, resources can be managed, decisions can be made, and
devices can be actuated to optimize metrics, such as cost, health, safety, and
convenience.
","['Mary Ann Weitnauer', 'Jennifer Rexford', 'Nicholas Laneman', 'Matthieu Bloch', 'Santiago Griljava', 'Catherine Ross', 'Gee-Kung Chang']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2409.18125v3,3D printing,2024-09-26T17:59:11Z,2025-04-27T06:50:23Z,"LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with
  3D-awareness","  Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced
their proficiency in 2D visual understanding tasks, enabling them to
effectively process and understand images and videos. However, the development
of LMMs with 3D scene understanding capabilities has been hindered by the lack
of large-scale 3D vision-language datasets and powerful 3D encoders. In this
paper, we introduce a simple yet effective framework called LLaVA-3D.
Leveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D
efficiently adapts LLaVA for 3D scene understanding without compromising 2D
understanding capabilities. To achieve this, we utilize the 3D position
embeddings to enhance the 2D CLIP Patches with 3D spatial context information
and construct 3D patches. By integrating the 3D position embeddings into 2D
LMMs and employing joint 2D and 3D vision-language instruction tuning, we
establish a unified architecture for both 2D visual understanding and 3D scene
understanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding
accurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from
these 3D patches, without relying on the time-consuming off-the-shelf 3D
segmentors. Experimental results show that LLaVA-3D converges 3.5x faster than
existing 3D LMMs when trained on 3D vision-language datasets. Moreover,
LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks
but also maintains comparable 2D visual understanding and vision-language
conversation capabilities with LLaVA.
","['Chenming Zhu', 'Tai Wang', 'Wenwei Zhang', 'Jiangmiao Pang', 'Xihui Liu']"
http://arxiv.org/abs/2303.15780v1,3D printing,2023-03-28T07:50:45Z,2023-03-28T07:50:45Z,Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion,"  We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our
method is designed for a novel task, which is to convert a given 3D scene to
another scene according to text instructions. Instruct 3D-to-3D applies
pretrained Image-to-Image diffusion models for 3D-to-3D conversion. This
enables the likelihood maximization of each viewpoint image and high-quality 3D
generation. In addition, our proposed method explicitly inputs the source 3D
scene as a condition, which enhances 3D consistency and controllability of how
much of the source 3D scene structure is reflected. We also propose dynamic
scaling, which allows the intensity of the geometry transformation to be
adjusted. We performed quantitative and qualitative evaluations and showed that
our proposed method achieves higher quality 3D-to-3D conversions than baseline
methods.
","['Hiromichi Kamata', 'Yuiko Sakuma', 'Akio Hayakawa', 'Masato Ishii', 'Takuya Narihira']"
http://arxiv.org/abs/2401.13923v2,3D printing,2024-01-25T03:42:00Z,2024-03-17T08:51:45Z,Towards 3D Molecule-Text Interpretation in Language Models,"  Language Models (LMs) have greatly influenced diverse domains. However, their
inherent limitation in comprehending 3D molecular structures has considerably
constrained their potential in the biomolecular domain. To bridge this gap, we
focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular
Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze
3D molecules by equipping the LM with a 3D molecular encoder. This integration
is achieved by a 3D molecule-text projector, bridging the 3D molecular
encoder's representation space and the LM's input space. Moreover, to enhance
3D-MoLM's ability of cross-modal molecular understanding and instruction
following, we meticulously curated a 3D molecule-centric instruction tuning
dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric
instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder
and LM. It significantly surpasses existing baselines on downstream tasks,
including molecule-text retrieval, molecule captioning, and more challenging
open-text molecular QA tasks, especially focusing on 3D-dependent properties.
We release our codes and datasets at https://github.com/lsh0520/3D-MoLM.
","['Sihang Li', 'Zhiyuan Liu', 'Yanchen Luo', 'Xiang Wang', 'Xiangnan He', 'Kenji Kawaguchi', 'Tat-Seng Chua', 'Qi Tian']"
http://arxiv.org/abs/2503.24229v1,3D printing,2025-03-31T15:42:10Z,2025-03-31T15:42:10Z,"Pre-training with 3D Synthetic Data: Learning 3D Point Cloud Instance
  Segmentation from 3D Synthetic Scenes","  In the recent years, the research community has witnessed growing use of 3D
point cloud data for the high applicability in various real-world applications.
By means of 3D point cloud, this modality enables to consider the actual size
and spatial understanding. The applied fields include mechanical control of
robots, vehicles, or other real-world systems. Along this line, we would like
to improve 3D point cloud instance segmentation which has emerged as a
particularly promising approach for these applications. However, the creation
of 3D point cloud datasets entails enormous costs compared to 2D image
datasets. To train a model of 3D point cloud instance segmentation, it is
necessary not only to assign categories but also to provide detailed
annotations for each point in the large-scale 3D space. Meanwhile, the increase
of recent proposals for generative models in 3D domain has spurred proposals
for using a generative model to create 3D point cloud data. In this work, we
propose a pre-training with 3D synthetic data to train a 3D point cloud
instance segmentation model based on generative model for 3D scenes represented
by point cloud data. We directly generate 3D point cloud data with Point-E for
inserting a generated data into a 3D scene. More recently in 2025, although
there are other accurate 3D generation models, even using the Point-E as an
early 3D generative model can effectively support the pre-training with 3D
synthetic data. In the experimental section, we compare our pre-training method
with baseline methods indicated improved performance, demonstrating the
efficacy of 3D generative models for 3D point cloud instance segmentation.
","['Daichi Otsuka', 'Shinichi Mae', 'Ryosuke Yamada', 'Hirokatsu Kataoka']"
http://arxiv.org/abs/2009.09633v1,3D printing,2020-09-21T06:26:39Z,2020-09-21T06:26:39Z,3D-FUTURE: 3D Furniture shape with TextURE,"  The 3D CAD shapes in current 3D benchmarks are mostly collected from online
model repositories. Thus, they typically have insufficient geometric details
and less informative textures, making them less attractive for comprehensive
and subtle research in areas such as high-quality 3D mesh and texture recovery.
This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a
richly-annotated and large-scale repository of 3D furniture shapes in the
household scenario. At the time of this technical report, 3D-FUTURE contains
20,240 clean and realistic synthetic images of 5,000 different rooms. There are
9,992 unique detailed 3D instances of furniture with high-resolution textures.
Experienced designers developed the room scenes, and the 3D CAD shapes in the
scene are used for industrial production. Given the well-organized 3D-FUTURE,
we provide baseline experiments on several widely studied tasks, such as joint
2D instance segmentation and 3D object pose estimation, image-based 3D shape
retrieval, 3D object reconstruction from a single image, and texture recovery
for 3D shapes, to facilitate related future researches on our database.
","['Huan Fu', 'Rongfei Jia', 'Lin Gao', 'Mingming Gong', 'Binqiang Zhao', 'Steve Maybank', 'Dacheng Tao']"
http://arxiv.org/abs/2401.04099v1,3D printing,2024-01-08T18:56:33Z,2024-01-08T18:56:33Z,AGG: Amortized Generative 3D Gaussians for Single Image to 3D,"  Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/
","['Dejia Xu', 'Ye Yuan', 'Morteza Mardani', 'Sifei Liu', 'Jiaming Song', 'Zhangyang Wang', 'Arash Vahdat']"
http://arxiv.org/abs/2303.09036v2,3D printing,2023-03-16T02:18:41Z,2023-08-07T08:10:55Z,Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation,"  Generating images with both photorealism and multiview 3D consistency is
crucial for 3D-aware GANs, yet existing methods struggle to achieve them
simultaneously. Improving the photorealism via CNN-based 2D super-resolution
can break the strict 3D consistency, while keeping the 3D consistency by
learning high-resolution 3D representations for direct rendering often
compromises image quality. In this paper, we propose a novel learning strategy,
namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate
high-quality images while maintaining their strict 3D consistency, by letting
the images synthesized by the generator's 3D rendering branch to mimic those
generated by its 2D super-resolution branch. We also introduce 3D-aware
convolutions into the generator for better 3D representation learning, which
further improves the image generation quality. With the above strategies, our
method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats,
respectively, at 512x512 resolution, largely outperforming existing 3D-aware
GANs using direct 3D rendering and coming very close to the previous
state-of-the-art method that leverages 2D super-resolution. Project website:
https://seanchenxy.github.io/Mimic3DWeb.
","['Xingyu Chen', 'Yu Deng', 'Baoyuan Wang']"
http://arxiv.org/abs/2403.12438v1,3D printing,2024-03-19T04:51:38Z,2024-03-19T04:51:38Z,Precise-Physics Driven Text-to-3D Generation,"  Text-to-3D generation has shown great promise in generating novel 3D content
based on given text prompts. However, existing generative methods mostly focus
on geometric or visual plausibility while ignoring precise physics perception
for the generated 3D shapes. This greatly hinders the practicality of generated
3D shapes in real-world applications. In this work, we propose Phy3DGen, a
precise-physics-driven text-to-3D generation method. By analyzing the solid
mechanics of generated 3D shapes, we reveal that the 3D shapes generated by
existing text-to-3D generation methods are impractical for real-world
applications as the generated 3D shapes do not conform to the laws of physics.
To this end, we leverage 3D diffusion models to provide 3D shape priors and
design a data-driven differentiable physics layer to optimize 3D shape priors
with solid mechanics. This allows us to optimize geometry efficiently and learn
precise physics information about 3D shapes at the same time. Experimental
results demonstrate that our method can consider both geometric plausibility
and precise physics perception, further bridging 3D virtual modeling and
precise physical worlds.
","['Qingshan Xu', 'Jiao Liu', 'Melvin Wong', 'Caishun Chen', 'Yew-Soon Ong']"
http://arxiv.org/abs/2010.11504v1,3D printing,2020-10-22T07:45:09Z,2020-10-22T07:45:09Z,3D Meta-Registration: Learning to Learn Registration of 3D Point Clouds,"  Deep learning-based point cloud registration models are often generalized
from extensive training over a large volume of data to learn the ability to
predict the desired geometric transformation to register 3D point clouds. In
this paper, we propose a meta-learning based 3D registration model, named 3D
Meta-Registration, that is capable of rapidly adapting and well generalizing to
new 3D registration tasks for unseen 3D point clouds. Our 3D Meta-Registration
gains a competitive advantage by training over a variety of 3D registration
tasks, which leads to an optimized model for the best performance on the
distribution of registration tasks including potentially unseen tasks.
Specifically, the proposed 3D Meta-Registration model consists of two modules:
3D registration learner and 3D registration meta-learner. During the training,
the 3D registration learner is trained to complete a specific registration task
aiming to determine the desired geometric transformation that aligns the source
point cloud with the target one. In the meantime, the 3D registration
meta-learner is trained to provide the optimal parameters to update the 3D
registration learner based on the learned task distribution. After training,
the 3D registration meta-learner, which is learned with the optimized coverage
of distribution of 3D registration tasks, is able to dynamically update 3D
registration learners with desired parameters to rapidly adapt to new
registration tasks. We tested our model on synthesized dataset ModelNet and
FlyingThings3D, as well as real-world dataset KITTI. Experimental results
demonstrate that 3D Meta-Registration achieves superior performance over other
previous techniques (e.g. FlowNet3D).
","['Lingjing Wang', 'Yu Hao', 'Xiang Li', 'Yi Fang']"
http://arxiv.org/abs/2502.08503v1,3D printing,2025-02-12T15:34:45Z,2025-02-12T15:34:45Z,Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?,"  In this work, we identify the ""2D-Cheating"" problem in 3D LLM evaluation,
where these tasks might be easily solved by VLMs with rendered images of point
clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We
test VLM performance across multiple 3D LLM benchmarks and, using this as a
reference, propose principles for better assessing genuine 3D understanding. We
also advocate explicitly separating 3D abilities from 1D or 2D aspects when
evaluating 3D LLMs.
","['Jiahe Jin', 'Yanheng He', 'Mingyan Yang']"
http://arxiv.org/abs/1901.08373v2,3D printing,2019-01-24T12:11:05Z,2019-09-14T08:12:30Z,"Three-dimensional Backbone Network for 3D Object Detection in Traffic
  Scenes","  The task of detecting 3D objects in traffic scenes has a pivotal role in many
real-world applications. However, the performance of 3D object detection is
lower than that of 2D object detection due to the lack of powerful 3D feature
extraction methods. To address this issue, this study proposes a 3D backbone
network to acquire comprehensive 3D feature maps for 3D object detection. It
primarily consists of sparse 3D convolutional neural network operations in the
point cloud. The 3D backbone network can inherently learn 3D features from the
raw data without compressing the point cloud into multiple 2D images. The
sparse 3D convolutional neural network takes full advantage of the sparsity in
the 3D point cloud to accelerate computation and save memory, which makes the
3D backbone network feasible in a real-world application. Empirical experiments
were conducted on the KITTI benchmark and comparable results were obtained with
respect to the state-of-the-art performance for 3D object detection.
","['Xuesong Li', 'Jose Guivant', 'Ngaiming Kwok', 'Yongzhi Xu', 'Ruowei Li', 'Hongkun Wu']"
http://arxiv.org/abs/2011.11534v4,3D printing,2020-11-23T16:48:35Z,2022-04-19T05:59:10Z,Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation,"  Whole-body 3D human mesh estimation aims to reconstruct the 3D human body,
hands, and face simultaneously. Although several methods have been proposed,
accurate prediction of 3D hands, which consist of 3D wrist and fingers, still
remains challenging due to two reasons. First, the human kinematic chain has
not been carefully considered when predicting the 3D wrists. Second, previous
works utilize body features for the 3D fingers, where the body feature barely
contains finger information. To resolve the limitations, we present Hand4Whole,
which has two strong points over previous works. First, we design Pose2Pose, a
module that utilizes joint features for 3D joint rotations. Using Pose2Pose,
Hand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints
largely contribute to 3D wrist rotations in the human kinematic chain. Second,
Hand4Whole discards the body feature when predicting 3D finger rotations. Our
Hand4Whole is trained in an end-to-end manner and produces much better 3D hand
results than previous whole-body 3D human mesh estimation methods. The codes
are available here at https://github.com/mks0601/Hand4Whole_RELEASE.
","['Gyeongsik Moon', 'Hongsuk Choi', 'Kyoung Mu Lee']"
http://arxiv.org/abs/2111.03098v1,3D printing,2021-11-04T18:30:37Z,2021-11-04T18:30:37Z,"Voxel-based 3D Detection and Reconstruction of Multiple Objects from a
  Single Image","  Inferring 3D locations and shapes of multiple objects from a single 2D image
is a long-standing objective of computer vision. Most of the existing works
either predict one of these 3D properties or focus on solving both for a single
object. One fundamental challenge lies in how to learn an effective
representation of the image that is well-suited for 3D detection and
reconstruction. In this work, we propose to learn a regular grid of 3D voxel
features from the input image which is aligned with 3D scene space via a 3D
feature lifting operator. Based on the 3D voxel features, our novel
CenterNet-3D detection head formulates the 3D detection as keypoint detection
in the 3D space. Moreover, we devise an efficient coarse-to-fine reconstruction
module, including coarse-level voxelization and a novel local PCA-SDF shape
representation, which enables fine detail reconstruction and one order of
magnitude faster inference than prior methods. With complementary supervision
from both 3D detection and reconstruction, one enables the 3D voxel features to
be geometry and context preserving, benefiting both tasks.The effectiveness of
our approach is demonstrated through 3D detection and reconstruction in single
object and multiple object scenarios.
","['Feng Liu', 'Xiaoming Liu']"
http://arxiv.org/abs/2206.09221v1,3D printing,2022-06-18T15:21:24Z,2022-06-18T15:21:24Z,"3D Face Parsing via Surface Parameterization and 2D Semantic
  Segmentation Network","  Face parsing assigns pixel-wise semantic labels as the face representation
for computers, which is the fundamental part of many advanced face
technologies. Compared with 2D face parsing, 3D face parsing shows more
potential to achieve better performance and further application, but it is
still challenging due to 3D mesh data computation. Recent works introduced
different methods for 3D surface segmentation, while the performance is still
limited. In this paper, we propose a method based on the ""3D-2D-3D"" strategy to
accomplish 3D face parsing. The topological disk-like 2D face image containing
spatial and textural information is transformed from the sampled 3D face data
through the face parameterization algorithm, and a specific 2D network called
CPFNet is proposed to achieve the semantic segmentation of the 2D parameterized
face data with multi-scale technologies and feature aggregation. The 2D
semantic result is then inversely re-mapped to 3D face data, which finally
achieves the 3D face parsing. Experimental results show that both CPFNet and
the ""3D-2D-3D"" strategy accomplish high-quality 3D face parsing and outperform
state-of-the-art 2D networks as well as 3D methods in both qualitative and
quantitative comparisons.
","['Wenyuan Sun', 'Ping Zhou', 'Yangang Wang', 'Zongpu Yu', 'Jing Jin', 'Guangquan Zhou']"
http://arxiv.org/abs/2211.14091v2,3D printing,2022-11-25T13:21:59Z,2022-12-11T03:35:39Z,Language-Assisted 3D Feature Learning for Semantic Scene Understanding,"  Learning descriptive 3D features is crucial for understanding 3D scenes with
diverse objects and complex structures. However, it is usually unknown whether
important geometric attributes and scene context obtain enough emphasis in an
end-to-end trained 3D scene understanding network. To guide 3D feature learning
toward important geometric attributes and scene context, we explore the help of
textual scene descriptions. Given some free-form descriptions paired with 3D
scenes, we extract the knowledge regarding the object relationships and object
attributes. We then inject the knowledge to 3D feature learning through three
classification-based auxiliary tasks. This language-assisted training can be
combined with modern object detection and instance segmentation methods to
promote 3D semantic scene understanding, especially in a label-deficient
regime. Moreover, the 3D feature learned with language assistance is better
aligned with the language features, which can benefit various 3D-language
multimodal tasks. Experiments on several benchmarks of 3D-only and 3D-language
tasks demonstrate the effectiveness of our language-assisted 3D feature
learning. Code is available at
https://github.com/Asterisci/Language-Assisted-3D.
","['Junbo Zhang', 'Guofan Fan', 'Guanghan Wang', 'Zhengyuan Su', 'Kaisheng Ma', 'Li Yi']"
http://arxiv.org/abs/2308.08769v1,3D printing,2023-08-17T03:52:15Z,2023-08-17T03:52:15Z,"Chat-3D: Data-efficiently Tuning Large Language Model for Universal
  Dialogue of 3D Scenes","  3D scene understanding has gained significant attention due to its wide range
of applications. However, existing methods for 3D scene understanding are
limited to specific downstream tasks, which hinders their practicality in
real-world applications. This paper presents Chat-3D, which combines the 3D
visual perceptual ability of pre-trained 3D representations and the impressive
reasoning and conversation capabilities of advanced LLMs to achieve the first
universal dialogue systems for 3D scenes. Specifically, we align 3D
representations into the feature space of LLMs, thus enabling LLMs to perceive
the 3D world. Given the scarcity of 3D scene-text data, we propose a
three-stage training strategy to efficiently utilize the available data for
better alignment. To enhance the reasoning ability and develop a user-friendly
interaction scheme, we further construct a high-quality object-centric 3D
instruction dataset and design an associated object-centric prompt. Our
experiments show that Chat-3D achieves an impressive ability to comprehend
diverse instructions for 3D scenes, engage in intricate spatial reasoning, and
incorporate external knowledge into its responses. Chat-3D achieves a 75.6%
relative score compared with GPT-4 on the constructed instruction dataset.
","['Zehan Wang', 'Haifeng Huang', 'Yang Zhao', 'Ziang Zhang', 'Zhou Zhao']"
http://arxiv.org/abs/2403.15383v2,3D printing,2024-03-22T17:59:01Z,2024-05-15T06:56:30Z,ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars,"  Real-world applications often require a large gallery of 3D assets that share
a consistent theme. While remarkable advances have been made in general 3D
content creation from text or image, synthesizing customized 3D assets
following the shared theme of input 3D exemplars remains an open and
challenging problem. In this work, we present ThemeStation, a novel approach
for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D
assets based on given few exemplars with two goals: 1) unity for generating 3D
assets that thematically align with the given exemplars and 2) diversity for
generating 3D assets with a high degree of variations. To this end, we design a
two-stage framework that draws a concept image first, followed by a
reference-informed 3D modeling stage. We propose a novel dual score
distillation (DSD) loss to jointly leverage priors from both the input
exemplars and the synthesized concept image. Extensive experiments and user
studies confirm that ThemeStation surpasses prior works in producing diverse
theme-aware 3D models with impressive quality. ThemeStation also enables
various applications such as controllable 3D-to-3D generation.
","['Zhenwei Wang', 'Tengfei Wang', 'Gerhard Hancke', 'Ziwei Liu', 'Rynson W. H. Lau']"
http://arxiv.org/abs/2406.05132v3,3D printing,2024-06-07T17:59:59Z,2025-03-20T23:06:14Z,"3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and
  Less Hallucination","  The integration of language and 3D perception is crucial for embodied agents
and robots that comprehend and interact with the physical world. While large
language models (LLMs) have demonstrated impressive language understanding and
generation capabilities, their adaptation to 3D environments (3D-LLMs) remains
in its early stages. A primary challenge is a lack of large-scale datasets with
dense grounding between language and 3D scenes. We introduce 3D-GRAND, a
pioneering large-scale dataset comprising 40,087 household scenes paired with
6.2 million densely-grounded scene-language instructions. Our results show that
instruction tuning with 3D-GRAND significantly enhances grounding capabilities
and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose
a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in
3D-LLMs, enabling fair comparisons of models. Our experiments highlight a
scaling effect between dataset size and 3D-LLM performance, emphasizing the
importance of large-scale 3D-text datasets for embodied AI research. Our
results demonstrate early signals for effective sim-to-real transfer,
indicating that models trained on large synthetic data can perform well on
real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied
AI community with resources and insights to lead to more reliable and
better-grounded 3D-LLMs. Project website: https://3d-grand.github.io
","['Jianing Yang', 'Xuweiyi Chen', 'Nikhil Madaan', 'Madhavan Iyengar', 'Shengyi Qian', 'David F. Fouhey', 'Joyce Chai']"
http://arxiv.org/abs/1802.10271v1,3D printing,2018-02-28T06:02:55Z,2018-02-28T06:02:55Z,"Multimodal Sensor-Based Semantic 3D Mapping for a Large-Scale
  Environment","  Semantic 3D mapping is one of the most important fields in robotics, and has
been used in many applications, such as robot navigation, surveillance, and
virtual reality. In general, semantic 3D mapping is mainly composed of 3D
reconstruction and semantic segmentation. As these technologies evolve, there
has been great progress in semantic 3D mapping in recent years. Furthermore,
the number of robotic applications requiring semantic information in 3D mapping
to perform high-level tasks has increased, and many studies on semantic 3D
mapping have been published. Existing methods use a camera for both 3D
reconstruction and semantic segmentation. However, this is not suitable for
large-scale environments and has the disadvantage of high computational
complexity. To address this problem, we propose a multimodal sensor-based
semantic 3D mapping system using a 3D Lidar combined with a camera. In this
study, we build a 3D map by estimating odometry based on a global positioning
system (GPS) and an inertial measurement unit (IMU), and use the latest 2D
convolutional neural network (CNN) for semantic segmentation. To build a
semantic 3D map, we integrate the 3D map with semantic information by using
coordinate transformation and Bayes' update scheme. In order to improve the
semantic 3D map, we propose a 3D refinement process to correct wrongly
segmented voxels and remove traces of moving vehicles in the 3D map. Through
experiments on challenging sequences, we demonstrate that our method
outperforms state-of-the-art methods in terms of accuracy and intersection over
union (IoU). Thus, our method can be used for various applications that require
semantic information in 3D map.
","['Jongmin Jeong', 'Tae Sung Yoon', 'Jin Bae Park']"
http://arxiv.org/abs/2209.10020v2,3D printing,2022-09-20T22:04:31Z,2024-02-18T11:59:16Z,Towards 3D VR-Sketch to 3D Shape Retrieval,"  Growing free online 3D shapes collections dictated research on 3D retrieval.
Active debate has however been had on (i) what the best input modality is to
trigger retrieval, and (ii) the ultimate usage scenario for such retrieval. In
this paper, we offer a different perspective towards answering these questions
-- we study the use of 3D sketches as an input modality and advocate a
VR-scenario where retrieval is conducted. Thus, the ultimate vision is that
users can freely retrieve a 3D model by air-doodling in a VR environment. As a
first stab at this new 3D VR-sketch to 3D shape retrieval problem, we make four
contributions. First, we code a VR utility to collect 3D VR-sketches and
conduct retrieval. Second, we collect the first set of $167$ 3D VR-sketches on
two shape categories from ModelNet. Third, we propose a novel approach to
generate a synthetic dataset of human-like 3D sketches of different abstract
levels to train deep networks. At last, we compare the common multi-view and
volumetric approaches: We show that, in contrast to 3D shape to 3D shape
retrieval, volumetric point-based approaches exhibit superior performance on 3D
sketch to 3D shape retrieval due to the sparse and abstract nature of 3D
VR-sketches. We believe these contributions will collectively serve as enablers
for future attempts at this problem. The VR interface, code and datasets are
available at https://tinyurl.com/3DSketch3DV.
","['Ling Luo', 'Yulia Gryaditskaya', 'Yongxin Yang', 'Tao Xiang', 'Yi-Zhe Song']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/1105.1534v1,Artificial intelligence,2011-05-08T16:47:35Z,2011-05-08T16:47:35Z,Taking the redpill: Artificial Evolution in native x86 systems,"  In analogon to successful artificial evolution simulations as Tierra or
avida, this text presents a way to perform artificial evolution in a native x86
system. The implementation of the artificial chemistry and first results of
statistical experiments are presented.
",['Thomas Sperl']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/0906.2824v1,Artificial intelligence,2009-06-15T23:47:28Z,2009-06-15T23:47:28Z,What Does Artificial Life Tell Us About Death?,"  Short philosophical essay
",['Carlos Gershenson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2010.00543v1,Artificial intelligence,2020-10-01T16:57:40Z,2020-10-01T16:57:40Z,"Artificial Creations: Ascription, Ownership, Time-Specific Monopolies","  Creativity has always been synonymous with humans. No other living species
could boast of creativity as humans could. Even the smartest computers thrived
only on the ingenious imaginations of its coders. However, that is steadily
changing with highly advanced artificially intelligent systems that demonstrate
incredible capabilities to autonomously (i.e., with minimal or no human input)
produce creative products that would ordinarily deserve intellectual property
status if created by a human. These systems could be called artificial creators
and their creative products artificial creations. The use of artificial
creators is likely to become a part of mainstream production practices in the
creative and innovation industries sooner than we realize. When they do,
intellectual property regimes (that are inherently designed to reward human
creativity) must be sufficiently prepared to aptly respond to the phenomenon of
what could be called artificial creativity. Needless to say, any such response
must be guided by considerations of public welfare. This study analyzes what
that response ought to look like by revisiting the determinants of intellectual
property and critiquing its nature and modes. This understanding of
intellectual property is then applied to investigate the determinants of
intellectual property in artificial creations so as to determine the intrinsic
justifications for intellectual property rewards for artificial creativity, and
accordingly, develop general modalities for granting intellectual property
status to artificial creations. Finally, the treatment of artificial works
(i.e., copyrightable artificial creations) and artificial inventions (i.e.,
patentable artificial creations) by current intellectual property regimes is
critiqued, and specific modalities for granting intellectual property status to
artificial works and artificial inventions are developed.
",['Raj Shekhar']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/cs/0701087v2,Artificial intelligence,2007-01-13T16:50:37Z,2007-04-27T18:00:27Z,Artificiality in Social Sciences,"  This text provides with an introduction to the modern approach of
artificiality and simulation in social sciences. It presents the relationship
between complexity and artificiality, before introducing the field of
artificial societies which greatly benefited from the computer power fast
increase, gifting social sciences with formalization and experimentation tools
previously owned by ""hard"" sciences alone. It shows that as ""a new way of doing
social sciences"", artificial societies should undoubtedly contribute to a
renewed approach in the study of sociality and should play a significant part
in the elaboration of original theories of social phenomena.
",['Jean-Philippe Rennard']
http://arxiv.org/abs/0901.0317v1,Artificial intelligence,2009-01-03T17:35:49Z,2009-01-03T17:35:49Z,Design of a P System based Artificial Graph Chemistry,"  Artificial Chemistries (ACs) are symbolic chemical metaphors for the
exploration of Artificial Life, with specific focus on the origin of life. In
this work we define a P system based artificial graph chemistry to understand
the principles leading to the evolution of life-like structures in an AC set up
and to develop a unified framework to characterize and classify symbolic
artificial chemistries by devising appropriate formalism to capture semantic
and organizational information. An extension of P system is considered by
associating probabilities with the rules providing the topological framework
for the evolution of a labeled undirected graph based molecular reaction
semantics.
",['Janardan Misra']
http://arxiv.org/abs/1412.6703v2,Artificial intelligence,2014-12-20T22:35:48Z,2014-12-23T16:17:35Z,"Quantifying Natural and Artificial Intelligence in Robots and Natural
  Systems with an Algorithmic Behavioural Test","  One of the most important aims of the fields of robotics, artificial
intelligence and artificial life is the design and construction of systems and
machines as versatile and as reliable as living organisms at performing high
level human-like tasks. But how are we to evaluate artificial systems if we are
not certain how to measure these capacities in living systems, let alone how to
define life or intelligence? Here I survey a concrete metric towards measuring
abstract properties of natural and artificial systems, such as the ability to
react to the environment and to control one's own behaviour.
",['Hector Zenil']
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2308.13791v1,Augmented reality,2023-08-26T07:33:23Z,2023-08-26T07:33:23Z,Handwritten image augmentation,"  In this paper, we introduce Handwritten augmentation, a new data augmentation
for handwritten character images. This method focuses on augmenting handwritten
image data by altering the shape of input characters in training. The proposed
handwritten augmentation is similar to position augmentation, color
augmentation for images but a deeper focus on handwritten characters.
Handwritten augmentation is data-driven, easy to implement, and can be
integrated with CNN-based optical character recognition models. Handwritten
augmentation can be implemented along with commonly used data augmentation
techniques such as cropping, rotating, and yields better performance of models
for handwritten image datasets developed using optical character recognition
methods.
",['Mahendran N']
http://arxiv.org/abs/2104.00722v1,Augmented reality,2021-04-01T19:00:17Z,2021-04-01T19:00:17Z,GABO: Graph Augmentations with Bi-level Optimization,"  Data augmentation refers to a wide range of techniques for improving model
generalization by augmenting training examples. Oftentimes such methods require
domain knowledge about the dataset at hand, spawning a plethora of recent
literature surrounding automated techniques for data augmentation. In this work
we apply one such method, bilevel optimization, to tackle the problem of graph
classification on the ogbg-molhiv dataset. Our best performing augmentation
achieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which
makes it the most effective augmenter for this classifier on the leaderboard.
This framework combines a GIN layer augmentation generator with a bias
transformation and outperforms the same classifier augmented using the
state-of-the-art FLAG augmentation.
","['Heejung W. Chung', 'Avoy Datta', 'Chris Waites']"
http://arxiv.org/abs/2501.18648v2,Augmented reality,2025-01-29T16:38:57Z,2025-03-21T18:17:47Z,"Multimodal Large Language Models for Image, Text, and Speech Data
  Augmentation: A Survey","  In the past five years, research has shifted from traditional Machine
Learning (ML) and Deep Learning (DL) approaches to leveraging Large Language
Models (LLMs) , including multimodality, for data augmentation to enhance
generalization, and combat overfitting in training deep convolutional neural
networks. However, while existing surveys predominantly focus on ML and DL
techniques or limited modalities (text or images), a gap remains in addressing
the latest advancements and multi-modal applications of LLM-based methods. This
survey fills that gap by exploring recent literature utilizing multimodal LLMs
to augment image, text, and audio data, offering a comprehensive understanding
of these processes. We outlined various methods employed in the LLM-based
image, text and speech augmentation, and discussed the limitations identified
in current approaches. Additionally, we identified potential solutions to these
limitations from the literature to enhance the efficacy of data augmentation
practices using multimodal LLMs. This survey serves as a foundation for future
research, aiming to refine and expand the use of multimodal LLMs in enhancing
dataset quality and diversity for deep learning applications. (Surveyed Paper
GitHub Repo: https://github.com/WSUAgRobotics/data-aug-multi-modal-llm.
Keywords: LLM data augmentation, Grok text data augmentation, DeepSeek image
data augmentation, Grok speech data augmentation, GPT audio augmentation, voice
augmentation, DeepSeek for data augmentation, DeepSeek R1 text data
augmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM,
Text Augmentation using LLM, LLM data augmentation for deep learning
applications)
","['Ranjan Sapkota', 'Shaina Raza', 'Maged Shoman', 'Achyut Paudel', 'Manoj Karkee']"
http://arxiv.org/abs/2312.05520v1,Augmented reality,2023-12-09T10:24:59Z,2023-12-09T10:24:59Z,Augmenty: A Python Library for Structured Text Augmentation,"  Augmnety is a Python library for structured text augmentation. It is built on
top of spaCy and allows for augmentation of both the text and its annotations.
Augmenty provides a wide range of augmenters which can be combined in a
flexible manner to create complex augmentation pipelines. It also includes a
set of primitives that can be used to create custom augmenters such as word
replacement augmenters. This functionality allows for augmentations within a
range of applications such as named entity recognition (NER), part-of-speech
tagging, and dependency parsing.
",['Kenneth Enevoldsen']
http://arxiv.org/abs/2101.05469v1,Augmented reality,2021-01-14T05:59:23Z,2021-01-14T05:59:23Z,Text Augmentation in a Multi-Task View,"  Traditional data augmentation aims to increase the coverage of the input
distribution by generating augmented examples that strongly resemble original
samples in an online fashion where augmented examples dominate training. In
this paper, we propose an alternative perspective -- a multi-task view (MTV) of
data augmentation -- in which the primary task trains on original examples and
the auxiliary task trains on augmented examples. In MTV data augmentation, both
original and augmented samples are weighted substantively during training,
relaxing the constraint that augmented examples must resemble original data and
thereby allowing us to apply stronger levels of augmentation. In empirical
experiments using four common data augmentation techniques on three benchmark
text classification datasets, we find that the MTV leads to higher and more
robust performance improvements than traditional augmentation.
","['Jason Wei', 'Chengyu Huang', 'Shiqi Xu', 'Soroush Vosoughi']"
http://arxiv.org/abs/2203.06172v2,Augmented reality,2022-03-11T18:57:27Z,2022-03-15T15:36:24Z,Deep AutoAugment,"  While recent automated data augmentation methods lead to state-of-the-art
results, their design spaces and the derived data augmentation strategies still
incorporate strong human priors. In this work, instead of fixing a set of
hand-picked default augmentations alongside the searched data augmentations, we
propose a fully automated approach for data augmentation search named Deep
AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data
augmentation pipeline from scratch by stacking augmentation layers one at a
time until reaching convergence. For each augmentation layer, the policy is
optimized to maximize the cosine similarity between the gradients of the
original and augmented data along the direction with low variance. Our
experiments show that even without default augmentations, we can learn an
augmentation policy that achieves strong performance with that of previous
works. Extensive ablation studies show that the regularized gradient matching
is an effective search method for data augmentation policies. Our code is
available at: https://github.com/MSU-MLSys-Lab/DeepAA .
","['Yu Zheng', 'Zhi Zhang', 'Shen Yan', 'Mi Zhang']"
http://arxiv.org/abs/2301.03174v2,Augmented reality,2023-01-09T05:21:17Z,2023-02-27T10:40:22Z,Augmented Quaternion and Augmented Unit Quaternion Optimization,"  In this paper, we introduce and explore augmented quaternions and augmented
unit quaternions, and present an augmented unit quaternion optimization model.
An augmented quaternion consist of a quaternion and a translation vector. The
multiplication rule of augmented quaternion is defined. An augmented unit
quaternion consists of a unit quaternion and a translation vector. The
augmented unit quaternions form a Lie group. By means of augmented unit
quaternions, we study the error model and kinematics. Then we formulate two
classical problems in robot research, i.e., the hand-eye calibration problem
and the simultaneous localization and mapping (SLAM) problem as augmented unit
quaternion optimization problems, which are actually real smooth spherical
equality constrained optimization problems. Comparing with the corresponding
unit dual quaternion optimization model, the augmented unit quaternion
optimization model has less variables and removes the orthogonality
constraints.
","['Liqun Qi', 'Xiangke Wang', 'Chunfeng Cui']"
http://arxiv.org/abs/2312.11309v2,Augmented reality,2023-12-18T16:02:43Z,2024-10-23T05:30:24Z,"The Ultimate Combo: Boosting Adversarial Example Transferability by
  Composing Data Augmentations","  To help adversarial examples generalize from surrogate machine-learning (ML)
models to targets, certain transferability-based black-box evasion attacks
incorporate data augmentations (e.g., random resizing). Yet, prior work has
explored limited augmentations and their composition. To fill the gap, we
systematically studied how data augmentation affects transferability.
Specifically, we explored 46 augmentation techniques originally proposed to
help ML models generalize to unseen benign samples, and assessed how they
impact transferability, when applied individually or composed. Performing
exhaustive search on a small subset of augmentation techniques and genetic
search on all techniques, we identified augmentation combinations that help
promote transferability. Extensive experiments with the ImageNet and CIFAR-10
datasets and 18 models showed that simple color-space augmentations (e.g.,
color to greyscale) attain high transferability when combined with standard
augmentations. Furthermore, we discovered that composing augmentations impacts
transferability mostly monotonically (i.e., more augmentations $\rightarrow$
$\ge$transferability). We also found that the best composition significantly
outperformed the state of the art (e.g., 91.8% vs. $\le$82.5% average
transferability to adversarially trained targets on ImageNet). Lastly, our
theoretical analysis, backed by empirical evidence, intuitively explains why
certain augmentations promote transferability.
","['Zebin Yun', 'Achi-Or Weingarten', 'Eyal Ronen', 'Mahmood Sharif']"
http://arxiv.org/abs/2403.00875v1,Augmented reality,2024-03-01T07:58:29Z,2024-03-01T07:58:29Z,"Enhancing Protein Predictive Models via Proteins Data Augmentation: A
  Benchmark and New Directions","  Augmentation is an effective alternative to utilize the small amount of
labeled protein data. However, most of the existing work focuses on design-ing
new architectures or pre-training tasks, and relatively little work has studied
data augmentation for proteins. This paper extends data augmentation techniques
previously used for images and texts to proteins and then benchmarks these
techniques on a variety of protein-related tasks, providing the first
comprehensive evaluation of protein augmentation. Furthermore, we propose two
novel semantic-level protein augmentation methods, namely Integrated Gradients
Substitution and Back Translation Substitution, which enable protein
semantic-aware augmentation through saliency detection and biological
knowledge. Finally, we integrate extended and proposed augmentations into an
augmentation pool and propose a simple but effective framework, namely
Automated Protein Augmentation (APA), which can adaptively select the most
suitable augmentation combinations for different tasks. Extensive experiments
have shown that APA enhances the performance of five protein related tasks by
an average of 10.55% across three architectures compared to vanilla
implementations without augmentation, highlighting its potential to make a
great impact on the field.
","['Rui Sun', 'Lirong Wu', 'Haitao Lin', 'Yufei Huang', 'Stan Z. Li']"
http://arxiv.org/abs/2410.01088v2,Augmented reality,2024-10-01T21:33:10Z,2025-02-04T17:27:51Z,Exploring Empty Spaces: Human-in-the-Loop Data Augmentation,"  Data augmentation is crucial to make machine learning models more robust and
safe. However, augmenting data can be challenging as it requires generating
diverse data points to rigorously evaluate model behavior on edge cases and
mitigate potential harms. Creating high-quality augmentations that cover these
""unknown unknowns"" is a time- and creativity-intensive task. In this work, we
introduce Amplio, an interactive tool to help practitioners navigate ""unknown
unknowns"" in unstructured text datasets and improve data diversity by
systematically identifying empty data spaces to explore. Amplio includes three
human-in-the-loop data augmentation techniques: Augment With Concepts, Augment
by Interpolation, and Augment with Large Language Model. In a user study with
18 professional red teamers, we demonstrate the utility of our augmentation
methods in helping generate high-quality, diverse, and relevant model safety
prompts. We find that Amplio enabled red teamers to augment data quickly and
creatively, highlighting the transformative potential of interactive
augmentation workflows.
","['Catherine Yeh', 'Donghao Ren', 'Yannick Assogba', 'Dominik Moritz', 'Fred Hohman']"
http://arxiv.org/abs/2107.11990v2,Augmented reality,2021-07-26T06:54:53Z,2023-03-16T05:23:18Z,Augmentation Pathways Network for Visual Recognition,"  Data augmentation is practically helpful for visual recognition, especially
at the time of data scarcity. However, such success is only limited to quite a
few light augmentations (e.g., random crop, flip). Heavy augmentations are
either unstable or show adverse effects during training, owing to the big gap
between the original and augmented images. This paper introduces a novel
network design, noted as Augmentation Pathways (AP), to systematically
stabilize training on a much wider range of augmentation policies. Notably, AP
tames various heavy data augmentations and stably boosts performance without a
careful selection among augmentation policies. Unlike traditional single
pathway, augmented images are processed in different neural paths. The main
pathway handles the light augmentations, while other pathways focus on the
heavier augmentations. By interacting with multiple paths in a dependent
manner, the backbone network robustly learns from shared visual patterns among
augmentations, and suppresses the side effect of heavy augmentations at the
same time. Furthermore, we extend AP to high-order versions for high-order
scenarios, demonstrating its robustness and flexibility in practical usage.
Experimental results on ImageNet demonstrate the compatibility and
effectiveness on a much wider range of augmentations, while consuming fewer
parameters and lower computational costs at inference time.
","['Yalong Bai', 'Mohan Zhou', 'Wei Zhang', 'Bowen Zhou', 'Tao Mei']"
http://arxiv.org/abs/1310.7526v3,Augmented reality,2013-10-28T18:30:24Z,2017-06-05T20:13:39Z,"KCH representations, augmentations, and $A$-polynomials","  We describe a correspondence between augmentations and certain
representations of the knot group. The correspondence makes the 2-variable
augmentation polynomial into a generalization of the classical $A$-polynomial.
It also associates to an augmentation a rank, which is bounded by the bridge
number and shares its behavior under connect sums. We also study augmentations
with rank equal to the braid index.
",['Christopher Cornwell']
http://arxiv.org/abs/1909.09148v2,Augmented reality,2019-09-19T08:36:45Z,2019-11-21T15:56:49Z,"Data Augmentation Revisited: Rethinking the Distribution Gap between
  Clean and Augmented Data","  Data augmentation has been widely applied as an effective methodology to
improve generalization in particular when training deep neural networks.
Recently, researchers proposed a few intensive data augmentation techniques,
which indeed improved accuracy, yet we notice that these methods augment data
have also caused a considerable gap between clean and augmented data. In this
paper, we revisit this problem from an analytical perspective, for which we
estimate the upper-bound of expected risk using two terms, namely, empirical
risk and generalization error, respectively. We develop an understanding of
data augmentation as regularization, which highlights the major features. As a
result, data augmentation significantly reduces the generalization error, but
meanwhile leads to a slightly higher empirical risk. On the assumption that
data augmentation helps models converge to a better region, the model can
benefit from a lower empirical risk achieved by a simple method, i.e., using
less-augmented data to refine the model trained on fully-augmented data. Our
approach achieves consistent accuracy gain on a few standard image
classification benchmarks, and the gain transfers to object detection.
","['Zhuoxun He', 'Lingxi Xie', 'Xin Chen', 'Ya Zhang', 'Yanfeng Wang', 'Qi Tian']"
http://arxiv.org/abs/2003.06606v1,Augmented reality,2020-03-14T11:18:22Z,2020-03-14T11:18:22Z,"Learn to Augment: Joint Data Augmentation and Network Optimization for
  Text Recognition","  Handwritten text and scene text suffer from various shapes and distorted
patterns. Thus training a robust recognition model requires a large amount of
data to cover diversity as much as possible. In contrast to data collection and
annotation, data augmentation is a low cost way. In this paper, we propose a
new method for text image augmentation. Different from traditional augmentation
methods such as rotation, scaling and perspective transformation, our proposed
augmentation method is designed to learn proper and efficient data augmentation
which is more effective and specific for training a robust recognizer. By using
a set of custom fiducial points, the proposed augmentation method is flexible
and controllable. Furthermore, we bridge the gap between the isolated processes
of data augmentation and network optimization by joint learning. An agent
network learns from the output of the recognition network and controls the
fiducial points to generate more proper training samples for the recognition
network. Extensive experiments on various benchmarks, including regular scene
text, irregular scene text and handwritten text, show that the proposed
augmentation and the joint learning methods significantly boost the performance
of the recognition networks. A general toolkit for geometric augmentation is
available.
","['Canjie Luo', 'Yuanzhi Zhu', 'Lianwen Jin', 'Yongpan Wang']"
http://arxiv.org/abs/2010.11171v2,Augmented reality,2020-10-21T17:46:32Z,2021-10-27T00:16:34Z,How Data Augmentation affects Optimization for Linear Regression,"  Though data augmentation has rapidly emerged as a key tool for optimization
in modern machine learning, a clear picture of how augmentation schedules
affect optimization and interact with optimization hyperparameters such as
learning rate is nascent. In the spirit of classical convex optimization and
recent work on implicit bias, the present work analyzes the effect of
augmentation on optimization in the simple convex setting of linear regression
with MSE loss.
  We find joint schedules for learning rate and data augmentation scheme under
which augmented gradient descent provably converges and characterize the
resulting minimum. Our results apply to arbitrary augmentation schemes,
revealing complex interactions between learning rates and augmentations even in
the convex setting. Our approach interprets augmented (S)GD as a stochastic
optimization method for a time-varying sequence of proxy losses. This gives a
unified way to analyze learning rate, batch size, and augmentations ranging
from additive noise to random projections. From this perspective, our results,
which also give rates of convergence, can be viewed as Monro-Robbins type
conditions for augmented (S)GD.
","['Boris Hanin', 'Yi Sun']"
http://arxiv.org/abs/2105.13608v2,Augmented reality,2021-05-28T06:32:32Z,2021-06-02T15:18:33Z,"Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data
  Augmentation via MiniMax","  In Natural Language Processing (NLP), finding data augmentation techniques
that can produce high-quality human-interpretable examples has always been
challenging. Recently, leveraging kNN such that augmented examples are
retrieved from large repositories of unlabelled sentences has made a step
toward interpretable augmentation. Inspired by this paradigm, we introduce
Minimax-kNN, a sample efficient data augmentation strategy tailored for
Knowledge Distillation (KD). We exploit a semi-supervised approach based on KD
to train a model on augmented data. In contrast to existing kNN augmentation
techniques that blindly incorporate all samples, our method dynamically selects
a subset of augmented samples that maximizes KL-divergence between the teacher
and student models. This step aims to extract the most efficient samples to
ensure our augmented data covers regions in the input space with maximum loss
value. We evaluated our technique on several text classification tasks and
demonstrated that Minimax-kNN consistently outperforms strong baselines. Our
results show that Minimax-kNN requires fewer augmented examples and less
computation to achieve superior performance over the state-of-the-art kNN-based
augmentation techniques.
","['Ehsan Kamalloo', 'Mehdi Rezagholizadeh', 'Peyman Passban', 'Ali Ghodsi']"
http://arxiv.org/abs/2110.13555v2,Augmented reality,2021-10-26T10:33:25Z,2021-11-29T03:38:19Z,Directional Self-supervised Learning for Heavy Image Augmentations,"  Despite the large augmentation family, only a few cherry-picked robust
augmentation policies are beneficial to self-supervised image representation
learning. In this paper, we propose a directional self-supervised learning
paradigm (DSSL), which is compatible with significantly more augmentations.
Specifically, we adapt heavy augmentation policies after the views lightly
augmented by standard augmentations, to generate harder view (HV). HV usually
has a higher deviation from the original image than the lightly augmented
standard view (SV). Unlike previous methods equally pairing all augmented views
to symmetrically maximize their similarities, DSSL treats augmented views of
the same instance as a partially ordered set (with directions as
SV$\leftrightarrow $SV, SV$\leftarrow$HV), and then equips a directional
objective function respecting to the derived relationships among views. DSSL
can be easily implemented with a few lines of codes and is highly flexible to
popular self-supervised learning frameworks, including SimCLR, SimSiam, BYOL.
Extensive experimental results on CIFAR and ImageNet demonstrated that DSSL can
stably improve various baselines with compatibility to a wider range of
augmentations.
","['Yalong Bai', 'Yifan Yang', 'Wei Zhang', 'Tao Mei']"
http://arxiv.org/abs/2206.04726v2,Augmented reality,2022-06-09T18:46:38Z,2022-06-13T07:11:26Z,"COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive
  Learning","  Graph contrastive learning (GCL) improves graph representation learning,
leading to SOTA on various downstream tasks. The graph augmentation step is a
vital but scarcely studied step of GCL. In this paper, we show that the node
embedding obtained via the graph augmentations is highly biased, somewhat
limiting contrastive models from learning discriminative features for
downstream tasks. Thus, instead of investigating graph augmentation in the
input space, we alternatively propose to perform augmentations on the hidden
features (feature augmentation). Inspired by so-called matrix sketching, we
propose COSTA, a novel COvariance-preServing feaTure space Augmentation
framework for GCL, which generates augmented features by maintaining a ""good
sketch"" of original features. To highlight the superiority of feature
augmentation with COSTA, we investigate a single-view setting (in addition to
multi-view one) which conserves memory and computations. We show that the
feature augmentation with COSTA achieves comparable/better results than graph
augmentation based models.
","['Yifei Zhang', 'Hao Zhu', 'Zixing Song', 'Piotr Koniusz', 'Irwin King']"
http://arxiv.org/abs/2211.01184v1,Augmented reality,2022-11-02T14:58:03Z,2022-11-02T14:58:03Z,"Joint Data and Feature Augmentation for Self-Supervised Representation
  Learning on Point Clouds","  To deal with the exhausting annotations, self-supervised representation
learning from unlabeled point clouds has drawn much attention, especially
centered on augmentation-based contrastive methods. However, specific
augmentations hardly produce sufficient transferability to high-level tasks on
different datasets. Besides, augmentations on point clouds may also change
underlying semantics. To address the issues, we propose a simple but efficient
augmentation fusion contrastive learning framework to combine data
augmentations in Euclidean space and feature augmentations in feature space. In
particular, we propose a data augmentation method based on sampling and graph
generation. Meanwhile, we design a data augmentation network to enable a
correspondence of representations by maximizing consistency between augmented
graph pairs. We further design a feature augmentation network that encourages
the model to learn representations invariant to the perturbations using an
encoder perturbation. We comprehensively conduct extensive object
classification experiments and object part segmentation experiments to validate
the transferability of the proposed framework. Experimental results demonstrate
that the proposed framework is effective to learn the point cloud
representation in a self-supervised manner, and yields state-of-the-art results
in the community. The source code is publicly available at:
https://zhiyongsu.github.io/Project/AFSRL.html.
","['Zhuheng Lu', 'Yuewei Dai', 'Weiqing Li', 'Zhiyong Su']"
http://arxiv.org/abs/2305.13520v1,Augmented reality,2023-05-22T22:23:40Z,2023-05-22T22:23:40Z,"Tied-Augment: Controlling Representation Similarity Improves Data
  Augmentation","  Data augmentation methods have played an important role in the recent advance
of deep learning models, and have become an indispensable component of
state-of-the-art models in semi-supervised, self-supervised, and supervised
training for vision. Despite incurring no additional latency at test time, data
augmentation often requires more epochs of training to be effective. For
example, even the simple flips-and-crops augmentation requires training for
more than 5 epochs to improve performance, whereas RandAugment requires more
than 90 epochs. We propose a general framework called Tied-Augment, which
improves the efficacy of data augmentation in a wide range of applications by
adding a simple term to the loss that can control the similarity of
representations under distortions. Tied-Augment can improve state-of-the-art
methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g.
SAM), and semi-supervised learning (e.g. FixMatch). For example,
Tied-RandAugment can outperform RandAugment by 2.0% on ImageNet. Notably, using
Tied-Augment, data augmentation can be made to improve generalization even when
training for a few epochs and when fine-tuning. We open source our code at
https://github.com/ekurtulus/tied-augment/tree/main.
","['Emirhan Kurtulus', 'Zichao Li', 'Yann Dauphin', 'Ekin Dogus Cubuk']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2203.00502v1,Cancer vaccine,2022-02-04T11:50:19Z,2022-02-04T11:50:19Z,"Sensor technologies in cancer research for new directions in diagnosis
  and treatment: and exploratory analysis","  The goal of this study is an exploratory analysis concerning main sensor
technologies applied in cancer research to detect new directions in diagnosis
and treatments. The study focused on types of cancer having a high incidence
and mortality worldwide: breast, lung, colorectal and prostate. Data of the Web
of Science (WOS) core collection database are used to retrieve articles related
to sensor technologies and cancer research over 1991-2021 period. We utilized
Gephi software version 0.9.2 to visualize the co-word networks of the
interaction between sensor technologies and cancers under study. Results show
main clusters of interaction per typology of cancer. Biosensor is the only type
of sensor that plays an essential role in all types of cancer: breast cancer,
lung cancer, prostate cancer, and colorectal cancer. Electrochemical sensor is
applied in all types of cancer under study except lung cancer. Electrochemical
biosensor is used in breast cancer, lung cancer, and prostate cancer research
but not colorectal cancer. Optical sensor can also be considered one of the
sensor technologies that significantly is used in breast cancer, prostate
cancer, and colorectal cancer. This study shows that this type of sensor is
applied in more diversified approaches. Moreover, the oxygen sensor is mostly
studied in lung cancer and breast cancer due to the usage in breath analysis
for the treatment process. Finally, Cmos sensor is a technology used mainly in
lung cancer and colorectal cancer. Results here suggest new directions for the
evolution of science and technology of sensors in cancer research to support
innovation and research policy directed to new technological trajectories
having a potential of accelerated growth and positive social impact for
diagnosis and treatments of cancer.
","['Mario Coccia', 'Saeed Roshani', 'Melika Mosleh']"
http://arxiv.org/abs/1612.09478v1,Cancer vaccine,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/1110.5865v1,Cancer vaccine,2011-10-26T18:07:37Z,2011-10-26T18:07:37Z,"Cancer Networks: A general theoretical and computational framework for
  understanding cancer","  We present a general computational theory of cancer and its developmental
dynamics. The theory is based on a theory of the architecture and function of
developmental control networks which guide the formation of multicellular
organisms. Cancer networks are special cases of developmental control networks.
Cancer results from transformations of normal developmental networks. Our
theory generates a natural classification of all possible cancers based on
their network architecture. Each cancer network has a unique topology and
semantics and developmental dynamics that result in distinct clinical tumor
phenotypes. We apply this new theory with a series of proof of concept cases
for all the basic cancer types. These cases have been computationally modeled,
their behavior simulated and mathematically described using a multicellular
systems biology approach. There are fascinating correspondences between the
dynamic developmental phenotype of computationally modeled {\em in silico}
cancers and natural {\em in vivo} cancers. The theory lays the foundation for a
new research paradigm for understanding and investigating cancer. The theory of
cancer networks implies that new diagnostic methods and new treatments to cure
cancer will become possible.
",['Eric Werner']
http://arxiv.org/abs/2303.09141v1,Cancer vaccine,2023-03-16T08:09:49Z,2023-03-16T08:09:49Z,On a fundamental problem in the analysis of cancer registry data,"  In epidemiology research with cancer registry data, it is often of primary
interest to make inference on cancer death, not overall survival. Since cause
of death is not easy to collect or is not necessarily reliable in cancer
registries, some special methodologies have been introduced and widely used by
using the concepts of the relative survival ratio and the net survival. In
making inference of those measures, external life tables of the general
population are utilized to adjust the impact of non-cancer death on overall
survival. The validity of this adjustment relies on the assumption that
mortality in the external life table approximates non-cancer mortality of
cancer patients. However, the population used to calculate a life table may
include cancer death and cancer patients. Sensitivity analysis proposed by
Talb\""{a}ck and Dickman to address it requires additional information which is
often not easily available. We propose a method to make inference on the net
survival accounting for potential presence of cancer patients and cancer death
in the life table for the general population. The idea of adjustment is to
consider correspondence of cancer mortality in the life table and that in the
cancer registry. We realize a novel method to adjust cancer mortality in the
cancer registry without any additional information to the standard analyses of
cancer registries. Our simulation study revealed that the proposed method
successfully removed the bias. We illustrate the proposed method with the
cancer registry data in England.
","['Sho Komukai', 'Satoshi Hattori', 'Bernard Rachet']"
http://arxiv.org/abs/1409.3263v1,Cancer vaccine,2014-09-10T21:45:52Z,2014-09-10T21:45:52Z,"Understanding genomic alterations in cancer genomes using an integrative
  network approach","  In recent years, cancer genome sequencing and other high-throughput studies
of cancer genomes have generated many notable discoveries. In this review,
Novel genomic alteration mechanisms, such as chromothripsis (chromosomal
crisis) and kataegis (mutation storms), and their implications for cancer are
discussed. Genomic alterations spur cancer genome evolution. Thus, the
relationship between cancer clonal evolution and cancer stems cells is
commented. The key question in cancer biology concerns how these genomic
alterations support cancer development and metastasis in the context of
biological functioning. Thus far, efforts such as pathway analysis have
improved the understanding of the functional contributions of genetic mutations
and DNA copy number variations to cancer development, progression and
metastasis. However, the known pathways correspond to a small fraction,
plausibly 5-10%, of somatic mutations and genes with an altered copy number. To
develop a comprehensive understanding of the function of these genomic
alterations in cancer, an integrative network framework is proposed and
discussed. Finally, the challenges and the directions of studying cancer omic
data using an integrative network approach are commented.
",['Edwin Wang']
http://arxiv.org/abs/1408.2009v1,Cancer vaccine,2014-08-09T02:46:28Z,2014-08-09T02:46:28Z,"Predictive genomics: A cancer hallmark network framework for predicting
  tumor clinical phenotypes using genome sequencing data","  We discuss a cancer hallmark network framework for modelling
genome-sequencing data to predict cancer clonal evolution and associated
clinical phenotypes. Strategies of using this framework in conjunction with
genome sequencing data in an attempt to predict personalized drug targets, drug
resistance, and metastasis for a cancer patient, as well as cancer risks for a
healthy individual are discussed. Accurate prediction of cancer clonal
evolution and clinical phenotypes will have substantial impact on timely
diagnosis, personalized management and prevention of cancer.
","['Edwin Wang', 'Naif Zaman', 'Shauna Mcgee', 'Jean-Sébastien Milanese', 'Ali Masoudi-Nejad', ""Maureen O'Connor""]"
http://arxiv.org/abs/1910.03934v1,Cancer vaccine,2019-10-08T05:42:36Z,2019-10-08T05:42:36Z,Noncoding RNAs serve as the deadliest regulators for cancer,"  Cancer is one of the leading causes of human death. Many efforts have made to
understand its mechanism and have further identified many proteins and DNA
sequence variations as suspected targets for therapy. However, drugs targeting
these targets have low success rates, suggesting the basic mechanism still
remains unclear. Here, we develop a computational software combining Cox
proportional-hazards model and stability-selection to unearth an overlooked,
yet the most important cancer drivers hidden in massive data from The Cancer
Genome Atlas (TCGA), including 11,574 RNAseq samples and clinic data.
Generally, noncoding RNAs primarily regulate cancer deaths and work as the
deadliest cancer inducers and repressors, in contrast to proteins as
conventionally thought. Especially, processed-pseudogenes serve as the primary
cancer inducers, while lincRNA and antisense RNAs dominate the repressors.
Strikingly, noncoding RNAs serves as the universal strongest regulators for all
cancer types although personal clinic variables such as alcohol and smoking
significantly alter cancer genome. Furthermore, noncoding RNAs also work as
central hubs in cancer regulatory network and as biomarkers to discriminate
cancer types. Therefore, noncoding RNAs overall serve as the deadliest cancer
regulators, which refreshes the basic concept of cancer mechanism and builds a
novel basis for cancer research and therapy. Biological functions of
pseudogenes have rarely been recognized. Here we reveal them as the most
important cancer drivers for all cancer types from big data, breaking a wall to
explore their biological potentials.
","['Anyou Wang', 'Hai Rong']"
http://arxiv.org/abs/2007.00887v1,Cancer vaccine,2020-07-02T05:18:08Z,2020-07-02T05:18:08Z,Computational methods for cancer driver discovery: A survey,"  Motivation: Uncovering the genomic causes of cancer, known as cancer driver
genes, is a fundamental task in biomedical research. Cancer driver genes drive
the development and progression of cancer, thus identifying cancer driver genes
and their regulatory mechanism is crucial to the design of cancer treatment and
intervention. Many computational methods, which take the advantages of computer
science and data science, have been developed to utilise multiple types of
genomic data to reveal cancer drivers and their regulatory mechanism behind
cancer development and progression. Due to the complexity of the mechanistic
insight of cancer genes in driving cancer and the fast development of the
field, it is necessary to have a comprehensive review about the current
computational methods for discovering different types of cancer drivers.
Results: We survey computational methods for identifying cancer drivers from
genomic data. We categorise the methods into three groups, methods for single
driver identification, methods for driver module identification, and methods
for identifying personalised cancer drivers. We also conduct a case study to
compare the performance of the current methods. We further analyse the
advantages and limitations of the current methods, and discuss the challenges
and future directions of the topic. In addition, we investigate the resources
for discovering and validating cancer drivers in order to provide a one-stop
reference of the tools to facilitate cancer driver discovery. The ultimate goal
of the paper is to help those interested in the topic to establish a solid
background to carry out further research in the field.
","['Vu Viet Hoang Pham', 'Lin Liu', 'Cameron Bracken', 'Gregory Goodall', 'Jiuyong Li', 'Thuc Duy Le']"
http://arxiv.org/abs/1205.1923v1,Cancer vaccine,2012-05-09T09:37:52Z,2012-05-09T09:37:52Z,"Using data mining techniques for diagnosis and prognosis of cancer
  disease","  Breast cancer is one of the leading cancers for women in developed countries
including India. It is the second most common cause of cancer death in women.
The high incidence of breast cancer in women has increased significantly in the
last years. In this paper we have discussed various data mining approaches that
have been utilized for breast cancer diagnosis and prognosis. Breast Cancer
Diagnosis is distinguishing of benign from malignant breast lumps and Breast
Cancer Prognosis predicts when Breast Cancer is to recur in patients that have
had their cancers excised. This study paper summarizes various review and
technical articles on breast cancer diagnosis and prognosis also we focus on
current research being carried out using the data mining techniques to enhance
the breast cancer diagnosis and prognosis.
",['Shweta Kharya']
http://arxiv.org/abs/2310.16650v1,Cancer vaccine,2023-10-25T13:59:36Z,2023-10-25T13:59:36Z,"Data-integration with pseudoweights and survey-calibration: application
  to developing US-representative lung cancer risk models for use in screening","  Accurate cancer risk estimation is crucial to clinical decision-making, such
as identifying high-risk people for screening. However, most existing cancer
risk models incorporate data from epidemiologic studies, which usually cannot
represent the target population. While population-based health surveys are
ideal for making inference to the target population, they typically do not
collect time-to-cancer incidence data. Instead, time-to-cancer specific
mortality is often readily available on surveys via linkage to vital
statistics. We develop calibrated pseudoweighting methods that integrate
individual-level data from a cohort and a survey, and summary statistics of
cancer incidence from national cancer registries. By leveraging
individual-level cancer mortality data in the survey, the proposed methods
impute time-to-cancer incidence for survey sample individuals and use survey
calibration with auxiliary variables of influence functions generated from Cox
regression to improve robustness and efficiency of the inverse-propensity
pseudoweighting method in estimating pure risks. We develop a lung cancer
incidence pure risk model from the Prostate, Lung, Colorectal, and Ovarian
(PLCO) Cancer Screening Trial using our proposed methods by integrating data
from the National Health Interview Survey (NHIS) and cancer registries.
","['Lingxiao Wang', 'Yan Li', 'Barry Graubard', 'Hormuzd Katki']"
http://arxiv.org/abs/2405.05643v1,Cancer vaccine,2024-05-09T09:30:54Z,2024-05-09T09:30:54Z,"Cancer mortality projection: disparities, COVID-19, and late diagnosis
  impact","  This paper investigates projection of two major causes of cancer mortality,
breast cancer and lung cancer, by using a Bayesian modelling framework. We
investigate patterns in 2001-2018 (as baseline) in cause-specific cancer
mortality and project these by year of death and various risk factors: age,
gender, regions of England, income deprivation quintile, average
age-at-diagnosis, and non-smoker prevalence rates. We then assess excess cancer
mortality during the COVID-19 pandemic years, and we examine the impact of
diagnosis delays on lung cancer mortality across various scenarios. Our
findings indicate that socio-economic disparities in lung cancer mortality will
persist in the future. Additionally, we observe slight variations in breast
cancer mortality across different regions up to 2036. Furthermore, marginal
increases in excess deaths from lung and breast cancer are estimated in
specific regions of England throughout the pandemic year (2020-2022),
contrasting with the national trend. However, the excess lung cancer deaths
markedly differ by age, region and deprivation as a result of delays in cancer
diagnosis. Specifically, we find a notably higher number of excess deaths in
the northern regions of England compared to the southern regions, as well as
among individuals living in the most deprived areas compared to those in the
least deprived areas.
","['A. Arik', 'A. J. G. Cairns', 'G. Streftaris']"
http://arxiv.org/abs/2407.19330v1,Cancer vaccine,2024-07-27T19:53:41Z,2024-07-27T19:53:41Z,"Mapping Cancer Stem Cell Markers Distribution:A Hypergraph Analysis
  Across Organs","  This study presents an interdisciplinary approach to analyse the distribution
of cancer stem cell markers (CSCMs) across various cancer-affected organs using
hypergraphs. Cancer stem cells (CSCs) play a crucial role in cancer initiation,
progression, and metastasis. By employing hypergraphs, we model the
relationships between CSCM locations and cancerous organs, providing a
comprehensive representation of these interactions. Initially, we utilised an
unweighted incidence matrix and its Markov transition matrices to gain a
dynamic perspective on CSCM distributions. This method allows us to observe how
these markers spread and influence cancer progression in a dynamical context.
By calculating mutual information for each node and hyperedge, our analysis
uncovers complex interaction patterns between CSCMs and organs, highlighting
the critical roles of certain markers in cancer progression and metastasis. Our
approach offers a detailed representation of cancer stem cell networks,
enhancing our understanding of the mechanisms driving cancer heterogeneity and
metastasis. By integrating hypergraph theory with cancer biology, this study
provides valuable insights for developing targeted cancer therapies.
","['David H. Margarit', 'Gustavo Paccosi', 'Marcela V. Reale', 'Lilia M. Romanelli']"
http://arxiv.org/abs/1112.1510v2,Cancer vaccine,2011-12-07T10:11:20Z,2012-10-11T06:46:23Z,"An approach for the identification of targets specific to bone
  metastasis using cancer genes interactome and gene ontology analysis","  Metastasis is one of the most enigmatic aspects of cancer pathogenesis and is
a major cause of cancer-associated mortality. Secondary bone cancer (SBC) is a
complex disease caused by metastasis of tumor cells from their primary site and
is characterized by intricate interplay of molecular interactions.
Identification of targets for multifactorial diseases such as SBC, the most
frequent complication of breast and prostate cancers, is a challenge. Towards
achieving our aim of identification of targets specific to SBC, we constructed
a 'Cancer Genes Network', a representative protein interactome of cancer genes.
Using graph theoretical methods, we obtained a set of key genes that are
relevant for generic mechanisms of cancers and have a role in biological
essentiality. We also compiled a curated dataset of 391 SBC genes from
published literature which serves as a basis of ontological correlates of
secondary bone cancer. Building on these results, we implement a strategy based
on generic cancer genes, SBC genes and gene ontology enrichment method, to
obtain a set of targets that are specific to bone metastasis. Through this
study, we present an approach for probing one of the major complications in
cancers, namely, metastasis. The results on genes that play generic roles in
cancer phenotype, obtained by network analysis of 'Cancer Genes Network', have
broader implications in understanding the role of molecular regulators in
mechanisms of cancers. Specifically, our study provides a set of potential
targets that are of ontological and regulatory relevance to secondary bone
cancer.
","['Shikha Vashisht', 'Ganesh Bagler']"
http://arxiv.org/abs/2110.12057v1,Cancer vaccine,2021-10-21T05:32:36Z,2021-10-21T05:32:36Z,"Predictive factors associated with survival rate of cervical cancer
  patients in Brunei Darussalam","  Introduction: Cervical cancer is the third most prevalent cancer among women
in Brunei Darussalam. This study aims to report the overall survival rates and
associated factors of patients diagnosed with malignant cervical cancer in
Brunei Darussalam. Methods: A retrospective study of patients diagnosed with
cervical cancer from 2007 to 2017 in Brunei Darussalam. The data were obtained
from the population-based cancer registry in Brunei Darussalam. Kaplan- Meier
survival analysis was used to estimate the overall survival rates at 1-, 3- and
5-year intervals while the log-rank test was used to assess differences in
survival between groups. Cox Proportional Hazard (PH) regression analysis was
used to examine the association of demographic and clinical factors on the
survival of cervical cancer patients. Results: A total of 329 registered
malignant cervical cancer cases were analyzed. The mean age at diagnosis of
patients with cervical cancer was 46.7 years. There were 28.6% deaths and the
overall survival rates at 1, 3 and 5 years were 85.4%, 72.6% and 68.6%
respectively. Age at diagnosis, cancer stage and histology types were
significant predictive factors for overall survival of the patients diagnosed
with cervical cancers when analysed on both log rank tests and Cox PH model.
Conclusion: Age at diagnosis, cancer stage and histology types were
significantly associated with the overall survival rates of cervical cancer
patients in Brunei Darussalam. Early detection and management of cervical
cancer at early stages should be prioritized to improve the survival rate and
quality of cancer care.
","['Fadhliah Madli', 'Elvynna Leong', 'Sok King Ong', 'Edwin Lim', 'Khairul Amilin Tengah']"
http://arxiv.org/abs/2308.02528v1,Cancer vaccine,2023-07-31T16:28:42Z,2023-07-31T16:28:42Z,A comprehensive review of deep learning in lung cancer,"  To provide the reader with a historical perspective on cancer classification
approaches, we first discuss the fundamentals of the area of cancer diagnosis
in this article, including the processes of cancer diagnosis and the standard
classification methods employed by clinicians. Current methods for cancer
diagnosis are deemed ineffective, calling for new and more intelligent
approaches.
",['Farzane Tajidini']
http://arxiv.org/abs/1711.09015v1,Cancer vaccine,2017-11-24T15:18:52Z,2017-11-24T15:18:52Z,"The cellular ROS-scavenging function, a key factor determining the
  specific vulnerability of cancer cells to cold atmospheric plasma in vitro","  Cold atmospheric plasma (CAP) has shown its promising application in cancer
treatment both in vitro and in vivo. However, the anti-cancer mechanism is
still largely unknown. CAP may kill cancer cells via triggering the rise of
intracellular ROS, DNA damage, mitochondrial damage, or cellular membrane
damage. While, the specific vulnerability of cancer cells to CAP has been
observed, the underlying mechanism of such cell-based specific vulnerability to
CAP is completely unknown. Here, through the comparison of CAP treatment and
H2O2 treatment on 10 different cancer cell lines in vitro, we observed that the
H2O2 consumption speed by cancer cells was strongly correlated to the
cytotoxicity of CAP treatment on cancer cells. Cancer cells that clear
extracellular H2O2 more quickly are more resistant to the cytotoxicity of CAP
treatment. This finding strongly indicates that the anti-oxidant system in
cancer cells play a key role in the specific vulnerability of cancer cells to
CAP treatment in vitro.
","['Dayun Yan', 'Jonathan H. Sherman', 'Jerome Canady', 'Barry Trink', 'Michael Keidar']"
http://arxiv.org/abs/1812.09203v1,Cancer vaccine,2018-12-21T15:42:00Z,2018-12-21T15:42:00Z,Pan-Cancer Epigenetic Biomarker Selection from Blood Samples Using SAS,"  A key focus in current cancer research is the discovery of cancer biomarkers
that allow earlier detection with high accuracy and lower costs for both
patients and hospitals. Blood samples have long been used as a health status
indicator, but DNA methylation signatures in blood have not been fully
appreciated in cancer research. Historically, analysis of cancer has been
conducted directly with the patient's tumor or related tissues. Such analyses
allow physicians to diagnose a patient's health and cancer status; however,
physicians must observe certain symptoms that prompt them to use biopsies or
imaging to verify the diagnosis. This is a post-hoc approach. Our study will
focus on epigenetic information for cancer detection, specifically information
about DNA methylation in human peripheral blood samples in cancer discordant
monozygotic twin-pairs. This information might be able to help us detect cancer
much earlier, before the first symptom appears. Several other types of
epigenetic data can also be used, but here we demonstrate the potential of
blood DNA methylation data as a biomarker for pan-cancer using SAS 9.3 and SAS
EM. We report that 55 methylation CpG sites measurable in blood samples can be
used as biomarkers for early cancer detection and classification.
","['Xi Chen', 'Jin Xie', 'Qingcong Yuan']"
http://arxiv.org/abs/1912.12379v1,Cancer vaccine,2019-12-28T01:07:36Z,2019-12-28T01:07:36Z,"A Common Gene Expression Signature Analysis Method for Multiple Types of
  Cancer","  Mining gene expression profiles has proven valuable for identifying
signatures serving as surrogates of cancer phenotypes. However, the
similarities of such signatures across different cancer types have not been
strong enough to conclude that they represent a universal biological mechanism
shared among multiple cancer types. Here we describe a network-based approach
that explores gene-to-gene connections in multiple cancer datasets while
maximizing the overall association of the subnetwork with clinical outcomes.
With the dataset of The Cancer Genome Atlas (TCGA), we studied the
characteristics of common gene expression of three types of cancers: Rectum
adenocarcinoma (READ), Breast invasive carcinoma (BRCA) and Colon
adenocarcinoma (COAD). By analyzing several pairs of highly correlated genes
after filtering and clustering work, we found that the co-expressed genes
across multiple types of cancers point to particular biological mechanisms
related to cancer cell progression , suggesting that they represent important
attributes of cancer in need of being elucidated for potential applications in
diagnostic, prognostic and therapeutic products applicable to multiple cancer
types.
","['Yingcheng Sun', 'Xiangru Liang', 'Kenneth Loparo']"
http://arxiv.org/abs/2301.11126v1,Cancer vaccine,2023-01-24T18:06:52Z,2023-01-24T18:06:52Z,Three facets of mathematical cancer biology research,"  Cancer, as the uncontrollable cell growth, is related to many branches of
biology. In this review, we will discuss three mathematical approaches for
studying cancer biology: population dynamics, gene regulation, and
developmental biology. If we understand all biochemical mechanisms of cancer
cells, we can directly calculate how the cancer cell population behaves.
Inversely, just from the cell count data, we can use population dynamics to
infer the mechanisms. Cancer cells emerge from certain genetic mutations, which
affect the expression of other genes through gene regulation. Therefore,
knowledge of gene regulation can help with cancer prevention and treatment.
Developmental biology studies acquisition and maintenance of normal cellular
function, which is inspiring to cancer biology in the opposite direction.
Besides, cancer cells implanted into an embryo can differentiate into normal
tissues, which provides a possible approach of curing cancer. This review
illustrates the role of mathematics in these three fields: what mathematical
models are used, what data analysis tools are applied, and what mathematical
theorems need to be proved. We hope that applied mathematicians and even pure
mathematicians can find meaningful mathematical problems related to cancer
biology.
",['Yue Wang']
http://arxiv.org/abs/2302.02456v2,Cancer vaccine,2023-02-05T18:50:12Z,2023-02-15T23:36:32Z,Deep Learning Approach for Early Stage Lung Cancer Detection,"  Lung cancer is the leading cause of death among different types of cancers.
Every year, the lives lost due to lung cancer exceed those lost to pancreatic,
breast, and prostate cancer combined. The survival rate for lung cancer
patients is very low compared to other cancer patients due to late diagnostics.
Thus, early lung cancer diagnostics is crucial for patients to receive early
treatments, increasing the survival rate or even becoming cancer-free. This
paper proposed a deep-learning model for early lung cancer prediction and
diagnosis from Computed Tomography (CT) scans. The proposed mode achieves high
accuracy. In addition, it can be a beneficial tool to support radiologists'
decisions in predicting and detecting lung cancer and its stage.
","['Saleh Abunajm', 'Nelly Elsayed', 'Zag ElSayed', 'Murat Ozer']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/2410.12971v1,Cultured meat,2024-10-16T19:06:08Z,2024-10-16T19:06:08Z,Self-Pluralising Culture Alignment for Large Language Models,"  As large language models (LLMs) become increasingly accessible in many
countries, it is essential to align them to serve pluralistic human values
across cultures. However, pluralistic culture alignment in LLMs remain an open
problem. In this paper, we propose CultureSPA, a Self-Pluralising Culture
Alignment framework that allows LLMs to simultaneously align to pluralistic
cultures. The framework first generates questions on various culture topics,
then yields LLM outputs in response to these generated questions under both
culture-aware and culture-unaware settings. By comparing culture-aware/unaware
outputs, we are able to detect and collect culture-related instances. These
instances are employed to fine-tune LLMs to serve pluralistic cultures in
either a culture-joint or culture-specific way. Extensive experiments
demonstrate that CultureSPA significantly improves the alignment of LLMs to
diverse cultures without compromising general abilities. And further
improvements can be achieved if CultureSPA is combined with advanced prompt
engineering techniques. Comparisons between culture-joint and culture-specific
tuning strategies, along with variations in data quality and quantity,
illustrate the robustness of our method. We also explore the mechanisms
underlying CultureSPA and the relations between different cultures it reflects.
","['Shaoyang Xu', 'Yongqi Leng', 'Linhao Yu', 'Deyi Xiong']"
http://arxiv.org/abs/2310.06458v2,Cultured meat,2023-10-10T09:29:38Z,2024-09-02T02:26:18Z,"Cultural Compass: Predicting Transfer Learning Success in Offensive
  Language Detection with Cultural Features","  The increasing ubiquity of language technology necessitates a shift towards
considering cultural diversity in the machine learning realm, particularly for
subjective tasks that rely heavily on cultural nuances, such as Offensive
Language Detection (OLD). Current understanding underscores that these tasks
are substantially influenced by cultural values, however, a notable gap exists
in determining if cultural features can accurately predict the success of
cross-cultural transfer learning for such subjective tasks. Addressing this,
our study delves into the intersection of cultural features and transfer
learning effectiveness. The findings reveal that cultural value surveys indeed
possess a predictive power for cross-cultural transfer learning success in OLD
tasks and that it can be further improved using offensive word distance. Based
on these results, we advocate for the integration of cultural information into
datasets. Additionally, we recommend leveraging data sources rich in cultural
information, such as surveys, to enhance cultural adaptability. Our research
signifies a step forward in the quest for more inclusive, culturally sensitive
language technologies.
","['Li Zhou', 'Antonia Karamolegkou', 'Wenyu Chen', 'Daniel Hershcovich']"
http://arxiv.org/abs/2504.08820v1,Cultured meat,2025-04-09T13:40:13Z,2025-04-09T13:40:13Z,"CAReDiO: Cultural Alignment of LLM via Representativeness and
  Distinctiveness Guided Data Optimization","  As Large Language Models (LLMs) more deeply integrate into human life across
various regions, aligning them with pluralistic cultures is crucial for
improving user experience and mitigating cultural conflicts. Existing
approaches develop culturally aligned LLMs primarily through fine-tuning with
massive carefully curated culture-specific corpora. Nevertheless, inspired by
culture theories, we identify two key challenges faced by these datasets: (1)
Representativeness: These corpora fail to fully capture the target culture's
core characteristics with redundancy, causing computation waste; (2)
Distinctiveness: They struggle to distinguish the unique nuances of a given
culture from shared patterns across other relevant ones, hindering precise
cultural modeling. To handle these challenges, we introduce CAReDiO, a novel
cultural data construction framework. Specifically, CAReDiO utilizes powerful
LLMs to automatically generate cultural conversation data, where both the
queries and responses are further optimized by maximizing representativeness
and distinctiveness. Using CAReDiO, we construct a small yet effective dataset,
covering five cultures, and compare it with several recent cultural corpora.
Extensive experiments demonstrate that our method generates more effective data
and enables cultural alignment with as few as 100 training samples, enhancing
both performance and efficiency.
","['Jing Yao', 'Xiaoyuan Yi', 'Jindong Wang', 'Zhicheng Dou', 'Xing Xie']"
http://arxiv.org/abs/2007.02359v5,Cultured meat,2020-07-05T15:06:50Z,2022-09-15T20:00:11Z,"Cultures as networks of cultural traits: A unifying framework for
  measuring culture and cultural distances","  Making use of the information from the World Value Survey (WVS), and
operationalizing a definition of national culture that encompasses both the
relevance of specific cultural traits and the interdependence among them, this
paper proposes a methodology to reveal the latent structure of national culture
and to measure cultural distance between countries that takes into account both
the difference in cultural traits and the difference in the network structure
of national cultures. Exploiting the possibilities offered by copula graphical
models for discrete data, this paper infers the cultural networks of all the
countries included in the WVS (Wave 6) and proposes a novel unifying framework
to measure national culture and international cultural distances. The Jeffreys'
divergence between copula graphical models, taken as the measure of cultural
distance between countries, captures the orthogonality of the two components of
cultural distance: the one based on cultural traits and the one based on the
network structure among them. Moreover, the two components are shown to
correlate with different national and structural characteristics of cultural
networks, thus encompassing the different informational sets related to
national cultures.
","['Luca De Benedictis', 'Roberto Rondinelli', 'Veronica Vinciotti']"
http://arxiv.org/abs/2309.12342v2,Cultured meat,2023-08-25T14:50:13Z,2024-05-08T14:48:39Z,"Cultural Alignment in Large Language Models: An Explanatory Analysis
  Based on Hofstede's Cultural Dimensions","  The deployment of large language models (LLMs) raises concerns regarding
their cultural misalignment and potential ramifications on individuals and
societies with diverse cultural backgrounds. While the discourse has focused
mainly on political and social biases, our research proposes a Cultural
Alignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's
cultural dimension framework, which offers an explanatory cross-cultural
comparison through the latent variable analysis. We apply our approach to
quantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the
cultural dimensions of regions like the United States, China, and Arab
countries, using different prompting styles and exploring the effects of
language-specific fine-tuning on the models' behavioural tendencies and
cultural values. Our results quantify the cultural alignment of LLMs and reveal
the difference between LLMs in explanatory cultural dimensions. Our study
demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows
a unique capability to adapt to cultural nuances, particularly in Chinese
settings. However, it faces challenges with American and Arab cultures. The
research also highlights that fine-tuning LLama 2 models with different
languages changes their responses to cultural questions, emphasizing the need
for culturally diverse development in AI for worldwide acceptance and ethical
use. For more details or to contribute to this research, visit our GitHub page
https://github.com/reemim/Hofstedes_CAT/
","['Reem I. Masoud', 'Ziquan Liu', 'Martin Ferianc', 'Philip Treleaven', 'Miguel Rodrigues']"
http://arxiv.org/abs/1805.09635v1,Cultured meat,2018-05-24T12:40:47Z,2018-05-24T12:40:47Z,When Cultures Meet: Modelling Cross-Cultural Knowledge Spaces,"  Cross cultural research projects are becoming a norm in our global world.
More and more projects are being executed using teams from eastern and western
cultures. Cultural competence might help project managers to achieve project
goals and avoid potential risks in cross cultural project environments and
would also support them to promote creativity and motivation through flexible
leadership. In our paper we introduce an idea for constructing an information
system, a cross cultural knowledge space, which could support cross cultural
communication, collaborative learning experiences and time based project
management functions. The case cultures in our project are Finnish and
Japanese. The system can be used both in virtual and in physical spaces for
example to clarify cultural business etiquette. The core of our system design
will be based on cross cultural ontology, and the system implementation on XML
technologies. Our approach is a practical, step by step example of constructive
research. In our paper we shortly describe Hofstede's dimensions for assessing
cultures as one example of a larger framework for our study. We also discuss
the concept of time in cultural context.
",['Anneli Heimbürger']
http://arxiv.org/abs/2211.07460v1,Cultured meat,2022-11-14T15:42:27Z,2022-11-14T15:42:27Z,"An Analytics of Culture: Modeling Subjectivity, Scalability,
  Contextuality, and Temporality","  There is a bidirectional relationship between culture and AI; AI models are
increasingly used to analyse culture, thereby shaping our understanding of
culture. On the other hand, the models are trained on collections of cultural
artifacts thereby implicitly, and not always correctly, encoding expressions of
culture. This creates a tension that both limits the use of AI for analysing
culture and leads to problems in AI with respect to cultural complex issues
such as bias.
  One approach to overcome this tension is to more extensively take into
account the intricacies and complexities of culture. We structure our
discussion using four concepts that guide humanistic inquiry into culture:
subjectivity, scalability, contextuality, and temporality. We focus on these
concepts because they have not yet been sufficiently represented in AI
research. We believe that possible implementations of these aspects into AI
research leads to AI that better captures the complexities of culture. In what
follows, we briefly describe these four concepts and their absence in AI
research. For each concept, we define possible research challenges.
","['Nanne van Noord', 'Melvin Wevers', 'Tobias Blanke', 'Julia Noordegraaf', 'Marcel Worring']"
http://arxiv.org/abs/2310.01929v3,Cultured meat,2023-10-03T10:13:36Z,2024-08-13T08:11:49Z,"Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of
  Text-To-Image Models","  Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have
demonstrated remarkable prompt-based image generation capabilities.
Multilingual encoders may have a substantial impact on the cultural agency of
these models, as language is a conduit of culture. In this study, we explore
the cultural perception embedded in TTI models by characterizing culture across
three hierarchical tiers: cultural dimensions, cultural domains, and cultural
concepts. Based on this ontology, we derive prompt templates to unlock the
cultural knowledge in TTI models, and propose a comprehensive suite of
evaluation techniques, including intrinsic evaluations using the CLIP space,
extrinsic evaluations with a Visual-Question-Answer (VQA) model and human
assessments, to evaluate the cultural content of TTI-generated images. To
bolster our research, we introduce the CulText2I dataset, derived from six
diverse TTI models and spanning ten languages. Our experiments provide insights
regarding Do, What, Which and How research questions about the nature of
cultural encoding in TTI models, paving the way for cross-cultural applications
of these models.
","['Mor Ventura', 'Eyal Ben-David', 'Anna Korhonen', 'Roi Reichart']"
http://arxiv.org/abs/2406.14504v2,Cultured meat,2024-06-20T17:06:58Z,2024-10-14T15:39:36Z,Translating Across Cultures: LLMs for Intralingual Cultural Adaptation,"  LLMs are increasingly being deployed for multilingual applications and have
demonstrated impressive translation capabilities between several low and
high-resource languages. An aspect of translation that often gets overlooked is
that of cultural adaptation, or modifying source culture references to suit the
target culture. While specialized translation models still outperform LLMs on
the machine translation task when viewed from the lens of correctness, they are
not sensitive to cultural differences often requiring manual correction. LLMs
on the other hand have a rich reservoir of cultural knowledge embedded within
its parameters that can be potentially exploited for such applications. In this
paper, we define the task of cultural adaptation and create an evaluation
framework to evaluate the performance of modern LLMs for cultural adaptation
and analyze their cross-cultural knowledge while connecting related concepts
across different cultures. We also analyze possible issues with automatic
adaptation. We hope that this task will offer more insight into the cultural
understanding of LLMs and their creativity in cross-cultural scenarios.
","['Pushpdeep Singh', 'Mayur Patidar', 'Lovekesh Vig']"
http://arxiv.org/abs/1002.1196v1,Cultured meat,2010-02-05T11:02:02Z,2010-02-05T11:02:02Z,Cultural commons and cultural evolution,"  Culture evolves following a process that is akin to biological evolution,
although with some significant differences. At the same time culture has often
a collective good value for human groups. This paper studies culture in an
evolutionary perspective, with a focus on the implications of group definition
for the coexistence of different cultures. A model of cultural evolution is
presented where agents interacts in an artificial environment. The belonging to
a specific memetic group is a major factor allowing agents to exploit different
environmental niches with, as a result, the coexistence of different cultures
in the same environment.
",['Giangiacomo Bravo']
http://arxiv.org/abs/1810.03605v1,Cultured meat,2018-10-08T08:47:46Z,2018-10-08T08:47:46Z,"Critical review of models, containing cultural levels beyond the
  organizational one","  The current article traces back the scientific interest to cultural levels
across the organization at the University of National and World Economy, and
especially in the series of Economic Alternatives - an official scientific
magazine, issued by this Institution. Further, a wider and critical review of
international achievements in this field is performed, revealing diverse
analysis perspectives with respect to cultural levels. Also, a useful model of
exploring and teaching the cultural levels beyond the organization is proposed.
  Keywords: globalization, national culture, organization culture, cultural
levels, cultural economics. JEL: M14, Z10.
",['Kiril Dimitrov']
http://arxiv.org/abs/2211.15271v2,Cultured meat,2022-11-28T12:54:34Z,2022-11-29T11:22:38Z,The Myth of Culturally Agnostic AI Models,"  The paper discusses the potential of large vision-language models as objects
of interest for empirical cultural studies. Focusing on the comparative
analysis of outputs from two popular text-to-image synthesis models, DALL-E 2
and Stable Diffusion, the paper tries to tackle the pros and cons of striving
towards culturally agnostic vs. culturally specific AI models. The paper
discusses several examples of memorization and bias in generated outputs which
showcase the trade-off between risk mitigation and cultural specificity, as
well as the overall impossibility of developing culturally agnostic models.
",['Eva Cetinic']
http://arxiv.org/abs/2311.14367v2,Cultured meat,2023-11-24T09:23:13Z,2023-12-21T11:38:28Z,"Joint modelling of national cultures accounting for within and
  between-country heterogeneity","  Cultural values vary significantly around the world. Despite a large
heterogeneity, similarities across national cultures are present. This paper
studies cross-country culture heterogeneity via the joint inference of
country-specific copula graphical models from world-wide survey data. To this
end, a random graph generative model of the cultural networks is introduced,
with a latent space and proximity measures that embed cultural relatedness
across countries. Within-country heterogeneity is also accounted for, via
parametric modelling of the marginal distributions of each cultural trait. All
together, the different components of the model are able to identify several
dimensions of culture.
","['Veronica Vinciotti', 'Luca De Benedictis', 'Ernst C. Wit']"
http://arxiv.org/abs/2409.07475v1,Cultured meat,2024-08-28T10:24:54Z,2024-08-28T10:24:54Z,"Cross-Cultural Communication in the Digital Age: An Analysis of Cultural
  Representation and Inclusivity in Emojis","  Emojis have become a universal language in the digital world, enabling users
to express emotions, ideas, and identities across diverse cultural contexts. As
emojis incorporate more cultural symbols and diverse representations, they play
a crucial role in cross-cultural communication. This research project aims to
analyze the representation of different cultures in emojis, investigate how
emojis facilitate cross-cultural communication and promote inclusivity, and
explore the impact of emojis on understanding and interpretation in different
cultural contexts.
","['Lingfeng Li', 'Xiangwen Zheng']"
http://arxiv.org/abs/2503.16520v1,Cultured meat,2025-03-17T01:23:57Z,2025-03-17T01:23:57Z,"Not All Personas Are Worth It: Culture-Reflective Persona Data
  Augmentation","  Incorporating personas into conversational AI models is crucial for achieving
authentic and engaging interactions. However, the cultural diversity and
adaptability of existing persona datasets is often overlooked, reducing their
efficacy in building culturally aware AI systems. To address this issue, we
propose a two-step pipeline for generating culture-specific personas and
introduce KoPersona, a dataset comprising 200,000 personas designed to capture
Korean cultural values, behaviors, and social nuances. A comprehensive
evaluation through various metrics validates the quality of KoPersona and its
relevance to Korean culture. This work not only contributes to persona-based
research, but also establishes a scalable approach for creating culturally
relevant personas adaptable to various languages and cultural contexts.
","['Ji-Eun Han', 'Yoonseok Heo']"
http://arxiv.org/abs/2405.15145v3,Cultured meat,2024-05-24T01:49:02Z,2024-11-21T10:52:29Z,"CulturePark: Boosting Cross-cultural Understanding in Large Language
  Models","  Cultural bias is pervasive in many large language models (LLMs), largely due
to the deficiency of data representative of different cultures. Typically,
cultural datasets and benchmarks are constructed either by extracting subsets
of existing datasets or by aggregating from platforms such as Wikipedia and
social media. However, these approaches are highly dependent on real-world data
and human annotations, making them costly and difficult to scale. Inspired by
cognitive theories on social communication, this paper introduces CulturePark,
an LLM-powered multi-agent communication framework for cultural data
collection. CulturePark simulates cross-cultural human communication with
LLM-based agents playing roles in different cultures. It generates high-quality
cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using
CulturePark, we generated 41,000 cultural samples to fine-tune eight
culture-specific LLMs. We evaluated these models across three downstream tasks:
content moderation, cultural alignment, and cultural education. Results show
that for content moderation, our GPT-3.5-based models either match or
outperform GPT-4 on datasets. Regarding cultural alignment, our models surpass
GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of
human participants, our models demonstrate superior outcomes in both learning
efficacy and user experience compared to GPT-4. CulturePark proves an important
step in addressing cultural bias and advancing the democratization of AI,
highlighting the critical role of culturally inclusive data in model training.
Code is released at https://github.com/Scarelette/CulturePark.
","['Cheng Li', 'Damien Teney', 'Linyi Yang', 'Qingsong Wen', 'Xing Xie', 'Jindong Wang']"
http://arxiv.org/abs/2408.05102v1,Cultured meat,2024-08-09T14:45:22Z,2024-08-09T14:45:22Z,How Well Do LLMs Identify Cultural Unity in Diversity?,"  Much work on the cultural awareness of large language models (LLMs) focuses
on the models' sensitivity to geo-cultural diversity. However, in addition to
cross-cultural differences, there also exists common ground across cultures.
For instance, a bridal veil in the United States plays a similar
cultural-relevant role as a honggaitou in China. In this study, we introduce a
benchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the
cultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation
examples building upon 285 traditional cultural-specific concepts across 10
countries. Based on a systematic manual annotation of cultural-relevant
features per concept, we calculate the cultural association between any pair of
cross-cultural concepts. Built upon this dataset, we design a contrastive
matching task to evaluate the LLMs' capability to identify highly associated
cross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular
prompting strategies, under the settings of either giving all extracted concept
features or no features at all on CUNIT Interestingly, we find that cultural
associations across countries regarding clothing concepts largely differ from
food. Our analysis shows that LLMs are still limited to capturing
cross-cultural associations between concepts compared to humans. Moreover,
geo-cultural proximity shows a weak influence on model performance in capturing
cross-cultural associations.
","['Jialin Li', 'Junli Wang', 'Junjie Hu', 'Ming Jiang']"
http://arxiv.org/abs/2410.12880v3,Cultured meat,2024-10-15T18:13:10Z,2025-01-24T18:56:07Z,"Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to
  Sensitivity in Large Language Models","  As LLMs are increasingly deployed in global applications, the importance of
cultural sensitivity becomes paramount, ensuring that users from diverse
backgrounds feel respected and understood. Cultural harm can arise when these
models fail to align with specific cultural norms, resulting in
misrepresentations or violations of cultural values. This work addresses the
challenges of ensuring cultural sensitivity in LLMs, especially in
small-parameter models that often lack the extensive training data needed to
capture global cultural nuances. We present two key contributions: (1) A
cultural harm test dataset, created to assess model outputs across different
cultural contexts through scenarios that expose potential cultural
insensitivities, and (2) A culturally aligned preference dataset, aimed at
restoring cultural sensitivity through fine-tuning based on feedback from
diverse annotators. These datasets facilitate the evaluation and enhancement of
LLMs, ensuring their ethical and safe deployment across different cultural
landscapes. Our results show that integrating culturally aligned feedback leads
to a marked improvement in model behavior, significantly reducing the
likelihood of generating culturally insensitive or harmful content. Ultimately,
this work paves the way for more inclusive and respectful AI systems, fostering
a future where LLMs can safely and ethically navigate the complexities of
diverse cultural landscapes.
","['Somnath Banerjee', 'Sayan Layek', 'Hari Shrawgi', 'Rajarshi Mandal', 'Avik Halder', 'Shanu Kumar', 'Sagnik Basu', 'Parag Agrawal', 'Rima Hazra', 'Animesh Mukherjee']"
http://arxiv.org/abs/2311.14096v2,Cultured meat,2023-11-23T16:45:56Z,2024-06-26T15:26:44Z,Cultural Bias and Cultural Alignment of Large Language Models,"  Culture fundamentally shapes people's reasoning, behavior, and communication.
As people increasingly use generative artificial intelligence (AI) to expedite
and automate personal and professional tasks, cultural values embedded in AI
models may bias people's authentic expression and contribute to the dominance
of certain cultures. We conduct a disaggregated evaluation of cultural bias for
five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)
by comparing the models' responses to nationally representative survey data.
All models exhibit cultural values resembling English-speaking and Protestant
European countries. We test cultural prompting as a control strategy to
increase cultural alignment for each country/territory. For recent models
(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'
output for 71-81% of countries and territories. We suggest using cultural
prompting and ongoing evaluation to reduce cultural bias in the output of
generative AI.
","['Yan Tao', 'Olga Viberg', 'Ryan S. Baker', 'Rene F. Kizilcec']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2104.08256v1,Gene therapy,2021-04-16T17:38:12Z,2021-04-16T17:38:12Z,"Predicting synchronized gene coexpression patterns from fibration
  symmetries in gene regulatory networks in bacteria","  Background: Gene regulatory networks coordinate the expression of genes
across physiological states and ensure a synchronized expression of genes in
cellular subsystems, critical for the coherent functioning of cells. Here we
address the questions whether it is possible to predict gene synchronization
from network structure alone. We have recently shown that synchronized gene
expression may be predicted from symmetries in the gene regulatory networks
(GRN) and described by the concept of symmetry fibrations. We showed that
symmetry fibrations partition the genes into groups called fibers based on the
symmetries of their 'input trees', the set of paths in the network through
which signals can reach a gene. In idealized dynamic gene expression models,
all genes in a fiber are perfectly synchronized, while less idealized models -
with gene input functions differencing between genes - predict symmetry
breaking and desynchronization.
  Results: To study the functional role of gene fibers and to test whether some
of the fiber-induced coexpression remains in reality, we analyze gene
fibrations for the gene regulatory networks of E. coli and B. subtilis and
confront them with expression data. We find approximate gene coexpression
patterns consistent with symmetry fibrations with idealized gene expression
dynamics. This shows that network structure alone provides useful information
about gene synchronization, and suggest that gene input functions within fibers
may be further streamlined by evolutionary pressures to realize a coexpression
of genes.
  Conclusions: Thus, gene fibrations provides a sound conceptual tool to
describe tunable coexpression induced by network topology and shaped by
mechanistic details of gene expression.
","['Ian Leifer', 'Mishael Sánchez-Pérez', 'Cecilia Ishida', 'Hernán A. Makse']"
http://arxiv.org/abs/0805.3598v2,Gene therapy,2008-05-23T09:19:26Z,2008-07-23T04:04:23Z,"Gene profiling for determining pluripotent genes in a time course
  microarray experiment","  In microarray experiments, it is often of interest to identify genes which
have a pre-specified gene expression profile with respect to time. Methods
available in the literature are, however, typically not stringent enough in
identifying such genes, particularly when the profile requires equivalence of
gene expression levels at certain time points. In this paper, the authors
introduce a new methodology, called gene profiling, that uses simultaneous
differential and equivalent gene expression level testing to rank genes
according to a pre-specified gene expression profile. Gene profiling treats the
vector of true gene expression levels as a linear combination of appropriate
vectors, i.e., vectors that give the required criteria for the profile. This
gene-profile model is fitted to the data and the resultant parameter estimates
are summarized in a single test statistic that is then used to rank the genes.
The theoretical underpinnings of gene profiling (equivalence testing,
intersection-union tests) are discussed in this paper, and the gene profiling
methodology is applied to our motivating stem cell experiment.
","['J. Tuke', 'G. F. V. Glonek', 'P. J. Solomon']"
http://arxiv.org/abs/1003.1204v1,Gene therapy,2010-03-05T08:28:06Z,2010-03-05T08:28:06Z,"From gene trees to species trees II: Species tree inference in the deep
  coalescence model","  When gene copies are sampled from various species, the resulting gene tree
might disagree with the containing species tree. The primary causes of gene
tree and species tree discord include lineage sorting, horizontal gene
transfer, and gene duplication and loss. Each of these events yields a
different parsimony criterion for inferring the (containing) species tree from
gene trees. With lineage sorting, species tree inference is to find the tree
minimizing extra gene lineages that had to coexist along species lineages; with
gene duplication, it becomes to find the tree minimizing gene duplications
and/or losses. In this paper, we show the following results: (i) The deep
coalescence cost is equal to the number of gene losses minus two times the gene
duplication cost in the reconciliation of a uniquely leaf labeled gene tree and
a species tree. The deep coalescence cost can be computed in linear time for
any arbitrary gene tree and species tree. (ii) The deep coalescence cost is
always no less than the gene duplication cost in the reconciliation of an
arbitrary gene tree and a species tree. (iii) Species tree inference by
minimizing deep coalescences is NP-hard.
",['Louxin Zhang']
http://arxiv.org/abs/1101.3474v1,Gene therapy,2011-01-18T15:13:30Z,2011-01-18T15:13:30Z,"Integration of Differential Gene-combination Search and Gene Set
  Enrichment Analysis: A General Approach","  Gene Set Enrichment Analysis (GSEA) and its variations aim to discover
collections of genes that show moderate but coordinated differences in
expression. However, such techniques may be ineffective if many individual
genes in a phenotype-related gene set have weak discriminative power. A
potential solution is to search for combinations of genes that are highly
differentiating even when individual genes are not. Although such techniques
have been developed, these approaches have not been used with GSEA to any
significant degree because of the large number of potential gene combinations
and the heterogeneity of measures that assess the differentiation provided by
gene groups of different sizes.
  To integrate the search for differentiating gene combinations and GSEA, we
propose a general framework with two key components: (A) a procedure that
reduces the number of scores to be handled by GSEA to the number of genes by
summarizing the scores of the gene combinations involving a particular gene in
a single score, and (B) a procedure to integrate the heterogeneous scores from
combinations of different sizes and from different gene combination measures by
mapping the scores to p-values. Experiments on four gene expression data sets
demonstrate that the integration of GSEA and gene combination search can
enhance the power of traditional GSEA by discovering gene sets that include
genes with weak individual differentiation but strong joint discriminative
power. Also, gene sets discovered by the integrative framework share several
common biological processes and improve the consistency of the results among
three lung cancer data sets.
","['Gang Fang', 'Michael Steinbach', 'Chad L. Myers', 'Vipin Kumar']"
http://arxiv.org/abs/1301.6547v2,Gene therapy,2013-01-28T14:05:20Z,2014-11-20T23:14:20Z,The infinitely many genes model with horizontal gene transfer,"  The genome of bacterial species is much more flexible than that of
eukaryotes. Moreover, the distributed genome hypothesis for bacteria states
that the total number of genes present in a bacterial population is greater
than the genome of every single individual. The pangenome, i.e. the set of all
genes of a bacterial species (or a sample), comprises the core genes which are
present in all living individuals, and accessory genes, which are carried only
by some individuals. In order to use accessory genes for adaptation to
environmental forces, genes can be transferred horizontally between
individuals. Here, we extend the infinitely many genes model from Baumdicker,
Hess and Pfaffelhuber (2010) for horizontal gene transfer. We take a
genealogical view and give a construction -- called the Ancestral Gene Transfer
Graph -- of the joint genealogy of all genes in the pangenome. As application,
we compute moments of several statistics (e.g. the number of differences
between two individuals and the gene frequency spectrum) under the infinitely
many genes model with horizontal gene transfer.
","['Franz Baumdicker', 'Peter Pfaffelhuber']"
http://arxiv.org/abs/2211.08096v1,Gene therapy,2022-11-15T12:27:26Z,2022-11-15T12:27:26Z,"Unveiling interpretable development-specific gene signatures in the
  developing human prefrontal cortex with ICGS","  In this paper, to unveil interpretable development-specific gene signatures
in human PFC, we propose a novel gene selection method, named Interpretable
Causality Gene Selection (ICGS), which adopts a Bayesian Network (BN) to
represent causality between multiple gene variables and a development variable.
The proposed ICGS method combines the positive instances-based contrastive
learning with a Variational AutoEncoder (VAE) to obtain this optimal BN
structure and use a Markov Blanket (MB) to identify gene signatures causally
related to the development variable. Moreover, the differential expression
genes (DEGs) are used to filter redundant genes before gene selection. In order
to identify gene signatures, we apply the proposed ICGS to the human PFC
single-cell transcriptomics data. The experimental results demonstrate that the
proposed method can effectively identify interpretable development-specific
gene signatures in human PFC. Gene ontology enrichment analysis and ASD-related
gene analysis show that these identified gene signatures reveal the key
biological processes and pathways in human PFC and have more potential for
neurodevelopment disorder cure. These gene signatures are expected to bring
important implications for understanding PFC development heterogeneity and
function in humans.
","['Meng Huang', 'Xiucai Ye', 'Tetsuya Sakurai']"
http://arxiv.org/abs/2310.03611v2,Gene therapy,2023-10-05T15:45:53Z,2023-10-06T11:53:50Z,"GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene
  Interactions From Gene Expression Data","  Detecting and discovering new gene interactions based on known gene
expressions and gene interaction data presents a significant challenge. Various
statistical and deep learning methods have attempted to tackle this challenge
by leveraging the topological structure of gene interactions and gene
expression patterns to predict novel gene interactions. In contrast, some
approaches have focused exclusively on utilizing gene expression profiles. In
this context, we introduce GENER, a parallel-layer deep learning network
designed exclusively for the identification of gene-gene relationships using
gene expression data. We conducted two training experiments and compared the
performance of our network with that of existing statistical and deep learning
approaches. Notably, our model achieved an average AUROC score of 0.834 on the
combined BioGRID&DREAM5 dataset, outperforming competing methods in predicting
gene-gene interactions.
","['Ahmed Fakhry', 'Raneem Khafagy', 'Adriaan-Alexander Ludl']"
http://arxiv.org/abs/q-bio/0609018v1,Gene therapy,2006-09-13T07:25:11Z,2006-09-13T07:25:11Z,"Gene cluster analysis method reliably identifies horizontally
  transferred genes and reveals their involvement in operon formation","  The formation mechanism of operons remains controversial despite the proposal
of many models. Although acquisition of genes from other species, horizontal
gene transfer, is considered to occur, definitive concrete cases have been
unavailable. It is desirable to select horizontally transferred genes reliably
and examine their relationship to operons. We here developed a method to
identify candidates of horizontally transferred genes based on minimization of
gene cluster insertions/deletions. To select a benchmark set of positively
horizontally transferred genes against which the candidate set can be
appraised, we devised another procedure using intergenetic alignments.
Comparison with the benchmark set of horizontally transferred genes
demonstrated the absence of a significant number of false positives in the
candidates, showing that the method identifies horizontally transferred genes
with a high degree of confidence. Horizontally transferred genes constitute at
least 5.5% of the genes in Escherichia, Shigella, and Salmonella and ~46% of
which originate from other gamma-proteobacteria. Not only informational genes,
but also operational genes (those involved in housekeeping) are horizontally
transferred less frequently than expected. A gene-cluster analysis of
Escherichia coli K-12 operons revealed that horizontal transfer produced four
entire operons and expanded two operons, but deletion of intervening genes
accounts for the formation of no operons. We propose that operons generally
form by horizontal gene transfer. We further suggest that genes with related
essential functions tend to reside in conserved operons, while genes in
nonconserved operons generally confer slight advantage to the organisms and
frequently undergo horizontal transfer and decay.
","['Keiichi Homma', 'Satoshi Fukuchi', 'Yoji Nakamura', 'Takashi Gojobori', 'Ken Nishikawa']"
http://arxiv.org/abs/1201.3995v2,Gene therapy,2012-01-19T09:32:51Z,2012-05-03T02:23:39Z,Reconciliation of Gene and Species Trees With Polytomies,"  Motivation: Millions of genes in the modern species belong to only thousands
of `gene families'. A gene family includes instances of the same gene in
different species (orthologs) and duplicate genes in the same species
(paralogs). Genes are gained and lost during evolution. With advances in
sequencing technology, researchers are able to investigate the important roles
of gene duplications and losses in adaptive evolution. Because of gene complex
evolution, ortholog identification is a basic but difficult task in comparative
genomics. A key method for the task is to use an explicit model of the
evolutionary history of the genes being studied, called the gene (family) tree.
It compares the gene tree with the evolutionary history of the species in which
the genes reside, called the species tree, using the procedure known as tree
reconciliation. Reconciling binary gene and specific trees is simple. However,
both gene and species trees may be non-binary in practice and thus tree
reconciliation presents challenging problems. Here, non-binary gene and species
tree reconciliation is studied in a binary refinement model.
  Results: The problem of reconciling arbitrary gene and species trees is
proved NP-hard even for the duplication cost. We then present the first
efficient method for reconciling a non-binary gene tree and a non-binary
species tree. It attempts to find binary refinements of the given gene and
species trees that minimize reconciliation cost. Our algorithms have been
implemented into a software to support quick automated analysis of large data
sets.
  Availability: The program, together with the source code, is available at its
online server http://phylotoo.appspot.com.
","['Yu Zheng', 'Taoyang Wu', 'Louxin Zhang']"
http://arxiv.org/abs/1901.04847v1,Gene therapy,2019-01-11T23:53:33Z,2019-01-11T23:53:33Z,"Determining Multifunctional Genes and Diseases in Human Using Gene
  Ontology","  The study of human genes and diseases is very rewarding and can lead to
improvements in healthcare, disease diagnostics and drug discovery. In this
paper, we further our previous study on gene disease relationship specifically
with the multifunctional genes. We investigate the multifunctional gene disease
relationship based on the published molecular function annotations of genes
from the Gene Ontology which is the most comprehensive source on gene
functions.
","['Hisham Al-Mubaid', 'Sasikanth Potu', 'M. Shenify']"
http://arxiv.org/abs/1301.3933v2,Gene therapy,2013-01-16T22:02:22Z,2013-01-18T01:38:48Z,Gene set bagging for estimating replicability of gene set analyses,"  Background: Significance analysis plays a major role in identifying and
ranking genes, transcription factor binding sites, DNA methylation regions, and
other high-throughput features for association with disease. We propose a new
approach, called gene set bagging, for measuring the stability of ranking
procedures using predefined gene sets. Gene set bagging involves resampling the
original high-throughput data, performing gene-set analysis on the resampled
data, and confirming that biological categories replicate. This procedure can
be thought of as bootstrapping gene-set analysis and can be used to determine
which are the most reproducible gene sets. Results: Here we apply this approach
to two common genomics applications: gene expression and DNA methylation. Even
with state-of-the-art statistical ranking procedures, significant categories in
a gene set enrichment analysis may be unstable when subjected to resampling.
Conclusions: We demonstrate that gene lists are not necessarily stable, and
therefore additional steps like gene set bagging can improve biological
inference of gene set analysis.
","['Andrew E. Jaffe', 'John D. Storey', 'Hongkai Ji', 'Jeffrey T. Leek']"
http://arxiv.org/abs/1512.08798v1,Gene therapy,2015-12-29T21:15:22Z,2015-12-29T21:15:22Z,"Evolutionary and topological properties of gene modules and driver
  mutations in a leukemia gene regulatory network","  The diverse, specialized genes in today's lifeforms evolved from a common
core of ancient, elementary genes. However, these genes did not evolve
individually: gene expression is controlled by a complex network of
interactions, and alterations in one gene may drive reciprocal changes in its
proteins' binding partners. We show that the topology of a leukemia gene
regulatory network is strongly coupled with evolutionary properties.
Slowly-evolving (""cold""), old genes tend to interact with each other, as do
rapidly-evolving (""hot""), young genes, causing genes to evolve in clusters. We
argue that gene duplication placed old, cold genes at the center of the
network, and young, hot genes on the periphery, and demonstrate this with
single-node centrality measures and two new measures of efficiency. Integrating
centrality measures with evolutionary information, we define a
medically-relevant ""cancer network core,"" strongly enriched for common cancer
mutations ($p=2\times 10^{-14}$). This could aid in identifying driver
mutations and therapeutic targets.
","['Anthony Szedlak', 'Nicholas Smith', 'Li Liu', 'Giovanni Paternostro', 'Carlo Piermarocchi']"
http://arxiv.org/abs/2106.13642v1,Gene therapy,2021-06-25T13:51:46Z,2021-06-25T13:51:46Z,VEGN: Variant Effect Prediction with Graph Neural Networks,"  Genetic mutations can cause disease by disrupting normal gene function.
Identifying the disease-causing mutations from millions of genetic variants
within an individual patient is a challenging problem. Computational methods
which can prioritize disease-causing mutations have, therefore, enormous
applications. It is well-known that genes function through a complex regulatory
network. However, existing variant effect prediction models only consider a
variant in isolation. In contrast, we propose VEGN, which models variant effect
prediction using a graph neural network (GNN) that operates on a heterogeneous
graph with genes and variants. The graph is created by assigning variants to
genes and connecting genes with an gene-gene interaction network. In this
context, we explore an approach where a gene-gene graph is given and another
where VEGN learns the gene-gene graph and therefore operates both on given and
learnt edges. The graph neural network is trained to aggregate information
between genes, and between genes and variants. Variants can exchange
information via the genes they connect to. This approach improves the
performance of existing state-of-the-art models.
","['Jun Cheng', 'Carolin Lawrence', 'Mathias Niepert']"
http://arxiv.org/abs/2310.02275v1,Gene therapy,2023-09-29T13:33:53Z,2023-09-29T13:33:53Z,"MuSe-GNN: Learning Unified Gene Representation From Multimodal
  Biological Graph Data","  Discovering genes with similar functions across diverse biomedical contexts
poses a significant challenge in gene representation learning due to data
heterogeneity. In this study, we resolve this problem by introducing a novel
model called Multimodal Similarity Learning Graph Neural Network, which
combines Multimodal Machine Learning and Deep Graph Neural Networks to learn
gene representations from single-cell sequencing and spatial transcriptomic
data. Leveraging 82 training datasets from 10 tissues, three sequencing
techniques, and three species, we create informative graph structures for model
training and gene representations generation, while incorporating
regularization with weighted similarity learning and contrastive learning to
learn cross-data gene-gene relationships. This novel design ensures that we can
offer gene representations containing functional similarity across different
contexts in a joint space. Comprehensive benchmarking analysis shows our
model's capacity to effectively capture gene function similarity across
multiple modalities, outperforming state-of-the-art methods in gene
representation learning by up to 97.5%. Moreover, we employ bioinformatics
tools in conjunction with gene representations to uncover pathway enrichment,
regulation causal networks, and functions of disease-associated or
dosage-sensitive genes. Therefore, our model efficiently produces unified gene
representations for the analysis of gene functions, tissue functions, diseases,
and species evolution.
","['Tianyu Liu', 'Yuge Wang', 'Rex Ying', 'Hongyu Zhao']"
http://arxiv.org/abs/1501.00302v2,Gene therapy,2015-01-01T19:47:22Z,2015-02-24T23:25:39Z,An Event-Driven Approach for Studying Gene Block Evolution in Bacteria,"  Motivation: Gene blocks are genes co-located on the chromosome. In many
cases, genes blocks are conserved between bacterial species, sometimes as
operons, when genes are co-transcribed. The conservation is rarely absolute:
gene loss, gain, duplication, block splitting, and block fusion are frequently
observed. An open question in bacterial molecular evolution is that of the
formation and breakup of gene blocks, for which several models have been
proposed. These models, however, are not generally applicable to all types of
gene blocks, and consequently cannot be used to broadly compare and study gene
block evolution. To address this problem we introduce an event-based method for
tracking gene block evolution in bacteria. Results: We show here that the
evolution of gene blocks in proteobacteria can be described by a small set of
events. Those include the insertion of genes into, or the splitting of genes
out of a gene block, gene loss, and gene duplication. We show how the
event-based method of gene block evolution allows us to determine the
evolutionary rate, and to trace the ancestral states of their formation. We
conclude that the event-based method can be used to help us understand the
formation of these important bacterial genomic structures. Availability: The
software is available under GPLv3 license on
http://github.com/reamdc1/gene_block_evolution.git Supplementary online
material: http://iddo-friedberg.net/operon-evolution Contact: Iddo Friedberg
i.friedberg@miamioh.edu
","['David C Ream', 'Asma R Bankapur', 'Iddo Friedberg']"
http://arxiv.org/abs/2411.18391v1,Gene therapy,2024-11-27T14:33:13Z,2024-11-27T14:33:13Z,"GeneQuery: A General QA-based Framework for Spatial Gene Expression
  Predictions from Histology Images","  Gene expression profiling provides profound insights into molecular
mechanisms, but its time-consuming and costly nature often presents significant
challenges. In contrast, whole-slide hematoxylin and eosin (H&E) stained
histological images are readily accessible and allow for detailed examinations
of tissue structure and composition at the microscopic level. Recent
advancements have utilized these histological images to predict spatially
resolved gene expression profiles. However, state-of-the-art works treat gene
expression prediction as a multi-output regression problem, where each gene is
learned independently with its own weights, failing to capture the shared
dependencies and co-expression patterns between genes. Besides, existing works
can only predict gene expression values for genes seen during training,
limiting their ability to generalize to new, unseen genes.
  To address the above limitations, this paper presents GeneQuery, which aims
to solve this gene expression prediction task in a question-answering (QA)
manner for better generality and flexibility. Specifically, GeneQuery takes
gene-related texts as queries and whole-slide images as contexts and then
predicts the queried gene expression values. With such a transformation,
GeneQuery can implicitly estimate the gene distribution by introducing the gene
random variable. Besides, the proposed GeneQuery consists of two architecture
implementations, i.e., spot-aware GeneQuery for capturing patterns between
images and gene-aware GeneQuery for capturing patterns between genes.
Comprehensive experiments on spatial transcriptomics datasets show that the
proposed GeneQuery outperforms existing state-of-the-art methods on known and
unseen genes. More results also demonstrate that GeneQuery can potentially
analyze the tissue structure.
","['Ying Xiong', 'Linjing Liu', 'Yufei Cui', 'Shangyu Wu', 'Xue Liu', 'Antoni B. Chan', 'Chun Jason Xue']"
http://arxiv.org/abs/1105.1217v1,Gene therapy,2011-05-06T02:56:45Z,2011-05-06T02:56:45Z,"Marker Genes for Anatomical Regions in the Brain: Insights from the
  Allen Gene Expression Atlas","  Quantitative criteria are proposed to identify genes (and sets of genes)
whose expression marks a specific brain region (or a set of brain regions).
Gene-expression energies, obtained for thousands of mouse genes by numerization
of in-situ hybridization images in the Allen Gene Expression Atlas, are used to
test these methods in the mouse brain. Individual genes are ranked using
integrals of their expression energies across brain regions. The ranking is
generalized to sets of genes and the problem of optimal markers of a classical
region receives a linear-algebraic solution. Moreover, the goodness of the
fitting of the expression profile of a gene to the profile of a brain region is
closely related to the co-expression of genes. The geometric interpretation of
this fact leads to a quantitative criterion to detect markers of pairs of brain
regions. Local properties of the gene-expression profiles are also used to
detect genes that separate a given grain region from its environment.
","['Pascal Grange', 'Partha P. Mitra']"
http://arxiv.org/abs/2204.10473v1,Gene therapy,2022-04-22T02:54:01Z,2022-04-22T02:54:01Z,"Gene Function Prediction with Gene Interaction Networks: A Context Graph
  Kernel Approach","  Predicting gene functions is a challenge for biologists in the post genomic
era. Interactions among genes and their products compose networks that can be
used to infer gene functions. Most previous studies adopt a linkage assumption,
i.e., they assume that gene interactions indicate functional similarities
between connected genes. In this study, we propose to use a gene's context
graph, i.e., the gene interaction network associated with the focal gene, to
infer its functions. In a kernel-based machine-learning framework, we design a
context graph kernel to capture the information in context graphs. Our
experimental study on a testbed of p53-related genes demonstrates the advantage
of using indirect gene interactions and shows the empirical superiority of the
proposed approach over linkage-assumption-based methods, such as the algorithm
to minimize inconsistent connected genes and diffusion kernels.
","['Xin Li', 'Hsinchun Chen', 'Jiexun Li', 'Zhu Zhang']"
http://arxiv.org/abs/2412.12688v1,Gene therapy,2024-12-17T09:08:52Z,2024-12-17T09:08:52Z,"UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation
  Benchmarks with Unified Entrez Gene Identifiers","  Gene studies are crucial for fields such as protein structure prediction,
drug discovery, and cancer genomics, yet they face challenges in fully
utilizing the vast and diverse information available. Gene studies require
clean, factual datasets to ensure reliable results. Ontology graphs, neatly
organized domain terminology graphs, provide ideal sources for domain facts.
However, available gene ontology annotations are currently distributed across
various databases without unified identifiers for genes and gene products. To
address these challenges, we introduce Unified Entrez Gene Identifier Dataset
and Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale
public Gene Ontology Annotations (GOA) from various databases using unique gene
identifiers. UniEntrezDB includes a pre-training dataset and four downstream
tasks designed to comprehensively evaluate gene embedding performance from
gene, protein, and cell levels, ultimately enhancing the reliability and
applicability of LLMs in gene research and other professional settings.
","['Yuwei Miao', 'Yuzhi Guo', 'Hehuan Ma', 'Jingquan Yan', 'Feng Jiang', 'Weizhi An', 'Jean Gao', 'Junzhou Huang']"
http://arxiv.org/abs/q-bio/0509037v1,Gene therapy,2005-09-27T14:17:53Z,2005-09-27T14:17:53Z,"A probabilistic model for gene content evolution with duplication, loss,
  and horizontal transfer","  We introduce a Markov model for the evolution of a gene family along a
phylogeny. The model includes parameters for the rates of horizontal gene
transfer, gene duplication, and gene loss, in addition to branch lengths in the
phylogeny. The likelihood for the changes in the size of a gene family across
different organisms can be calculated in O(N+hM^2) time and O(N+M^2) space,
where N is the number of organisms, $h$ is the height of the phylogeny, and M
is the sum of family sizes. We apply the model to the evolution of gene content
in Preoteobacteria using the gene families in the COG (Clusters of Orthologous
Groups) database.
","['Miklós Csűrös', 'István Miklós']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/1705.02042v2,Neurotechnology,2017-05-04T22:54:54Z,2017-07-13T14:37:34Z,Exponential scaling of neural algorithms - a future beyond Moore's Law?,"  Although the brain has long been considered a potential inspiration for
future computing, Moore's Law - the scaling property that has seen revolutions
in technologies ranging from supercomputers to smart phones - has largely been
driven by advances in materials science. As the ability to miniaturize
transistors is coming to an end, there is increasing attention on new
approaches to computation, including renewed enthusiasm around the potential of
neural computation. This paper describes how recent advances in
neurotechnologies, many of which have been aided by computing's rapid
progression over recent decades, are now reigniting this opportunity to bring
neural computation insights into broader computing applications. As we
understand more about the brain, our ability to motivate new computing
paradigms with continue to progress. These new approaches to computing, which
we are already seeing in techniques such as deep learning and neuromorphic
hardware, will themselves improve our ability to learn about the brain and
accordingly can be projected to give rise to even further insights. This paper
will describe how this positive feedback has the potential to change the
complexion of how computing sciences and neurosciences interact, and suggests
that the next form of exponential scaling in computing may emerge from our
progressive understanding of the brain.
",['James B. Aimone']
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2206.06276v1,Reusable launch vehicle,2022-06-13T16:03:35Z,2022-06-13T16:03:35Z,On the reusability of samples in active learning,"  An interesting but not extensively studied question in active learning is
that of sample reusability: to what extent can samples selected for one learner
be reused by another? This paper explains why sample reusability is of
practical interest, why reusability can be a problem, how reusability could be
improved by importance-weighted active learning, and which obstacles to
universal reusability remain. With theoretical arguments and practical
demonstrations, this paper argues that universal reusability is impossible.
Because every active learning strategy must undersample some areas of the
sample space, learners that depend on the samples in those areas will learn
more from a random sample selection. This paper describes several experiments
with importance-weighted active learning that show the impact of the
reusability problem in practice. The experiments confirmed that universal
reusability does not exist, although in some cases -- on some datasets and with
some pairs of classifiers -- there is sample reusability. Finally, this paper
explores the conditions that could guarantee the reusability between two
classifiers.
","['Gijs van Tulder', 'Marco Loog']"
http://arxiv.org/abs/1409.2223v1,Reusable launch vehicle,2014-09-08T07:10:37Z,2014-09-08T07:10:37Z,"Assessment of classification techniques on predicting success or failure
  of Software reusability","  Assessment of classification techniques on predicting success or failure of
Software reusability
","['Nahid Hajizadeh', 'Manijeh Keshtgari', 'Marzieh Ahmadzadeh']"
http://arxiv.org/abs/1205.0289v2,Reusable launch vehicle,2012-05-01T23:47:56Z,2012-05-22T21:53:00Z,On the Power of Reusable Magic States,"  In this paper we study reusable magic states. These states are a special
subset of the standard magic states. Once distilled, reusable magic states can
be used, repeatedly, to apply some unitary U. Given this property, reusable
magic states have the potential to greatly lower qubit and gate overheads in
fault-tolerant quantum computation. While these states are promising, we
provide a strong argument for their limited computational power. Specifically,
we show that if reusable magic states can be used to apply non-Clifford
unitaries, then we can exploit them to efficiently simulate poly-sized quantum
circuits on a classical computer.
",['Jonas T. Anderson']
http://arxiv.org/abs/2304.03377v1,Reusable launch vehicle,2023-04-06T21:13:53Z,2023-04-06T21:13:53Z,"Leveraging Reusability: Improved Competitive Ratio of Greedy for
  Reusable Resources","  We study online weighted bipartite matching of reusable resources where an
adversarial sequence of requests for resources arrive over time. A resource
that is matched is 'used' for a random duration, drawn independently from a
resource-dependent distribution, after which it returns and is able to be
matched again. We study the performance of the greedy policy, which matches
requests to the resource that yields the highest reward. Previously, it was
known that the greedy policy is 1/2 competitive against a clairvoyant benchmark
that knows the request sequence in advance. In this work, we improve this
result by introducing a parameter that quantifies the degree of reusability of
the resources. Specifically, if p represents the smallest probability over the
usage distributions that a matched resource returns in one time step, the
greedy policy achieves a competitive ratio of $1/(2-p)$. Furthermore, when the
usage distributions are geometric, we establish a stronger competitive ratio of
$(1+p)/2$, which we demonstrate to be tight. Both of these results align with
the known results in the two extreme scenarios: p = 0 corresponds to
non-reusable resources, where 1/2 is known to be tight, while p = 1 corresponds
to every resource returning immediately, where greedy is the optimal policy and
hence the competitive ratio is 1. Finally, we show that both results are robust
to approximations of the greedy policy. Our work demonstrates that the
reusability of resources can enhance performance compared to the non-reusable
setting, and that a simple greedy policy suffices when the degree of
reusability is high. Our insights contribute to the understanding of how
resource reusability can influence the performance of online algorithms, and
highlight the potential for improved performance as the degree of reusability
increases.
","['Jackie Baek', 'Shixin Wang']"
http://arxiv.org/abs/1210.8011v1,Reusable launch vehicle,2012-10-30T13:57:19Z,2012-10-30T13:57:19Z,Reusability Framework for Cloud Computing,"  Cloud based development is a challenging task for several software
engineering projects, especially for those which needs development with
reusability. Present time of cloud computing is allowing new professional
models for using the software development. The expected upcoming trend of
computing is assumed to be this cloud computing because of speed of application
deployment, shorter time to market, and lower cost of operation. Until Cloud Co
mputing Reusability Model is considered a fundamental capability, the speed of
developing services is very slow. Th is paper spreads cloud computing with
component based development named Cloud Co mputing Reusability Model (CCR) and
enable reusability in cloud computing. In this paper Cloud Co mputing
Reusability Model has been proposed. The model has been validated by Cloudsim
an d experimental result shows that reusability based cloud computing approach
is effective in minimizing cost and time to market.
","['Sukhpal Singh', 'Rishideep Singh']"
http://arxiv.org/abs/2111.07002v1,Reusable launch vehicle,2021-11-13T00:19:03Z,2021-11-13T00:19:03Z,Refactoring for Reuse: An Empirical Study,"  Refactoring is the de-facto practice to optimize software health. While
several studies propose refactoring strategies to optimize software design
through applying design patterns and removing design defects, little is known
about how developers actually refactor their code to improve its reuse.
Therefore, we extract, from 1,828 open-source projects, a set of refactorings
that were intended to improve the software reusability. We analyze the impact
of reusability refactorings on the state-of-the-art reusability metrics, and we
compare the distribution of reusability refactoring types, with the
distribution of the remaining mainstream refactorings. Overall, we found that
the distribution of refactoring types, applied in the context of reusability,
is different from the distribution of refactoring types in mainstream
development. In the refactorings performed to improve reusability, source files
are subject to more design-level types of refactorings. Reusability
refactorings significantly impact, high-level code elements, such as packages,
classes, and methods, while typical refactorings, impact all code elements,
including identifiers, and parameters. These findings provide practical
insights into the current practice of refactoring in the context of code reuse
involving the act of refactoring.
","['Eman Abdullah AlOmar', 'Tianjia Wang', 'Vaibhavi Raut', 'Mohamed Wiem Mkaouer', 'Christian Newman', 'Ali Ouni']"
http://arxiv.org/abs/2309.07291v1,Reusable launch vehicle,2023-09-13T20:17:43Z,2023-09-13T20:17:43Z,Reusability Challenges of Scientific Workflows: A Case Study for Galaxy,"  Scientific workflow has become essential in software engineering because it
provides a structured approach to designing, executing, and analyzing
scientific experiments. Software developers and researchers have developed
hundreds of scientific workflow management systems so scientists in various
domains can benefit from them by automating repetitive tasks, enhancing
collaboration, and ensuring the reproducibility of their results. However, even
for expert users, workflow creation is a complex task due to the dramatic
growth of tools and data heterogeneity. Thus, scientists attempt to reuse
existing workflows shared in workflow repositories. Unfortunately, several
challenges prevent scientists from reusing those workflows. In this study, we
thus first attempted to identify those reusability challenges. We also offered
an action list and evidence-based guidelines to promote the reusability of
scientific workflows. Our intensive manual investigation examined the
reusability of existing workflows and exposed several challenges. The
challenges preventing reusability include tool upgrading, tool support
unavailability, design flaws, incomplete workflows, failure to load a workflow,
etc. Such challenges and our action list offered guidelines to future workflow
composers to create better workflows with enhanced reusability. In the future,
we plan to develop a recommender system using reusable workflows that can
assist scientists in creating effective and error-free workflows.
","['Khairul Alam', 'Banani Roy', 'Alexander Serebrenik']"
http://arxiv.org/abs/2405.04021v1,Reusable launch vehicle,2024-05-07T05:48:02Z,2024-05-07T05:48:02Z,"Robust and Reusable Fuzzy Extractors for Low-entropy Rate Randomness
  Sources","  Fuzzy extractors (FE) are cryptographic primitives that extract reliable
cryptographic key from noisy real world random sources such as biometric
sources. The FE generation algorithm takes a source sample, extracts a key and
generates some helper data that will be used by the reproduction algorithm to
recover the key. Reusability of FE guarantees that security holds when FE is
used multiple times with the same source, and robustness of FE requires
tampering with the helper data be detectable.
  In this paper, we consider information theoretic FEs, define a strong notion
of reusability, and propose strongly robust and reusable FEs (srrFE) that
provides the strongest combined notion of reusability and robustness for FEs.
We give two constructions, one for reusable FEs and one for srrFE with
information theoretic (IT) security for structured sources. The constructions
are for structured sources and use sample-then-lock approach. We discuss each
construction and show their unique properties in relation to existing work.
  Construction 2 is the first robust and reusable FE with IT-security without
assuming random oracle. The robustness is achieved by using an IT-secure MAC
with security against key-shift attack, which can be of independent interest.
","['Somnath Panja', 'Shaoquan Jiang', 'Reihaneh Safavi-Naini']"
http://arxiv.org/abs/1903.04165v2,Reusable launch vehicle,2019-03-11T08:18:32Z,2019-03-16T09:16:07Z,"Object-oriented requirements: reusable, understandable, verifiable","  Insufficient requirements reusability, understandability and verifiability
jeopardize software projects. Empirical studies show little success in
improving these qualities separately. Applying object-oriented thinking to
requirements leads to their unified treatment. An online library of reusable
requirement templates implements recurring requirement structures, offering a
starting point for practicing the unified approach.
",['Alexandr Naumchev']
http://arxiv.org/abs/1007.5123v1,Reusable launch vehicle,2010-07-29T06:53:20Z,2010-07-29T06:53:20Z,"Building Reusable Software Component For Optimization Check in ABAP
  Coding","  Software component reuse is the software engineering practice of developing
new software products from existing components. A reuse library or component
reuse repository organizes stores and manages reusable components. This paper
describes how a reusable component is created, how it reuses the function and
checking if optimized code is being used in building programs and applications.
Finally providing coding guidelines, standards and best practices used for
creating reusable components and guidelines and best practices for making
configurable and easy to use.
","['P. Shireesha', 'S. S. V. N. Sharma']"
http://arxiv.org/abs/1207.1173v1,Reusable launch vehicle,2012-07-05T07:22:15Z,2012-07-05T07:22:15Z,"A Comprehensive Model to achieve Service Reusability for Multi level
  stakeholders using Non-Functional attributes of Service Oriented Architecture","  SOA is a prominent paradigm for accomplishing reuse of services. Service
reusability is one dominant factor which has a greater influence on achieving
quality in SOA systems. There exists sufficient research in this area and
researchers have contributed many works towards achieving quality in SOA
systems but much emphasis was not provided on service reusability [1] [2] [3].
Few authors have addressed reusability factor with limited non-functional
attributes. Our study focuses on identifying the non-functional attributes
which have major or greater influence towards obtaining reusability in SOA
systems. The objective of this study goes into the next level, to categorize
the non-functional attributes on multi stakeholder's perspective i.e. Service
Consumer, Service Provider and Service Developer which paves the way to build a
comprehensive quality model for achieving Service Reusability
","['Shanmugasundaram G.', 'V. Prasanna Venkatesan', 'C. Punitha Devi']"
http://arxiv.org/abs/2303.14959v1,Reusable launch vehicle,2023-03-27T07:45:14Z,2023-03-27T07:45:14Z,Some Initial Guidelines for Building Reusable Quantum Oracles,"  The evolution of quantum hardware is highlighting the need for advances in
quantum software engineering that help developers create quantum software with
good quality attributes. Specifically, reusability has been traditionally
considered an important quality attribute in terms of efficiency of cost and
effort. Increasing the reusability of quantum software will help developers
create more complex solutions, by reusing simpler components, with better
quality attributes, as long as the reused components have also these
attributes. This work focuses on the reusability of oracles, a well-known
pattern of quantum algorithms that can be used to perform functions used as
input by other algorithms. In particular, in this work, we present several
guidelines for making reusable quantum oracles. These guidelines include three
different levels for oracle reuse: the ideas inspiring the oracle, the function
which creates the oracle, and the oracle itself. To demonstrate these
guidelines, two different implementations of a range of integers oracle have
been built by reusing simpler oracles. The quality of these implementations is
evaluated in terms of functionality and quantum circuit depth. Then, we provide
an example of documentation following the proposed guidelines for both
implementations to foster reuse of the provided oracles. This work aims to be a
first point of discussion towards quantum software reusability. Additional work
is needed to establish more specific criteria for quantum software reusability.
","['Javier Sanchez-Rivero', 'Daniel Talaván', 'Jose Garcia-Alonso', 'Antonio Ruiz-Cortés', 'Juan Manuel Murillo']"
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/1411.1102v1,Reusable launch vehicle,2014-11-04T22:48:59Z,2014-11-04T22:48:59Z,"Enhancing software module reusability using port plug-ins: an experiment
  with the iCub robot","  Systematically developing high--quality reusable software components is a
difficult task and requires careful design to find a proper balance between
potential reuse, functionalities and ease of implementation. Extendibility is
an important property for software which helps to reduce cost of development
and significantly boosts its reusability. This work introduces an approach to
enhance components reusability by extending their functionalities using
plug-ins at the level of the connection points (ports). Application--dependent
functionalities such as data monitoring and arbitration can be implemented
using a conventional scripting language and plugged into the ports of
components. The main advantage of our approach is that it avoids to introduce
application--dependent modifications to existing components, thus reducing
development time and fostering the development of simpler and therefore more
reusable components. Another advantage of our approach is that it reduces
communication and deployment overheads as extra functionalities can be added
without introducing additional modules.
","['Ali Paikan', 'Vadim Tikhanoff', 'Giorgio Metta', 'Lorenzo Natale']"
http://arxiv.org/abs/1608.00466v2,Reusable launch vehicle,2016-08-01T15:14:08Z,2016-10-10T03:57:26Z,"Learning Semantically Coherent and Reusable Kernels in Convolution
  Neural Nets for Sentence Classification","  The state-of-the-art CNN models give good performance on sentence
classification tasks. The purpose of this work is to empirically study
desirable properties such as semantic coherence, attention mechanism and
reusability of CNNs in these tasks. Semantically coherent kernels are
preferable as they are a lot more interpretable for explaining the decision of
the learned CNN model. We observe that the learned kernels do not have semantic
coherence. Motivated by this observation, we propose to learn kernels with
semantic coherence using clustering scheme combined with Word2Vec
representation and domain knowledge such as SentiWordNet. We suggest a
technique to visualize attention mechanism of CNNs for decision explanation
purpose. Reusable property enables kernels learned on one problem to be used in
another problem. This helps in efficient learning as only a few additional
domain specific filters may have to be learned. We demonstrate the efficacy of
our core ideas of learning semantically coherent kernels and leveraging
reusable kernels for efficient learning on several benchmark datasets.
Experimental results show the usefulness of our approach by achieving
performance close to the state-of-the-art methods but with semantic and
reusable properties.
","['Madhusudan Lakshmana', 'Sundararajan Sellamanickam', 'Shirish Shevade', 'Keerthi Selvaraj']"
http://arxiv.org/abs/2010.02756v2,Reusable launch vehicle,2020-10-06T14:21:05Z,2023-05-31T04:06:15Z,Learning Diverse Options via InfoMax Termination Critic,"  We consider the problem of autonomously learning reusable temporally extended
actions, or options, in reinforcement learning. While options can speed up
transfer learning by serving as reusable building blocks, learning reusable
options for unknown task distribution remains challenging. Motivated by the
recent success of mutual information (MI) based skill learning, we hypothesize
that more diverse options are more reusable. To this end, we propose a method
for learning termination conditions of options by maximizing MI between options
and corresponding state transitions. We derive a scalable approximation of this
MI maximization via gradient ascent, yielding the InfoMax Termination Critic
(IMTC) algorithm. Our experiments demonstrate that IMTC significantly improves
the diversity of learned options without extrinsic rewards combined with an
intrinsic option learning method. Moreover, we test the reusability of learned
options by transferring options into various tasks, confirming that IMTC helps
quick adaptation, especially in complex domains where an agent needs to
manipulate objects.
","['Yuji Kanagawa', 'Tomoyuki Kaneko']"
http://arxiv.org/abs/2309.15175v1,Reusable launch vehicle,2023-09-26T18:19:38Z,2023-09-26T18:19:38Z,"Large scale reuse of microservices using DevOps and InnerSource
  practices -- A longitudinal case study","  Contemporary practices such as InnerSource and DevOps promote software reuse.
This study investigates the implications of using contemporary practices on
software reuse. In particular, we investigate the costs, benefits, challenges,
and potential improvements in contemporary reuse at Ericsson. We performed the
study in two phases: a) the initial data collection based on a combination of
data collection methods (e.g., interviews, discussions, company portals), and
b) a follow-up group discussion after a year to understand the status of the
challenges and improvements identified in the first phase. Our results indicate
that developing reusable assets resulted in upfront costs, such as additional
effort in ensuring compliance. Furthermore, development with reuse also
resulted in additional effort, for example, in integrating and understanding
reusable assets. Ericsson perceived the additional effort as an investment
resulting in long-term benefits such as improved quality, productivity,
customer experience, and way of working. Ericsson's main challenge was
increased pressure on the producers of reusable assets, which was mitigated by
scaling the InnerSource adoption. InnerSource success is evident from the
increase in the contributions to reusable assets. In addition, Ericsson
implemented measures such as automating the compliance check, which enhanced
the maturity of reusable assets and resulted in increased reuse.
","['Deepika Badampudi', 'Muhammad Usman', 'Xingru Chen']"
http://arxiv.org/abs/2403.00787v1,Reusable launch vehicle,2024-02-19T23:40:46Z,2024-02-19T23:40:46Z,"Reusable MLOps: Reusable Deployment, Reusable Infrastructure and
  Hot-Swappable Machine Learning models and services","  Although Machine Learning model building has become increasingly accessible
due to a plethora of tools, libraries and algorithms being available freely,
easy operationalization of these models is still a problem. It requires
considerable expertise in data engineering, software development, cloud and
DevOps. It also requires planning, agreement, and vision of how the model is
going to be used by the business applications once it is in production, how it
is going to be continuously trained on fresh incoming data, and how and when a
newer model would replace an existing model. This leads to developers and data
scientists working in silos and making suboptimal decisions. It also leads to
wasted time and effort. We introduce the Acumos AI platform we developed and we
demonstrate some unique novel capabilities that the Acumos model runner
possesses, that can help solve the above problems. We introduce a new
sustainable concept in the field of AI/ML operations - called Reusable MLOps -
where we reuse the existing deployment and infrastructure to serve new models
by hot-swapping them without tearing down the infrastructure or the
microservice, thus achieving reusable deployment and operations for AI/ML
models while still having continuously trained models in production.
","['D Panchal', 'P Verma', 'I Baran', 'D Musgrove', 'D Lu']"
http://arxiv.org/abs/1003.5777v1,Reusable launch vehicle,2010-03-30T09:57:26Z,2010-03-30T09:57:26Z,Specifying Reusable Components,"  Reusable software components need expressive specifications. This paper
outlines a rigorous foundation to model-based contracts, a method to equip
classes with strong contracts that support accurate design, implementation, and
formal verification of reusable components. Model-based contracts
conservatively extend the classic Design by Contract with a notion of model,
which underpins the precise definitions of such concepts as abstract
equivalence and specification completeness. Experiments applying model-based
contracts to libraries of data structures suggest that the method enables
accurate specification of practical software.
","['Nadia Polikarpova', 'Carlo A. Furia', 'Bertrand Meyer']"
http://arxiv.org/abs/1202.5609v1,Reusable launch vehicle,2012-02-25T05:23:21Z,2012-02-25T05:23:21Z,A Framework Studio for Component Reusability,"  The deployment of a software product requires considerable amount of time and
effort. In order to increase the productivity of the software products,
reusability strategies were proposed in the literature. However effective reuse
is still a challenging issue. This paper presents a framework studio for
effective components reusability which provides the selection of components
from framework studio and generation of source code based on stakeholders
needs. The framework studio is implemented using swings which are integrated
onto the Net Beans IDE which help in faster generation of the source code.
","['N Md Jubair Basha', 'Salman Abdul Moiz']"
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2202.08118v1,Smart contracts,2022-02-16T14:53:18Z,2022-02-16T14:53:18Z,"Smart Cities, Smart Libraries and Smart Knowledge Managers: Ushering in
  the neo-Knowledge Society","  The emergence of smart cities as a specific concept is not very old. In
simple terms, it refers to cities which are sustainable and driven
predominantly by their Information and Communication Technology (ICT)
infrastructure. Smart libraries and smart knowledge managers, alongside its
other smart component-entities, are vital for their emergence, sustenance and
progress. The paper attempts at deducing a symbiosis amongst smart cities,
smart libraries and smart knowledge managers. It further elaborates on how
these will usher in the neo-knowledge society, and the opportunities it'll
offer vis-\`a-vis Library and Information Science (LIS). Finally, it concludes
on an optimistic note, mentioning possible future research activities in this
regard.
",['Mayukh Bagchi']
http://arxiv.org/abs/2309.12344v1,Smart contracts,2023-08-25T19:23:33Z,2023-08-25T19:23:33Z,"Exploring IoT in Smart Cities: Practices, Challenges and Way Forward","  The rise of Internet of things (IoT) technology has revolutionized urban
living, offering immense potential for smart cities in which smart home, smart
infrastructure, and smart industry are essential aspects that contribute to the
development of intelligent urban ecosystems. The integration of smart home
technology raises concerns regarding data privacy and security, while smart
infrastructure implementation demands robust networking and interoperability
solutions. Simultaneously, deploying IoT in industrial settings faces
challenges related to scalability, standardization, and data management. This
research paper offers a systematic literature review of published research in
the field of IoT in smart cities including 55 relevant primary studies that
have been published in reputable journals and conferences. This extensive
literature review explores and evaluates various aspects of smart home, smart
infrastructure, and smart industry and the challenges like security and
privacy, smart sensors, interoperability and standardization. We provide a
unified perspective, as we seek to enhance the efficiency and effectiveness of
smart cities while overcoming security concerns. It then explores their
potential for collective integration and impact on the development of smart
cities. Furthermore, this study addresses the challenges associated with each
component individually and explores their combined impact on enhancing urban
efficiency and sustainability. Through a comprehensive analysis of security
concerns, this research successfully integrates these IoT components in a
unified approach, presenting a holistic framework for building smart cities of
the future. Integrating smart home, smart infrastructure, and smart industry,
this research highlights the significance of an integrated approach in
developing smart cities.
","['Kashif Ishaq', 'Syed Shah Farooq']"
http://arxiv.org/abs/1807.08165v1,Smart contracts,2018-07-21T14:57:49Z,2018-07-21T14:57:49Z,"On Computational Infraestruture Requirements to Smart and Autonomic
  Cities Framework","  Smart cities are an actual trend being pursued by research that,
fundamentally, tries to improve city's management on behalf of a better human
quality of live. This paper proposes a new autonomic complementary approach for
smart cities management. It is argued that smart city management systems with
autonomic characteristics will improve and facilitate management
functionalities in general. A framework is also presented as use case
considering specific application scenarios like smart-health, smart-grid,
smart-environment and smart-streets.
","['Romildo Bezerra', 'Flavia Nascimento', 'Joberto Martins']"
http://arxiv.org/abs/1804.01242v1,Smart contracts,2018-04-04T05:27:09Z,2018-04-04T05:27:09Z,A Smart Home Gateway Platform for Data Collection and Awareness,"  Smart homes have attracted much attention due to the expanding of
Internet-of-Things (IoT) and smart devices. In this paper, we propose a smart
gateway platform for data collection and awareness in smart home networks. A
smart gateway will replace the traditional network gateway to connect the home
network and the Internet. A smart home network supports different types of
smart devices, such as in home IoT devices, smart phones, smart electric
appliances, etc. A traditional network gateway is not capable of providing
quality-of-service measurement, user behavioral analytics, or network
optimization. Such tasks are traditionally performed with measurement agents
such as optical splitters or network probes deployed in the core network. Our
proposed platform is a lightweight plug-in for the smart gateway to accomplish
data collection, awareness and reporting. While the smart gateway is able to
adjust the control policy for data collection and awareness locally, a
cloud-based controller is also included for more refined control policy
updates. Furthermore, we propose a multi-dimensional awareness framework to
achieve accurate data awareness at the smart gateway. The efficiency of data
collection and accuracy of data awareness of the proposed platform is
demonstrated based on the tests using actual data traffic from a large number
of smart home users.
","['Pan Wang', 'Feng Ye', 'Xuejiao Chen']"
http://arxiv.org/abs/2209.10300v1,Smart contracts,2022-09-21T12:14:46Z,2022-09-21T12:14:46Z,5G-Enabled Smart Manufacturing -- A booklet by 5G-SMART,"  In this booklet the most important learnings and key results of 5G-SMART in
the area of smart manufacturing are summarized.
","['Leefke Grosjean', 'Krister Landernäs', 'Berna Sayrac', 'Ognjen Dobrijevic', 'Niels König', 'Davit Harutyunyan', 'Dhruvin Patel', 'Jose F. Monserrat', 'Joachim Sachs']"
http://arxiv.org/abs/2109.05581v1,Smart contracts,2021-09-12T18:33:24Z,2021-09-12T18:33:24Z,Data Analytics for Smart cities: Challenges and Promises,"  The explosion of advancements in artificial intelligence, sensor
technologies, and wireless communication activates ubiquitous sensing through
distributed sensors. These sensors are various domains of networks that lead us
to smart systems in healthcare, transportation, environment, and other relevant
branches/networks. Having collaborative interaction among the smart systems
connects end-user devices to each other which enables achieving a new
integrated entity called Smart Cities. The goal of this study is to provide a
comprehensive survey of data analytics in smart cities. In this paper, we aim
to focus on one of the smart cities important branches, namely Smart Mobility,
and its positive ample impact on the smart cities decision-making process.
Intelligent decision-making systems in smart mobility offer many advantages
such as saving energy, relaying city traffic, and more importantly, reducing
air pollution by offering real-time useful information and imperative
knowledge. Making a decision in smart cities in time is challenging due to
various and high dimensional factors and parameters, which are not frequently
collected. In this paper, we first address current challenges in smart cities
and provide an overview of potential solutions to these challenges. Then, we
offer a framework of these solutions, called universal smart cities decision
making, with three main sections of data capturing, data analysis, and decision
making to optimize the smart mobility within smart cities. With this framework,
we elaborate on fundamental concepts of big data, machine learning, and deep
leaning algorithms that have been applied to smart cities and discuss the role
of these algorithms in decision making for smart mobility in smart cities.
","['Farid Ghareh Mohammadi', 'Farzan Shenavarmasouleh', 'M. Hadi Amini', 'Hamid R. Arabnia']"
http://arxiv.org/abs/1711.09184v1,Smart contracts,2017-11-25T04:06:24Z,2017-11-25T04:06:24Z,A Formal Specification Framework for Smart Grid Components,"  Smart grid can be considered as the next step in the evolution of power
systems. It comprises of different entities and objects ranging from smart
appliances, smart meters, generators, smart storages, and more. One key problem
in modeling smart grid is that while currently there is a considerable focus on
the practical aspects of smart grid, there are very few modeling attempts and
even lesser attempts at formalization. To the best of our knowledge, among
other formal methods, formal specification has previously not been applied in
the domain of smart grid. In this paper, we attempt to bridge this gap by
presenting a novel approach to modeling smart grid components using a formal
specification approach. We use a state-based formal specification language
namely Z (pronounced as `Zed') since we believe Z is particularly suited for
modeling smart grid components.We demonstrate the application of Z on key smart
grid components. The presented formal specification can be considered as first
steps towards modeling of smart grid using a Software Engineering formalism. It
also demonstrates how complex systems, such as the smart grid, can be modeled
elegantly using formal specification.
","['Waseem Akram', 'Muaz A. Niazi']"
http://arxiv.org/abs/1909.02914v1,Smart contracts,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2405.06930v1,Smart contracts,2024-05-11T06:26:07Z,2024-05-11T06:26:07Z,"Extended Reality for Smart Built Environments Design: Smart Lighting
  Design Testbed","  Smart Built Environment is an eco-system of `connected' and `smart' Internet
of Things (IoT) devices that are embedded in a built environment. Smart
lighting is an important category of smart IoT devices that has recently
attracted research interest, particularly for residential areas. In this paper,
we present an extended reality based smart lighting design testbed that can
generate design prototypes based on the functionality of the physical
environment. The emphasis is on designing a smart lighting system in a
controlled residential environment, with some evaluation of well-being and
comfort.
","['Elham Mohammadrezaei', 'Denis Gracanin']"
http://arxiv.org/abs/1610.06855v1,Smart contracts,2016-10-21T16:50:11Z,2016-10-21T16:50:11Z,Big Data: Perspektiven fuer Smart Grids und Smart Buildings,"  This paper gives a short survey of recent trends in the emerging field of big
data. It explains the definitions and useful methods. In addition, application
fields of smart buildings and smart grids are discussed.
",['Ralf Mikut']
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2206.02760v1,Smart contracts,2022-06-06T17:37:51Z,2022-06-06T17:37:51Z,Blockchain for the Cybersecurity of Smart City Applications,"  Cybersecurity is an inherent characteristic that should be addressed before
the large deployment of smart city applications. Recently, Blockchain appears
as a promising technology to provide several cybersecurity aspects of smart
city applications. This paper provides a comprehensive review of the existing
blockchain-based solutions for the cybersecurity of the main smart city
applications, namely smart healthcare, smart transportation, smart agriculture,
supply chain management, smart grid, and smart homes. We describe the existing
solutions and we discuss their merits and limits. Moreover, we define the
security requirements of each smart city application and we give a mapping of
the studied solutions to these defined requirements. Additionally, future
directions are given. We believe that the present survey is a good starting
point for every researcher in the fields of cybersecurity, blockchain, and
smart cities.
","['Omar Cheikhrouhou', 'Ichrak Amdouni', 'Khaleel Mershad', 'Maryem Ammi', 'Tuan Nguyen Gia']"
http://arxiv.org/abs/2207.04424v1,Smart contracts,2022-07-10T09:22:10Z,2022-07-10T09:22:10Z,"An Overview of Cyber Threats, Attacks, and Countermeasures on the
  Primary Domains of Smart Cities","  A smart city is a place where existing facilities and services are enhanced
by digital technology to benefit people and companies. The most critical
infrastructures in this city are interconnected. Increased data exchange across
municipal domains aims to manage the essential assets, leading to more
automation in city governance and optimization of the dynamic offered services.
However, no clear guideline or standard exists for modeling these data flows.
As a result, operators, municipalities, policymakers, manufac-turers, solution
providers, and vendors are forced to accept systems with limited scalability
and varying needs. Nonetheless, it is critical to raise awareness about smart
city cybersecurity and implement suitable measures to safeguard citizens'
privacy and security because the cyber threats seem to be well-organized,
diverse, and sophisticated. This study aims to present an overview of cyber
threats, attacks, and countermeasures on the primary domains of smart cities
(smart government, smart mobility, smart environment, smart living, smart
healthcare, smart economy, and smart people) to present information extracted
from state-of-the-art to policymakers to perceive the critical situation and,
at the same time, to be a valuable resource for the scientific community.
","['Vasiliki Demertzi', 'Stavros Demertzis', 'Konstantinos Demertzis']"
http://arxiv.org/abs/2402.00568v1,Smart contracts,2024-02-01T13:01:47Z,2024-02-01T13:01:47Z,Secure Supervised Learning-Based Smart Home Authentication Framework,"  The Smart home possesses the capability of facilitating home services to
their users with the systematic advance in The Internet of Things (IoT) and
information and communication technologies (ICT) in recent decades. The home
service offered by the smart devices helps the users in utilize maximized level
of comfort for the objective of improving life quality. As the user and smart
devices communicate through an insecure channel, the smart home environment is
prone to security and privacy problems. A secure authentication protocol needs
to be established between the smart devices and the user, such that a situation
for device authentication can be made feasible in smart home environments. Most
of the existing smart home authentication protocols were identified to fail in
facilitating a secure mutual authentication and increases the possibility of
lunching the attacks of session key disclosure, impersonation and stolen smart
device. In this paper, Secure Supervised Learning-based Smart Home
Authentication Framework (SSL-SHAF) is proposed as are liable mutual
authentication that can be contextually imposed for better security. The formal
analysis of the proposed SSL-SHAF confirmed better resistance against session
key disclosure, impersonation and stolen smart device attacks. The results of
SSL-SHAF confirmed minimized computational costs and security compared to the
baseline protocols considered for investigation.
","['K. Swapna Sudha', 'N. Jeyanthi', 'Celestine Iwendi']"
http://arxiv.org/abs/1112.1158v1,Smart contracts,2011-12-06T04:55:32Z,2011-12-06T04:55:32Z,"Wireless Communications and Networking Technologies for Smart Grid:
  Paradigms and Challenges","  Smart grid, regarded as the next generation power grid, uses two-way flows of
electricity and information to create a widely distributed automated energy
delivery network. In this work we present our vision on smart grid from the
perspective of wireless communications and networking technologies. We present
wireless communication and networking paradigms for four typical scenarios in
the future smart grid and also point out the research challenges of the
wireless communication and networking technologies used in smart grid
","['Xi Fang', 'Dejun Yang', 'Guoliang Xue']"
http://arxiv.org/abs/1807.03111v1,Smart contracts,2018-06-13T10:03:44Z,2018-06-13T10:03:44Z,"A Framework for Detecting and Translating User Behavior from Smart Meter
  Data","  The European adoption of smart electricity meters triggers the developments
of new value-added service for smart energy and optimal consumption. Recently,
several algorithms and tools have been built to analyze smart meter's data.
This paper introduces an open framework and prototypes for detecting and
presenting user behavior from its smart meter power consumption data. The
framework aims at presenting the detected user behavior in natural language
reports. In order to validate the proposed framework, an experiment has been
performed and the results have been presented.
","['Egon Kidmose', 'Emad Ebeid', 'Rune Hylsberg Jacobsen']"
http://arxiv.org/abs/2101.06519v1,Smart contracts,2021-01-16T20:46:43Z,2021-01-16T20:46:43Z,"Intrusion Detection Systems for Smart Home IoT Devices: Experimental
  Comparison Study","  Smart homes are one of the most promising applications of the emerging
Internet of Things (IoT) technology. With the growing number of IoT related
devices such as smart thermostats, smart fridges, smart speaker, smart light
bulbs and smart locks, smart homes promise to make our lives easier and more
comfortable. However, the increased deployment of such smart devices brings an
increase in potential security risks and home privacy breaches. In order to
overcome such risks, Intrusion Detection Systems are presented as pertinent
tools that can provide network-level protection for smart devices deployed in
home environments. These systems monitor the network activities of the smart
home-connected de-vices and focus on alerting suspicious or malicious activity.
They also can deal with detected abnormal activities by hindering the impostors
in accessing the victim devices. However, the employment of such systems in the
context of a smart home can be challenging due to the devices hardware
limitations, which may restrict their ability to counter the existing and
emerging attack vectors. Therefore, this paper proposes an experimental
comparison between the widely used open-source NIDSs namely Snort, Suricata and
Bro IDS to find the most appropriate one for smart homes in term of detection
accuracy and resources consumption including CP and memory utilization.
Experimental Results show that Suricata is the best performing NIDS for smart
homes
","['Faisal Alsakran', 'Gueltoum Bendiab', 'Stavros Shiaeles', 'Nicholas Kolokotronis']"
http://arxiv.org/abs/2304.12851v1,Smart contracts,2023-04-25T14:22:48Z,2023-04-25T14:22:48Z,Towards Smart Education through the Internet of Things: A Review,"  IoT is a fundamental enabling technology for creating smart spaces, which can
assist the effective face-to-face and online education systems. The transition
to smart education (integrating IoT and AI into the education system) is
appealing, which has a concrete impact on learners' engagement, motivation,
attendance, and deep learning. Traditional education faces many challenges,
including administration, pedagogy, assessment, and classroom supervision.
Recent developments in ICT (e.g., IoT, AI and 5G, etc.) have yielded lots of
smart solutions for various aspects of life; however, smart solutions are not
well integrated into the education system. In particular, the COVID-19 pandemic
situation had further emphasized the adoption of new smart solutions in
education. This study reviews the related studies and addresses the (i)
problems in the traditional education system with possible solutions, (ii) the
transition towards smart education, and (iii) research challenges in the
transition to smart education (i.e, computational and social resistance).
Considering these studies, smart solutions (e.g., smart pedagogy, smart
assessment, smart classroom, smart administration, etc.) are introduced to the
problems of the traditional system. This exploratory study opens new trends for
scholars and the market to integrate ICT, IoT, and AI into smart education.
","['Afzal Badshah', 'Anwar Ghani', 'Ali Daud', 'Ateeqa Jalal', 'Muhammad Bilal', 'Jon Crowcroft']"
http://arxiv.org/abs/1109.4474v1,Smart contracts,2011-09-21T04:33:49Z,2011-09-21T04:33:49Z,Smart Grid Information Security (IS) Functional Requirement,"  It is important to implement safe smart grid environment to enhance people's
lives and livelihoods. This paper provides information on smart grid IS
functional requirement by illustrating some discussion points to the sixteen
identified requirements. This paper introduces the smart grid potential hazards
that can be referred as a triggering factor to improve the system and security
of the entire grid. The background of smart information infrastructure and the
needs for smart grid IS is described with the adoption of hermeneutic circle as
methodology. Grid information technology and security-s session discusses that
grid provides the chance of a simple and transparent access to different
information sources. In addition, the transformation between traditional versus
smart grid networking trend and the IS importance on the communication field
reflects the criticality of grid IS functional requirement identification is
introduces. The smart grid IS functional requirements described in this paper
are general and can be adopted or modified to suit any smart grid system. This
paper has tutorial contents where some related backgrounds were provided,
especially for networking community, covering the cyber security requirement of
smart grid information infrastructure.
","['Amy Poh Ai Ling', 'Mukaidono Masao']"
http://arxiv.org/abs/1706.07363v1,Smart contracts,2017-06-22T15:19:16Z,2017-06-22T15:19:16Z,Smart Wireless Communication is the Cornerstone of Smart Infrastructures,"  Emerging smart infrastructures, such as Smart City, Smart Grid, Smart Health,
and Smart Transportation, need smart wireless connectivity. However, the
requirements of these smart infrastructures cannot be met with today's wireless
networks. A new wireless infrastructure is needed to meet unprecedented needs
in terms of agility, reliability, security, scalability, and partnerships.
  We are at the beginning of a revolution in how we live with technology,
resulting from a convergence of machine learning (ML), the Internet-of-Things
(IoT), and robotics. A smart infrastructure monitors and processes a vast
amount of data, collected from a dense and wide distribution of heterogeneous
sensors (e.g., the IoT), as well as from web applications like social media. In
real time, using machine learning, patterns and relationships in the data over
space, time, and application can be detected and predictions can be made; on
the basis of these, resources can be managed, decisions can be made, and
devices can be actuated to optimize metrics, such as cost, health, safety, and
convenience.
","['Mary Ann Weitnauer', 'Jennifer Rexford', 'Nicholas Laneman', 'Matthieu Bloch', 'Santiago Griljava', 'Catherine Ross', 'Gee-Kung Chang']"
