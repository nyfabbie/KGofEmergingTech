id,technology,published,updated,title,summary,authors
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1206.3959v2,Artificial intelligence,2012-06-13T16:43:44Z,2014-08-28T04:27:28Z,"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.
","['Jeff Bilmes', 'Andrew Ng']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/1705.02042v2,Neurotechnology,2017-05-04T22:54:54Z,2017-07-13T14:37:34Z,Exponential scaling of neural algorithms - a future beyond Moore's Law?,"  Although the brain has long been considered a potential inspiration for
future computing, Moore's Law - the scaling property that has seen revolutions
in technologies ranging from supercomputers to smart phones - has largely been
driven by advances in materials science. As the ability to miniaturize
transistors is coming to an end, there is increasing attention on new
approaches to computation, including renewed enthusiasm around the potential of
neural computation. This paper describes how recent advances in
neurotechnologies, many of which have been aided by computing's rapid
progression over recent decades, are now reigniting this opportunity to bring
neural computation insights into broader computing applications. As we
understand more about the brain, our ability to motivate new computing
paradigms with continue to progress. These new approaches to computing, which
we are already seeing in techniques such as deep learning and neuromorphic
hardware, will themselves improve our ability to learn about the brain and
accordingly can be projected to give rise to even further insights. This paper
will describe how this positive feedback has the potential to change the
complexion of how computing sciences and neurosciences interact, and suggests
that the next form of exponential scaling in computing may emerge from our
progressive understanding of the brain.
",['James B. Aimone']
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/1909.06494v1,Smart contracts,2019-09-14T00:36:13Z,2019-09-14T00:36:13Z,Transactional Smart Contracts in Blockchain Systems,"  This paper presents TXSC, a framework that provides smart contract developers
with transaction primitives. These primitives allow developers to write smart
contracts without the need to reason about the anomalies that can arise due to
concurrent smart contract function executions.
","['Victor Zakhary', 'Divyakant Agrawal', 'Amr El Abbadi']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1206.3959v2,Artificial intelligence,2012-06-13T16:43:44Z,2014-08-28T04:27:28Z,"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.
","['Jeff Bilmes', 'Andrew Ng']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/1705.02042v2,Neurotechnology,2017-05-04T22:54:54Z,2017-07-13T14:37:34Z,Exponential scaling of neural algorithms - a future beyond Moore's Law?,"  Although the brain has long been considered a potential inspiration for
future computing, Moore's Law - the scaling property that has seen revolutions
in technologies ranging from supercomputers to smart phones - has largely been
driven by advances in materials science. As the ability to miniaturize
transistors is coming to an end, there is increasing attention on new
approaches to computation, including renewed enthusiasm around the potential of
neural computation. This paper describes how recent advances in
neurotechnologies, many of which have been aided by computing's rapid
progression over recent decades, are now reigniting this opportunity to bring
neural computation insights into broader computing applications. As we
understand more about the brain, our ability to motivate new computing
paradigms with continue to progress. These new approaches to computing, which
we are already seeing in techniques such as deep learning and neuromorphic
hardware, will themselves improve our ability to learn about the brain and
accordingly can be projected to give rise to even further insights. This paper
will describe how this positive feedback has the potential to change the
complexion of how computing sciences and neurosciences interact, and suggests
that the next form of exponential scaling in computing may emerge from our
progressive understanding of the brain.
",['James B. Aimone']
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/1909.06494v1,Smart contracts,2019-09-14T00:36:13Z,2019-09-14T00:36:13Z,Transactional Smart Contracts in Blockchain Systems,"  This paper presents TXSC, a framework that provides smart contract developers
with transaction primitives. These primitives allow developers to write smart
contracts without the need to reason about the anomalies that can arise due to
concurrent smart contract function executions.
","['Victor Zakhary', 'Divyakant Agrawal', 'Amr El Abbadi']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1208.5154v2,Artificial intelligence,2012-08-25T18:22:17Z,2014-08-28T04:25:59Z,"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","  This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.
","['David McAllester', 'Petri Myllymaki']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/2001.00411v1,Robotics,2020-01-02T12:26:20Z,2020-01-02T12:26:20Z,Recent Advances in Human-Robot Collaboration Towards Joint Action,"  Robots existed as separate entities till now, but the horizons of a symbiotic
human-robot partnership are impending. Despite all the recent technical
advances in terms of hardware, robots are still not endowed with desirable
relational skills that ensure a social component in their existence. This
article draws from our experience as roboticists in Human-Robot Collaboration
(HRC) with humanoid robots and presents some of the recent advances made
towards realizing intuitive robot behaviors and partner-aware control involving
physical interactions.
","['Yeshasvi Tirupachuri', 'Gabriele Nava', 'Lorenzo Rapetti', 'Claudia Latella', 'Kourosh Darvish', 'Daniele Pucci']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2103.09314v1,Smart contracts,2021-03-16T20:46:31Z,2021-03-16T20:46:31Z,"iContractBot: A Chatbot for Smart Contracts' Specification and Code
  Generation","  Recently, Blockchain technology adoption has expanded to many application
areas due to the evolution of smart contracts. However, developing smart
contracts is non-trivial and challenging due to the lack of tools and expertise
in this field. A promising solution to overcome this issue is to use
Model-Driven Engineering (MDE), however, using models still involves a learning
curve and might not be suitable for non-technical users. To tackle this
challenge, chatbot or conversational interfaces can be used to assess the
non-technical users to specify a smart contract in gradual and interactive
manner.
  In this paper, we propose iContractBot, a chatbot for modeling and developing
smart contracts. Moreover, we investigate how to integrate iContractBot with
iContractML, a domain-specific modeling language for developing smart
contracts, and instantiate intention models from the chatbot. The iContractBot
framework provides a domain-specific language (DSL) based on the user intention
and performs model-to-text transformation to generate the smart contract code.
A smart contract use case is presented to demonstrate how iContractBot can be
utilized for creating models and generating the deployment artifacts for smart
contracts based on a simple conversation.
","['Ilham Qasse', 'Shailesh Mishra', 'Mohammad Hamdaqa']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1208.5154v2,Artificial intelligence,2012-08-25T18:22:17Z,2014-08-28T04:25:59Z,"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","  This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.
","['David McAllester', 'Petri Myllymaki']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/2001.00411v1,Robotics,2020-01-02T12:26:20Z,2020-01-02T12:26:20Z,Recent Advances in Human-Robot Collaboration Towards Joint Action,"  Robots existed as separate entities till now, but the horizons of a symbiotic
human-robot partnership are impending. Despite all the recent technical
advances in terms of hardware, robots are still not endowed with desirable
relational skills that ensure a social component in their existence. This
article draws from our experience as roboticists in Human-Robot Collaboration
(HRC) with humanoid robots and presents some of the recent advances made
towards realizing intuitive robot behaviors and partner-aware control involving
physical interactions.
","['Yeshasvi Tirupachuri', 'Gabriele Nava', 'Lorenzo Rapetti', 'Claudia Latella', 'Kourosh Darvish', 'Daniele Pucci']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2103.09314v1,Smart contracts,2021-03-16T20:46:31Z,2021-03-16T20:46:31Z,"iContractBot: A Chatbot for Smart Contracts' Specification and Code
  Generation","  Recently, Blockchain technology adoption has expanded to many application
areas due to the evolution of smart contracts. However, developing smart
contracts is non-trivial and challenging due to the lack of tools and expertise
in this field. A promising solution to overcome this issue is to use
Model-Driven Engineering (MDE), however, using models still involves a learning
curve and might not be suitable for non-technical users. To tackle this
challenge, chatbot or conversational interfaces can be used to assess the
non-technical users to specify a smart contract in gradual and interactive
manner.
  In this paper, we propose iContractBot, a chatbot for modeling and developing
smart contracts. Moreover, we investigate how to integrate iContractBot with
iContractML, a domain-specific modeling language for developing smart
contracts, and instantiate intention models from the chatbot. The iContractBot
framework provides a domain-specific language (DSL) based on the user intention
and performs model-to-text transformation to generate the smart contract code.
A smart contract use case is presented to demonstrate how iContractBot can be
utilized for creating models and generating the deployment artifacts for smart
contracts based on a simple conversation.
","['Ilham Qasse', 'Shailesh Mishra', 'Mohammad Hamdaqa']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1208.5154v2,Artificial intelligence,2012-08-25T18:22:17Z,2014-08-28T04:25:59Z,"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","  This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.
","['David McAllester', 'Petri Myllymaki']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/2001.00411v1,Robotics,2020-01-02T12:26:20Z,2020-01-02T12:26:20Z,Recent Advances in Human-Robot Collaboration Towards Joint Action,"  Robots existed as separate entities till now, but the horizons of a symbiotic
human-robot partnership are impending. Despite all the recent technical
advances in terms of hardware, robots are still not endowed with desirable
relational skills that ensure a social component in their existence. This
article draws from our experience as roboticists in Human-Robot Collaboration
(HRC) with humanoid robots and presents some of the recent advances made
towards realizing intuitive robot behaviors and partner-aware control involving
physical interactions.
","['Yeshasvi Tirupachuri', 'Gabriele Nava', 'Lorenzo Rapetti', 'Claudia Latella', 'Kourosh Darvish', 'Daniele Pucci']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2103.09314v1,Smart contracts,2021-03-16T20:46:31Z,2021-03-16T20:46:31Z,"iContractBot: A Chatbot for Smart Contracts' Specification and Code
  Generation","  Recently, Blockchain technology adoption has expanded to many application
areas due to the evolution of smart contracts. However, developing smart
contracts is non-trivial and challenging due to the lack of tools and expertise
in this field. A promising solution to overcome this issue is to use
Model-Driven Engineering (MDE), however, using models still involves a learning
curve and might not be suitable for non-technical users. To tackle this
challenge, chatbot or conversational interfaces can be used to assess the
non-technical users to specify a smart contract in gradual and interactive
manner.
  In this paper, we propose iContractBot, a chatbot for modeling and developing
smart contracts. Moreover, we investigate how to integrate iContractBot with
iContractML, a domain-specific modeling language for developing smart
contracts, and instantiate intention models from the chatbot. The iContractBot
framework provides a domain-specific language (DSL) based on the user intention
and performs model-to-text transformation to generate the smart contract code.
A smart contract use case is presented to demonstrate how iContractBot can be
utilized for creating models and generating the deployment artifacts for smart
contracts based on a simple conversation.
","['Ilham Qasse', 'Shailesh Mishra', 'Mohammad Hamdaqa']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1208.5154v2,Artificial intelligence,2012-08-25T18:22:17Z,2014-08-28T04:25:59Z,"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","  This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.
","['David McAllester', 'Petri Myllymaki']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/2001.00411v1,Robotics,2020-01-02T12:26:20Z,2020-01-02T12:26:20Z,Recent Advances in Human-Robot Collaboration Towards Joint Action,"  Robots existed as separate entities till now, but the horizons of a symbiotic
human-robot partnership are impending. Despite all the recent technical
advances in terms of hardware, robots are still not endowed with desirable
relational skills that ensure a social component in their existence. This
article draws from our experience as roboticists in Human-Robot Collaboration
(HRC) with humanoid robots and presents some of the recent advances made
towards realizing intuitive robot behaviors and partner-aware control involving
physical interactions.
","['Yeshasvi Tirupachuri', 'Gabriele Nava', 'Lorenzo Rapetti', 'Claudia Latella', 'Kourosh Darvish', 'Daniele Pucci']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2103.09314v1,Smart contracts,2021-03-16T20:46:31Z,2021-03-16T20:46:31Z,"iContractBot: A Chatbot for Smart Contracts' Specification and Code
  Generation","  Recently, Blockchain technology adoption has expanded to many application
areas due to the evolution of smart contracts. However, developing smart
contracts is non-trivial and challenging due to the lack of tools and expertise
in this field. A promising solution to overcome this issue is to use
Model-Driven Engineering (MDE), however, using models still involves a learning
curve and might not be suitable for non-technical users. To tackle this
challenge, chatbot or conversational interfaces can be used to assess the
non-technical users to specify a smart contract in gradual and interactive
manner.
  In this paper, we propose iContractBot, a chatbot for modeling and developing
smart contracts. Moreover, we investigate how to integrate iContractBot with
iContractML, a domain-specific modeling language for developing smart
contracts, and instantiate intention models from the chatbot. The iContractBot
framework provides a domain-specific language (DSL) based on the user intention
and performs model-to-text transformation to generate the smart contract code.
A smart contract use case is presented to demonstrate how iContractBot can be
utilized for creating models and generating the deployment artifacts for smart
contracts based on a simple conversation.
","['Ilham Qasse', 'Shailesh Mishra', 'Mohammad Hamdaqa']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1208.5154v2,Artificial intelligence,2012-08-25T18:22:17Z,2014-08-28T04:25:59Z,"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","  This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.
","['David McAllester', 'Petri Myllymaki']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,"  As the proliferation of technology dramatically infiltrates all aspects of
modern life, in many ways the world is becoming so dynamic and complex that
technological capabilities are overwhelming human capabilities to optimally
interact with and leverage those technologies. Fortunately, these technological
advancements have also driven an explosion of neuroscience research over the
past several decades, presenting engineers with a remarkable opportunity to
design and develop flexible and adaptive brain-based neurotechnologies that
integrate with and capitalize on human capabilities and limitations to improve
human-system interactions. Major forerunners of this conception are
brain-computer interfaces (BCIs), which to this point have been largely focused
on improving the quality of life for particular clinical populations and
include, for example, applications for advanced communications with paralyzed
or locked in patients as well as the direct control of prostheses and
wheelchairs. Near-term applications are envisioned that are primarily task
oriented and are targeted to avoid the most difficult obstacles to development.
In the farther term, a holistic approach to BCIs will enable a broad range of
task-oriented and opportunistic applications by leveraging pervasive
technologies and advanced analytical approaches to sense and merge critical
brain, behavioral, task, and environmental information. Communications and
other applications that are envisioned to be broadly impacted by BCIs are
highlighted; however, these represent just a small sample of the potential of
these technologies.
","['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/2001.00411v1,Robotics,2020-01-02T12:26:20Z,2020-01-02T12:26:20Z,Recent Advances in Human-Robot Collaboration Towards Joint Action,"  Robots existed as separate entities till now, but the horizons of a symbiotic
human-robot partnership are impending. Despite all the recent technical
advances in terms of hardware, robots are still not endowed with desirable
relational skills that ensure a social component in their existence. This
article draws from our experience as roboticists in Human-Robot Collaboration
(HRC) with humanoid robots and presents some of the recent advances made
towards realizing intuitive robot behaviors and partner-aware control involving
physical interactions.
","['Yeshasvi Tirupachuri', 'Gabriele Nava', 'Lorenzo Rapetti', 'Claudia Latella', 'Kourosh Darvish', 'Daniele Pucci']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2103.09314v1,Smart contracts,2021-03-16T20:46:31Z,2021-03-16T20:46:31Z,"iContractBot: A Chatbot for Smart Contracts' Specification and Code
  Generation","  Recently, Blockchain technology adoption has expanded to many application
areas due to the evolution of smart contracts. However, developing smart
contracts is non-trivial and challenging due to the lack of tools and expertise
in this field. A promising solution to overcome this issue is to use
Model-Driven Engineering (MDE), however, using models still involves a learning
curve and might not be suitable for non-technical users. To tackle this
challenge, chatbot or conversational interfaces can be used to assess the
non-technical users to specify a smart contract in gradual and interactive
manner.
  In this paper, we propose iContractBot, a chatbot for modeling and developing
smart contracts. Moreover, we investigate how to integrate iContractBot with
iContractML, a domain-specific modeling language for developing smart
contracts, and instantiate intention models from the chatbot. The iContractBot
framework provides a domain-specific language (DSL) based on the user intention
and performs model-to-text transformation to generate the smart contract code.
A smart contract use case is presented to demonstrate how iContractBot can be
utilized for creating models and generating the deployment artifacts for smart
contracts based on a simple conversation.
","['Ilham Qasse', 'Shailesh Mishra', 'Mohammad Hamdaqa']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1206.3959v2,Artificial intelligence,2012-06-13T16:43:44Z,2014-08-28T04:27:28Z,"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.
","['Jeff Bilmes', 'Andrew Ng']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/2505.24790v1,Neurotechnology,2025-05-30T16:52:44Z,2025-05-30T16:52:44Z,"Towards model-based design of causal manipulations of brain circuits
  with high spatiotemporal precision","  Recent advancements in neurotechnology enable precise spatiotemporal patterns
of microstimulations with single-cell resolution. The choice of perturbation
sites must satisfy two key criteria: efficacy in evoking significant responses
and selectivity for the desired target effects. This choice is currently based
on laborious trial-and-error procedures, unfeasible for sequences of multi-site
stimulations. Efficient methods to design complex perturbation patterns are
urgently needed. Can we design a spatiotemporal pattern of stimulation to steer
neural activity and behavior towards a desired target? We outline a method for
achieving this goal in two steps. First, we identify the most effective
perturbation sites, or hubs, only based on short observations of spontaneous
neural activity. Second, we provide an efficient method to design multi-site
stimulation patterns by combining approaches from nonlinear dynamical systems,
control theory and data-driven methods. We demonstrate the feasibility of our
approach using multi-site stimulation patterns in recurrent network models.
","['Anandita De', 'Roozbeh Kiani', 'Luca Mazzucato']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/2505.22619v1,Smart contracts,2025-05-28T17:40:21Z,2025-05-28T17:40:21Z,Smart Contracts for SMEs and Large Companies,"  Research on blockchains addresses multiple issues, with one being writing
smart contracts. In our previous research we described methodology and a tool
to generate, in automated fashion, smart contracts from BPMN models. The
generated smart contracts provide support for multi-step transactions that
facilitate repair/upgrade of smart contracts. In this paper we show how the
approach is used to support collaborations via smart contracts for companies
ranging from SMEs with little IT capabilities to companies with IT using
blockchain smart contracts. Furthermore, we also show how the approach is used
for certain applications to generate smart contracts by a BPMN modeler who does
not need any knowledge of blockchain technology or smart contract development -
thus we are hoping to facilitate democratization of smart contracts and
blockchain technology.
","['C. G. Liu', 'P. Bodorik', 'D. Jutla']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1206.3959v2,Artificial intelligence,2012-06-13T16:43:44Z,2014-08-28T04:27:28Z,"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.
","['Jeff Bilmes', 'Andrew Ng']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/2505.24790v1,Neurotechnology,2025-05-30T16:52:44Z,2025-05-30T16:52:44Z,"Towards model-based design of causal manipulations of brain circuits
  with high spatiotemporal precision","  Recent advancements in neurotechnology enable precise spatiotemporal patterns
of microstimulations with single-cell resolution. The choice of perturbation
sites must satisfy two key criteria: efficacy in evoking significant responses
and selectivity for the desired target effects. This choice is currently based
on laborious trial-and-error procedures, unfeasible for sequences of multi-site
stimulations. Efficient methods to design complex perturbation patterns are
urgently needed. Can we design a spatiotemporal pattern of stimulation to steer
neural activity and behavior towards a desired target? We outline a method for
achieving this goal in two steps. First, we identify the most effective
perturbation sites, or hubs, only based on short observations of spontaneous
neural activity. Second, we provide an efficient method to design multi-site
stimulation patterns by combining approaches from nonlinear dynamical systems,
control theory and data-driven methods. We demonstrate the feasibility of our
approach using multi-site stimulation patterns in recurrent network models.
","['Anandita De', 'Roozbeh Kiani', 'Luca Mazzucato']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/2505.22619v1,Smart contracts,2025-05-28T17:40:21Z,2025-05-28T17:40:21Z,Smart Contracts for SMEs and Large Companies,"  Research on blockchains addresses multiple issues, with one being writing
smart contracts. In our previous research we described methodology and a tool
to generate, in automated fashion, smart contracts from BPMN models. The
generated smart contracts provide support for multi-step transactions that
facilitate repair/upgrade of smart contracts. In this paper we show how the
approach is used to support collaborations via smart contracts for companies
ranging from SMEs with little IT capabilities to companies with IT using
blockchain smart contracts. Furthermore, we also show how the approach is used
for certain applications to generate smart contracts by a BPMN modeler who does
not need any knowledge of blockchain technology or smart contract development -
thus we are hoping to facilitate democratization of smart contracts and
blockchain technology.
","['C. G. Liu', 'P. Bodorik', 'D. Jutla']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/2505.24790v1,Neurotechnology,2025-05-30T16:52:44Z,2025-05-30T16:52:44Z,"Towards model-based design of causal manipulations of brain circuits
  with high spatiotemporal precision","  Recent advancements in neurotechnology enable precise spatiotemporal patterns
of microstimulations with single-cell resolution. The choice of perturbation
sites must satisfy two key criteria: efficacy in evoking significant responses
and selectivity for the desired target effects. This choice is currently based
on laborious trial-and-error procedures, unfeasible for sequences of multi-site
stimulations. Efficient methods to design complex perturbation patterns are
urgently needed. Can we design a spatiotemporal pattern of stimulation to steer
neural activity and behavior towards a desired target? We outline a method for
achieving this goal in two steps. First, we identify the most effective
perturbation sites, or hubs, only based on short observations of spontaneous
neural activity. Second, we provide an efficient method to design multi-site
stimulation patterns by combining approaches from nonlinear dynamical systems,
control theory and data-driven methods. We demonstrate the feasibility of our
approach using multi-site stimulation patterns in recurrent network models.
","['Anandita De', 'Roozbeh Kiani', 'Luca Mazzucato']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/2505.22619v1,Smart contracts,2025-05-28T17:40:21Z,2025-05-28T17:40:21Z,Smart Contracts for SMEs and Large Companies,"  Research on blockchains addresses multiple issues, with one being writing
smart contracts. In our previous research we described methodology and a tool
to generate, in automated fashion, smart contracts from BPMN models. The
generated smart contracts provide support for multi-step transactions that
facilitate repair/upgrade of smart contracts. In this paper we show how the
approach is used to support collaborations via smart contracts for companies
ranging from SMEs with little IT capabilities to companies with IT using
blockchain smart contracts. Furthermore, we also show how the approach is used
for certain applications to generate smart contracts by a BPMN modeler who does
not need any knowledge of blockchain technology or smart contract development -
thus we are hoping to facilitate democratization of smart contracts and
blockchain technology.
","['C. G. Liu', 'P. Bodorik', 'D. Jutla']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
http://arxiv.org/abs/2406.04641v1,3D printing,2024-06-07T04:56:05Z,2024-06-07T04:56:05Z,"Preparation of high precision aspherical lenses based on micro
  stereolithography technology","  The 3D printing technology based on digital light processing (DLP) has
highlighted its powerful manufacturing capabilities for optical components.
However, the printing structure obtained by DLP based down projection printing
is easily adhered to the printing window below, and the printed lens surface
will have a step effect. This article uses DLP 3D printing technology to print
non spherical lenses. During the printing process, a new type of inert liquid
fluoride solution was used as the isolation layer, which can more effectively
and conveniently prevent the printing structure from sticking to the printing
window. At the same time, a vertical lifting immersion method was proposed to
smooth the step effect on the surface of the lens.
","['Xiaoying Lu', 'Hua Liu']"
http://arxiv.org/abs/1405.0199v1,3D printing,2014-02-25T04:43:22Z,2014-02-25T04:43:22Z,"Liquid Phase 3D Printing for Quickly Manufacturing Metal Objects with
  Low Melting Point Alloy Ink","  Conventional 3D printings are generally time-consuming and printable metal
inks are rather limited. From an alternative way, we proposed a liquid phase 3D
printing for quickly making metal objects. Through introducing metal alloys
whose melting point is slightly above room temperature as printing inks,
several representative structures spanning from one, two and three dimension to
more complex patterns were demonstrated to be quickly fabricated. Compared with
the air cooling in a conventional 3D printing, the liquid-phase-manufacturing
offers a much higher cooling rate and thus significantly improves the speed in
fabricating metal objects. This unique strategy also efficiently prevents the
liquid metal inks from air oxidation which is hard to avoid otherwise in an
ordinary 3D printing. Several key physical factors (like properties of the
cooling fluid, injection speed and needle diameter, types and properties of the
printing ink, etc.) were disclosed which would evidently affect the printing
quality. In addition, a basic route to make future liquid phase 3D printer
incorporated with both syringe pump and needle arrays was also suggested. The
liquid phase 3D printing method, which owns potential values not available in a
conventional modality, opens an efficient way for quickly making metal objects
in the coming time.
","['Lei Wang', 'Jing Liu']"
http://arxiv.org/abs/2202.11426v2,3D printing,2022-02-23T11:14:24Z,2022-03-29T16:06:20Z,Open5x: Accessible 5-axis 3D printing and conformal slicing,"  The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.
","['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']"
http://arxiv.org/abs/2305.09394v1,3D printing,2023-05-16T12:28:15Z,2023-05-16T12:28:15Z,"3D Printing and Design in Isolation: A Case from a Simulated Lunar
  Mission","  Despite the decades-long history of 3D printing, it is not used to its full
potential. Yet 3D printing holds promise for isolated communities, aiming for
self-sufficiency. In this experiential study conducted in an analog space
habitat we evaluated challenges and opportunities of using 3D printing. Our
study revealed barriers such as: 1) setting up and maintaining the 3D printing
equipment while minding different kinds of pollution, that is air, temperature
and sound, 2) design skill and familiarity with specialized software as well as
materials and 3) the awareness of what can be achieved to meet community needs.
We observed that in-community experience and know-how are reliable sources of
3D print ideas, that improve quality of life of community members if they are
encouraged and supported by participatory design. Co-design of 3D prints in
small, specialized communities is a promising area of study, that can bring new
applications of 3D print technology.
","['Wiktor Stawski', 'Kinga Skorupska', 'Wiesław Kopeć']"
http://arxiv.org/abs/2401.11778v1,3D printing,2024-01-22T09:17:24Z,2024-01-22T09:17:24Z,All Inkjet-printed Organic Solar Cells on 3D Objects,"  Drop-on-demand inkjet printing is a promising and commercially relevant
technology for producing organic electronic devices of arbitrary shape on a
wide variety of different substrates. In this work we transfer the inkjet
printing process of organic photovoltaic devices from 2D to 3D substrates,
using a 5-axis robot system equipped with a multi nozzle inkjet printing unit.
We present a ready-to-use 3D printing system for industrial application, using
a 5-axis motion system controlled by commercial 3D motion software, combined
with a commonly used multi-nozzle inkjet print head controlled by the
corresponding printing software. The very first time inkjet-printed solar cells
on glass/ITO with power conversion efficiencies (PCE) of up to 7% are realized
on a 3D object with surfaces tilted by angles of up to 60{\deg} against the
horizontal direction. Undesired ink flow during deposition of the
inkjet-printed layers was avoided by proper ink formulation. In order to be
able to print organic (opto-)electronic devices also on substrates without
sputtered indium tin oxide bottom electrode, the bottom electrode was
inkjet-printed from silver nanoparticle (AgNP) ink, resulting in the first all
inkjet-printed (i.e., including bottom electrode) solar cell on a 3D object
ever with a record PCE of 2.5%. This work paves the way for functionalizing
even complex objects, such as cars, mobile phones, or Internet of Things (IoT)
applications with inkjet-printed (opto-)electronic devices.
","['Marc Steinberger', 'Andreas Distler', 'Johannes Hörber', 'Kai Cheong Tam', 'Christoph J. Brabec', 'Hans-Joachim Egelhaaf']"
http://arxiv.org/abs/2103.02063v1,3D printing,2021-03-02T22:25:34Z,2021-03-02T22:25:34Z,A 3D Printing Hexacopter: Design and Demonstration,"  3D printing using robots has garnered significant interest in manufacturing
and construction in recent years. A robot's versatility paired with the design
freedom of 3D printing offers promising opportunities for how parts and
structures are built in the future. However, 3D printed objects are still
limited in size and location due to a lack of vertical mobility of ground
robots. These limitations severely restrict the potential of the 3D printing
process. To overcome these limitations, we develop a hexacopter testbed that
can print via fused deposition modeling during flight. We discuss the design of
this testbed and develop a simple control strategy for initial print tests. By
successfully performing these initial print tests, we demonstrate the
feasibility of this approach and lay the groundwork for printing 3D parts and
structures with drones.
","['Alexander Nettekoven', 'Ufuk Topcu']"
http://arxiv.org/abs/2105.10943v1,3D printing,2021-05-23T14:25:34Z,2021-05-23T14:25:34Z,4D printing of mechanical metamaterials,"  Mechanical metamaterials owe their extraordinary properties and
functionalities to their micro-/nanoscale design of which shape, including both
geometry and topology, is perhaps the most important aspect. 4D printing
enables programmed, predictable, and precise change in the shape of mechanical
metamaterials to achieve multi-functionality, adaptive properties, and the
other types of desired behaviors that cannot be achieved using simple 3D
printing. This paper presents an overview of 4D printing as applied to
mechanical metamaterials. It starts by presenting a systematic definition of
what 4D printing is and what shape aspects (e.g., geometry, topology) are
relevant for the 4D printing of mechanical metamaterials. Instead of focusing
on different printing processes and materials, the paper addresses the most
fundamental aspects of the shapeshifting behaviors required for transforming a
flat construct to a target 3D shape (i.e., 2D to 3D shapeshifting) or
transforming a 3D shape to another 3D shape (i.e., 3D to 3D shapeshifting). In
either case, we will discuss the rigid-body shape morphing (e.g., rigid
origami) as well as deformable-body shapeshifting. The paper concludes with a
discussion of the major challenges ahead of us for applying 4D printing to
mechanical metamaterials and suggests several areas for future research.
",['Amir A. Zadpoor']
http://arxiv.org/abs/2403.16470v1,3D printing,2024-03-25T06:52:26Z,2024-03-25T06:52:26Z,Data-Driven Extrusion Force Control Tuning for 3D Printing,"  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
","['Xavier Guidetti', 'Ankita Mukne', 'Marvin Rueppel', 'Yannick Nagel', 'Efe C. Balta', 'John Lygeros']"
http://arxiv.org/abs/1705.05893v1,3D printing,2017-05-16T19:56:58Z,2017-05-16T19:56:58Z,"Computed Axial Lithography (CAL): Toward Single Step 3D Printing of
  Arbitrary Geometries","  Most additive manufacturing processes today operate by printing voxels (3D
pixels) serially point-by-point to build up a 3D part. In some more
recently-developed techniques, for example optical printing methods such as
projection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],
parts are printed layer-by-layer by curing full 2d (very thin in one dimension)
layers of the 3d part in each print step. There does not yet exist a technique
which is able to print arbitrarily-defined 3D geometries in a single print
step. If such a technique existed, it could be used to expand the range of
printable geometries in additive manufacturing and relax constraints on factors
such as overhangs in topology optimization. It could also vastly increase print
speed for 3D parts. In this work, we develop the principles for an approach for
single exposure 3D printing of arbitrarily defined geometries. The approach,
termed Computed Axial Lithgography (CAL), is based on tomographic
reconstruction, with mathematical optimization to generate a set of projections
to optically define an arbitrary dose distribution within a target volume. We
demonstrate the potential ability of the technique to print 3D parts using a
prototype CAL system based on sequential illumination from many angles. We also
propose new hardware designs which will help us to realize true single-shot
arbitrary-geometry 3D CAL.
","['Brett Kelly', 'Indrasen Bhattacharya', 'Maxim Shusteff', 'Robert M. Panas', 'Hayden K. Taylor', 'Christopher M. Spadaccini']"
http://arxiv.org/abs/1406.4817v1,3D printing,2014-06-15T06:28:17Z,2014-06-15T06:28:17Z,3D Printing of Scintillating Materials,"  We demonstrate, for the first time, the applicability of 3D printing
technique to the manufacture of scintillation detectors. We report of a
formulation, usable in stereolithographic printing, that exhibits scintillation
efficiency on the order of 30\% of that of commercial polystyrene based
scintillators. We discuss the applicability of these techniques and propose
future enhancements that will allow tailoring the printed scintillation
detectors to various application.
","['Y. Mishnayot', 'M. Layani', 'I. Cooperstein', 'S. Magdassi', 'G. Ron']"
http://arxiv.org/abs/1809.07940v1,3D printing,2018-09-21T04:28:49Z,2018-09-21T04:28:49Z,"Printing-while-moving: a new paradigm for large-scale robotic 3D
  Printing","  Building and Construction have recently become an exciting application ground
for robotics. In particular, rapid progress in materials formulation and in
robotics technology has made robotic 3D Printing of concrete a promising
technique for in-situ construction. Yet, scalability remains an important
hurdle to widespread adoption: the printing systems (gantry- based or
arm-based) are often much larger than the structure to be printed, hence
cumbersome. Recently, a mobile printing system - a manipulator mounted on a
mobile base - was proposed to alleviate this issue: such a system, by moving
its base, can potentially print a structure larger than itself. However, the
proposed system could only print while being stationary, imposing thereby a
limit on the size of structures that can be printed in a single take. Here, we
develop a system that implements the printing-while-moving paradigm, which
enables printing single-piece structures of arbitrary sizes with a single
robot. This development requires solving motion planning, localization, and
motion control problems that are specific to mobile 3D Printing. We report our
framework to address those problems, and demonstrate, for the first time, a
printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete
structure is printed by a robot arm that has a reach of 87 cm.
","['Mehmet Efe Tiryaki', 'Xu Zhang', 'Quang-Cuong Pham']"
http://arxiv.org/abs/1806.00394v1,3D printing,2018-06-01T15:28:58Z,2018-06-01T15:28:58Z,3D Conductive Polymer Printed Metasurface Antenna for Fresnel Focusing,"  We demonstrate a 3D printed holographic metasurface antenna for beam-focusing
applications at 10 GHz within the X-band frequency regime. The metasurface
antenna is printed using a dual-material 3D printer leveraging a biodegradable
conductive polymer material (Electrifi) to print the conductive parts and
polylactic acid (PLA) to print the dielectric substrate. The entire metasurface
antenna is 3D printed at once; no additional techniques, such as metal-plating
and laser etching, are required. It is demonstrated that using the 3D printed
conductive polymer metasurface antenna, high-fidelity beam focusing can be
achieved within the Fresnel region of the antenna. It is also shown that the
material conductivity for 3D printing has a substantial effect on the radiation
characteristics of the metasurface antenna.
","['Okan Yurduseven', 'Shengrong Ye', 'Thomas Fromenteze', 'Daniel L. Marks', 'Benjamin J. Wiley', 'David R. Smith']"
http://arxiv.org/abs/2404.11776v1,3D printing,2024-04-17T21:57:29Z,2024-04-17T21:57:29Z,"3D object quality prediction for Metal Jet Printer with Multimodal
  thermal encoder","  With the advancements in 3D printing technologies, it is extremely important
that the quality of 3D printed objects, and dimensional accuracies should meet
the customer's specifications. Various factors during metal printing affect the
printed parts' quality, including the power quality, the printing stage
parameters, the print part's location inside the print bed, the curing stage
parameters, and the metal sintering process. With the large data gathered from
HP's MetJet printing process, AI techniques can be used to analyze, learn, and
effectively infer the printed part quality metrics, as well as assist in
improving the print yield. In-situ thermal sensing data captured by
printer-installed thermal sensors contains the part thermal signature of fusing
layers. Such part thermal signature contains a convoluted impact from various
factors. In this paper, we use a multimodal thermal encoder network to fuse
data of a different nature including the video data vectorized printer control
data, and exact part thermal signatures with a trained encoder-decoder module.
We explored the data fusing techniques and stages for data fusing, the
optimized end-to-end model architecture indicates an improved part quality
prediction accuracy.
","[' Rachel', ' Chen', 'Wenjia Zheng', 'Sandeep Jalui', 'Pavan Suri', 'Jun Zeng']"
http://arxiv.org/abs/1605.03246v1,3D printing,2016-05-10T23:41:51Z,2016-05-10T23:41:51Z,"Analysis of 3D-printed metal for rapid-prototyped reflective terahertz
  optics","  We explore the potential of 3D metal printing to realize complex conductive
terahertz devices. Factors impacting performance such as printing resolution,
surface roughness, oxidation, and material loss are investigated via
analytical, numerical, and experimental approaches. The high degree of control
offered by a 3D-printed topology is exploited to realize a zone plate operating
at 530 GHz. Reflection efficiency at this frequency is found to be over 90%.
The high-performance of this preliminary device suggest that 3D metal printing
can play a strong role in guided-wave and general beam control devices in the
terahertz range.
","['Daniel Headland', 'Withawat Withayachumnankul', 'Michael Webb', 'Heike Ebendorff-Heidepriem', 'Andre Luiten', 'Derek Abbott']"
http://arxiv.org/abs/2501.11995v1,3D printing,2025-01-21T09:34:37Z,2025-01-21T09:34:37Z,"Fabrication of Poly (ε-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","  3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.
","['Martin Weber', 'Dmitry Nikolaev', 'Mikko Koskenniemi', 'Jere Hyvönen', 'Joel Jääskeläinen', 'Armand Navarre', 'Ekaterina Takmakova', 'Arun Teotia', 'Pekka Katajisto', 'Robert Luxenhofer', 'Edward Hæggström', 'Ari Salmi']"
http://arxiv.org/abs/2401.08982v1,3D printing,2024-01-17T05:26:30Z,2024-01-17T05:26:30Z,Robot Tape Manipulation for 3D Printing,"  3D printing has enabled various applications using different forms of
materials, such as filaments, sheets, and inks. Typically, during 3D printing,
feedstocks are transformed into discrete building blocks and placed or
deposited in a designated location similar to the manipulation and assembly of
discrete objects. However, 3D printing of continuous and flexible tape (with
the geometry between filaments and sheets) without breaking or transformation
remains underexplored and challenging. Here, we report the design and
implementation of a customized end-effector, i.e., tape print module (TPM), to
realize robot tape manipulation for 3D printing by leveraging the tension
formed on the tape between two endpoints. We showcase the feasibility of
manufacturing representative 2D and 3D structures while utilizing conductive
copper tape for various electronic applications, such as circuits and sensors.
We believe this manipulation strategy could unlock the potential of other tape
materials for manufacturing, including packaging tape and carbon fiber prepreg
tape, and inspire new mechanisms for robot manipulation, 3D printing, and
packaging.
","['Nahid Tushar', 'Rencheng Wu', 'Yu She', 'Wenchao Zhou', 'Wan Shou']"
http://arxiv.org/abs/1807.02921v1,3D printing,2018-07-09T02:52:01Z,2018-07-09T02:52:01Z,"Inferring Quality in Point Cloud-based 3D Printed Objects using
  Topological Data Analysis","  Assessing the quality of 3D printed models before they are printed remains a
challeng- ing problem, particularly when considering point cloud-based models.
This paper introduces an approach to quality assessment, which uses techniques
from the field of Topological Data Analy- sis (TDA) to compute a topological
abstraction of the eventual printed model. Two main tools of TDA, Mapper and
persistent homology, are used to analyze both the printed space and empty space
created by the model. This abstraction enables investigating certain qualities
of the model, with respect to print quality, and identifies potential anomalies
that may appear in the final product.
","['Paul Rosen', 'Mustafa Hajij', 'Junyi Tu', 'Tanvirul Arafin', 'Les Piegl']"
http://arxiv.org/abs/1605.09737v1,3D printing,2016-05-31T17:39:49Z,2016-05-31T17:39:49Z,3D Printed Stencils for Texturing Flat Surfaces,"  We address the problem of texturing flat surfaces by spray-painting through
3D printed stencils. We propose a system that (1) decomposes an image into
alpha-blended layers; (2) computes a stippling given a transparency channel;
(3) generates a 3D printed stencil given a stippling and (4) simulates the
effects of spray-painting through the stencil.
",['Vaibhav Vavilala']
http://arxiv.org/abs/2004.12471v2,3D printing,2020-04-26T20:22:31Z,2020-07-11T14:12:22Z,3D Printed Lightweight Composite Foams,"  The goal of this paper is to enable 3D printed lightweight composite foams by
blending hollow glass micro balloons (GMB) with high density polyethylene
(HDPE). To that end, lightweight feedstock for printing syntactic foam
composites is developed. The blend for this is prepared by varying GMB content
(20, 40, and 60 volume %) in HDPE for filament extrusion, which is subsequently
used for three-dimensional printing (3DP). The rheological properties and the
melt flow index (MFI) of blends are investigated for identifying suitable
printing parameters. It is observed that the storage and loss modulus, as well
as complex viscosity, increases with increasing GMB content, whereas MFI
decreases. Further, the coefficient of thermal expansion of HDPE and foam
filaments decreases with increasing GMB content, thereby lowering the thermal
stresses in prints, which promotes the reduction in warpage. The mechanical
properties of filaments are determined by subjecting them to tensile tests,
whereas 3D printed samples are tested under tensile and flexure tests. The
tensile modulus of the filament increases with increasing GMB content (8-47%)
as compared to HDPE and exhibit comparable filament strength. 3D printed foams
show higher specific tensile and flexural modulus as compared to neat HDPE,
making them suitable candidate materials for weight sensitive applications.
HDPE having 60% by volume GMB exhibited the highest modulus and is 48.02%
higher than the printed HDPE. Finally, the property map reveals higher modulus
and comparable strength against injection and compression molded foams. Printed
foam registered 1.8 times higher modulus than molded samples. Hence, 3D printed
foams have the potential for replacing components processed through
conventional manufacturing processes that have limitations on geometrically
complex designs, lead time, and associated costs.
","['Bharath H S', 'Dileep Bonthu', 'Pavana Prabhakar', 'Mrityunjay Doddamani']"
http://arxiv.org/abs/1605.04797v2,3D printing,2016-05-16T15:09:19Z,2016-07-02T03:15:10Z,"Thingi10K: A Dataset of 10,000 3D-Printing Models","  Empirically validating new 3D-printing related algorithms and implementations
requires testing data representative of inputs encountered \emph{in the wild}.
An ideal benchmarking dataset should not only draw from the same distribution
of shapes people print in terms of class (e.g., toys, mechanisms, jewelry),
representation type (e.g., triangle soup meshes) and complexity (e.g., number
of facets), but should also capture problems and artifacts endemic to 3D
printing models (e.g., self-intersections, non-manifoldness). We observe that
the contextual and geometric characteristics of 3D printing models differ
significantly from those used for computer graphics applications, not to
mention standard models (e.g., Stanford bunny, Armadillo, Fertility). We
present a new dataset of 10,000 models collected from an online 3D printing
model-sharing database. Via analysis of both geometric (e.g., triangle aspect
ratios, manifoldness) and contextual (e.g., licenses, tags, classes)
characteristics, we demonstrate that this dataset represents a more concise
summary of real-world models used for 3D printing compared to existing
datasets. To facilitate future research endeavors, we also present an online
query interface to select subsets of the dataset according to project-specific
characteristics. The complete dataset and per-model statistical data are freely
available to the public.
","['Qingnan Zhou', 'Alec Jacobson']"
http://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,"  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.
","['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']"
http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation.
",['Takanobu Mizuta']
http://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,"  Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.
",['Tshilidzi Marwala']
http://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,"  A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed.
",['Mark McPherson']
http://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","  This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997
","['Dan Geiger', 'Prakash Shenoy']"
http://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993
","['David Heckerman', 'E. Mamdani']"
http://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","  This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986
","['Laveen Kanal', 'John Lemmer']"
http://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,"  In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind.
",['Michael Swan Laufer']
http://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018
","['Frank Stein', 'Alun Preece', 'Mihai Boicu']"
http://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
","['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']"
http://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
",['Lana Sinapayen']
http://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
","['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']"
http://arxiv.org/abs/2007.07710v1,Artificial intelligence,2020-07-11T14:06:13Z,2020-07-11T14:06:13Z,Human $\neq$ AGI,"  Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction.
",['Roman V. Yampolskiy']
http://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
","['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']"
http://arxiv.org/abs/1712.06440v1,Artificial intelligence,2017-12-14T17:49:04Z,2017-12-14T17:49:04Z,Three IQs of AI Systems and their Testing Methods,"  The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.
","['Feng Liu', 'Yong Shi', 'Ying Liu']"
http://arxiv.org/abs/2108.04770v1,Artificial intelligence,2021-08-10T16:24:30Z,2021-08-10T16:24:30Z,"Examining correlation between trust and transparency with explainable
  artificial intelligence","  Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence.
",['Arnav Kartikeya']
http://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","['Michael Timothy Bennett', 'Yoshihiro Maruyama']"
http://arxiv.org/abs/1205.2596v2,Artificial intelligence,2012-05-11T18:35:50Z,2014-08-28T04:30:01Z,"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","  This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.
","['Fabio Cozman', 'Avi Pfeffer']"
http://arxiv.org/abs/1205.2597v2,Artificial intelligence,2012-05-11T18:40:29Z,2014-08-28T04:29:00Z,"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","  This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010.
","['Peter Grunwald', 'Peter Spirtes']"
http://arxiv.org/abs/1206.3959v2,Artificial intelligence,2012-06-13T16:43:44Z,2014-08-28T04:27:28Z,"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","  This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009.
","['Jeff Bilmes', 'Andrew Ng']"
http://arxiv.org/abs/1808.03413v1,Augmented reality,2018-08-10T05:23:37Z,2018-08-10T05:23:37Z,Inverse Augmented Reality: A Virtual Agent's Perspective,"  We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.
","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/1903.02723v1,Augmented reality,2019-03-07T04:29:50Z,2019-03-07T04:29:50Z,"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","  In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.
","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']"
http://arxiv.org/abs/2104.08579v2,Augmented reality,2021-04-17T15:47:48Z,2021-05-04T17:29:28Z,"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","  When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.
","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']"
http://arxiv.org/abs/2101.02565v1,Augmented reality,2021-01-07T14:43:51Z,2021-01-07T14:43:51Z,Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"  Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.
",['Nico Feld']
http://arxiv.org/abs/1106.5571v1,Augmented reality,2011-06-28T06:08:38Z,2011-06-28T06:08:38Z,Mobile Augmented Reality Applications,"  Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.
","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']"
http://arxiv.org/abs/1807.00279v1,Augmented reality,2018-07-01T06:51:23Z,2018-07-01T06:51:23Z,"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","  The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.
","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']"
http://arxiv.org/abs/2112.11190v1,Augmented reality,2021-12-03T20:46:50Z,2021-12-03T20:46:50Z,"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","  Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.
","['Omid Ziaee', 'Mohsen Hamedi']"
http://arxiv.org/abs/1106.5569v1,Augmented reality,2011-06-28T05:57:37Z,2011-06-28T05:57:37Z,Augmented Reality Implementation Methods in Mainstream Applications,"  Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.
","['David Prochazka', 'Tomas Koubek']"
http://arxiv.org/abs/1807.10659v1,Augmented reality,2018-07-23T12:36:54Z,2018-07-23T12:36:54Z,"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","  The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.
","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']"
http://arxiv.org/abs/1912.12101v1,Augmented reality,2019-12-27T13:56:13Z,2019-12-27T13:56:13Z,"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","  Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.
","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']"
http://arxiv.org/abs/1708.05006v1,Augmented reality,2017-08-16T09:40:53Z,2017-08-16T09:40:53Z,A Survey of Augmented Reality Navigation,"  Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.
",['Gaurav Bhorkar']
http://arxiv.org/abs/2201.07003v1,Augmented reality,2022-01-13T16:54:36Z,2022-01-13T16:54:36Z,"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","  The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).
","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']"
http://arxiv.org/abs/1305.5534v1,Augmented reality,2013-05-23T20:00:00Z,2013-05-23T20:00:00Z,Augmented Reality in Astrophysics,"  Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.
","['Frédéric P. A. Vogt', 'Luke J. Shingles']"
http://arxiv.org/abs/1508.02606v1,Augmented reality,2015-08-11T14:17:28Z,2015-08-11T14:17:28Z,InAR:Inverse Augmented Reality,"  Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.
","['Hao Hu', 'Hainan Cui']"
http://arxiv.org/abs/1508.04238v1,Augmented reality,2015-08-18T08:18:55Z,2015-08-18T08:18:55Z,Preprint ARPPS Augmented Reality Pipeline Prospect System,"  This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.
","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']"
http://arxiv.org/abs/1806.09316v1,Augmented reality,2018-06-25T08:01:45Z,2018-06-25T08:01:45Z,Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"  Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.
","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']"
http://arxiv.org/abs/2109.02386v1,Augmented reality,2021-08-07T17:27:13Z,2021-08-07T17:27:13Z,Augmented Reality for Education: A Review,"  Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.
",['Carlo H. Godoy Jr']
http://arxiv.org/abs/1807.01966v2,Augmented reality,2018-07-05T12:42:24Z,2018-12-03T16:45:21Z,The Cloud Technologies and Augmented Reality: the Prospects of Use,"  The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.
","['Maiia V. Popel', 'Mariya P. Shyshkina']"
http://arxiv.org/abs/1810.10206v1,Augmented reality,2018-10-24T06:23:46Z,2018-10-24T06:23:46Z,"Immercity: a curation content application in Virtual and Augmented
  reality","  When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.
","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']"
http://arxiv.org/abs/1808.06465v3,Augmented reality,2018-08-08T05:46:18Z,2021-05-03T09:04:40Z,"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","  The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.
","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']"
http://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,"  Recently, blockchain has been applied in various fields to secure data
exchanges and storage in decentralized systems. In a blockchain application
where the task of the application which makes use of the data stored in a
blockchain has to be accomplished by a time instant, the employed blockchain is
essentially finitely-long. In this paper, we consider a general finitely-long
blockchain model which is generalized from most existing works on finitely-long
blockchain applications, and take the first step towards characterizing the
vulnerability of finitely-long blockchains in securing data against
double-spending attacks. For the first time, we develop a general closed-form
expression for the probability of success in launching a double-spending attack
on a finitely-long blockchain. This probability essentially characterizes the
vulnerability of finitely-long blockchains. Then, we prove that the probability
of success in launching a double-spending attack on a finitely-long blockchain
is no greater than that on an infinitely-long blockchain, which implies that
finitely-long blockchains are less vulnerable to double-spending attacks than
infinitely-long blockchains. Moreover, we show that unlike infinitely-long
blockchains which can be surely paralyzed by a 51% attack, finitely-long
blockchains are more resistant to 51% attacks.
","['Yiming Jiang', 'Jiangfan Zhang']"
http://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,"  The suitability of a particular blockchain for a given use case depends
mainly on the blockchain's functional and non-functional properties. Such
properties may vary over time, and thus, a selected blockchain may become
unsuitable for a given use case. This uncertainty may hinder the widespread
adoption of blockchain technologies in general. To mitigate the impact of
volatile blockchain properties, we propose a framework that monitors several
blockchains, allows the user to define functional and non-functional
requirements, determines the most appropriate blockchain, and enables the
switchover to that chain at runtime. Our evaluation using a reference
implementation shows that switching to another blockchain can save cost and
enable users to benefit from better performance and a higher level of trust.
","['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']"
http://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,"Blockchain Technologies for Smart Energy Systems: Fundamentals,
  Challenges and Solutions","  In this paper, we discuss the integration of blockchain in smart energy
systems. We present various blockchain technology solutions, review important
blockchain platforms, and several blockchain based smart energy projects in
different smart energy domains. The majority of blockchain platforms with
embedded combination of blockchain technology solutions are computing- and
resource- intensive, and hence not entirely suitable for smart energy
applications. We consider the requirements of smart energy systems and
accordingly identify appropriate blockchain technology solutions for smart
energy applications. Our analysis can help in the development of flexible
blockchain platforms for smart energy systems.
","['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']"
http://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,"  Current blockchain technologies provide very limited means of
interoperability. In particular, solutions enabling blockchains to verify the
existence of data on other blockchains are either very costly or are not fully
decentralized. To overcome these limitations, we introduce Testimonium, a novel
blockchain relay scheme that applies a validation-on-demand pattern and the
on-chain execution of Simplified Payment Verifications to enable the
verification of data across blockchains while remaining fully decentralized.
Evaluating the scheme for Ethereum-based blockchains shows that Testimonium
achieves a cost reduction of up to 92% over existing solutions. As such, the
scheme lays a strong foundation for generic blockchain interoperability. For
instance, it enables the development of an atomic-commit protocol for
distributed transactions across blockchains.
","['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']"
http://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability
  Prediction","  Blockchain and blockchain-based decentralized applications are attracting
increasing attentions recently. In public blockchain systems, users usually
connect to third-party peers or run a peer to join the P2P blockchain network.
However, connecting to unreliable blockchain peers will make users waste
resources and even lose millions of dollars of cryptocurrencies. In order to
select the reliable blockchain peers, it is urgently needed to evaluate and
predict the reliability of them. Faced with this problem, we propose H-BRP,
Hybrid Blockchain Reliability Prediction model to extract the blockchain
reliability factors then make personalized prediction for each user.
Large-scale real-world experiments are conducted on 100 blockchain requesters
and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are
released. The experimental results show that the proposed model obtains better
accuracy than other approaches.
","['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']"
http://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,"Managing Blockchain Systems and Applications: A Process Model for
  Blockchain Configurations","  Blockchain is a radical innovation with a unique value proposition that
shifts trust from institutions to algorithms. Still, the potential of
blockchains remains elusive due to knowledge gaps between computer science
research and socio-economic research. Building on information technology
governance literature and the theory of coevolution, this study develops a
process model for blockchain configurations that captures blockchain capability
dimensions and application areas. We demonstrate the applicability of the
proposed blockchain configuration process model on four blockchain projects.
The proposed blockchain configuration process model assists with the selection
and configuration of blockchain systems based on a set of known requirements
for a blockchain project. Our findings contribute to research by bridging
knowledge gaps between computer science and socio-economic research on
blockchain. Specifically, we explore existing blockchain concepts and integrate
them in a process model for blockchain configurations.
","['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']"
http://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
","['Kai Brünnler', 'Dandolo Flumini', 'Thomas Studer']"
http://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,"  Blockchains have recently generated explosive interest from both academia and
industry, with many proposed applications. But descriptions of many these
proposals are more visionary projections than realizable proposals, and even
basic definitions are often missing. We define ""blockchain"" and ""blockchain
network"", and then discuss two very different, well known classes of blockchain
networks: cryptocurrencies and Git repositories. We identify common primitive
elements of both and use them to construct a framework for explicitly
articulating what characterizes blockchain networks. The framework consists of
a set of questions that every blockchain initiative should address at the very
outset. It is intended to help one decide whether or not blockchain is an
appropriate approach to a particular application, and if it is, to assist in
its initial design stage.
",['Ephraim Feig']
http://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain
  Rule","  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain
system which achieves high transaction throughput through a hierarchy of merged
mined blockchains, each operating in parallel on a partition the overall
application state. Most notably, the full PoW available within the network is
applied to all blockchains in BlockReduce, and cross-blockchain state
transitions are enabled seamlessly within the core protocol. This paper shows
that, given a hierarchy of blockchains and its associated security model, the
protocol scales superlinearly in transaction throughput with the number of
blockchains operated by the protocol.
","['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']"
http://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,"  Blockchain and distributed ledger technologies are gaining the interest of
the academy, companies, and institutions. Nonetheless, the path toward
blockchain adoption is not straightforward, as blockchain is a complex
technology that requires revisiting the standard way of addressing problems and
tackling them from a decentralized perspective. Thus, decision-makers adopt
blockchain technology for the wrong reasons or prefer it to more suitable ones.
This work presents a decision framework for blockchain adoption to help
decision-makers decide whether blockchain is applicable, valuable, and
preferable to other technologies. In particular, The decision framework is
composed of a small set of questions that can be answered from a managerial
standpoint and that do not require a deep technical knowledge of
blockchain-related topics.
","['Vittorio Capocasale', 'Guido Perboli']"
http://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for
  Supporting Hierarchical Storage","  The fast developing Industrial Internet of Things (IIoT) technologies provide
a promising opportunity to build large-scale systems to connect numerous
heterogeneous devices into the Internet. Most existing IIoT infrastructures are
based on a centralized architecture, which is easier for management but cannot
effectively support immutable and verifiable services among multiple parties.
Blockchain technology provides many desired features for large-scale IIoT
infrastructures, such as decentralization, trustworthiness, trackability, and
immutability. This paper presents a blockchain-based IIoT architecture to
support immutable and verifiable services. However, when applying blockchain
technology to the IIoT infrastructure, the required storage space posts a grant
challenge to resource-constrained IIoT infrastructures. To address the storage
issue, this paper proposes a hierarchical blockchain storage structure,
\textit{ChainSplitter}. Specially, the proposed architecture features a
hierarchical storage structure where the majority of the blockchain is stored
in the clouds, while the most recent blocks are stored in the overlay network
of the individual IIoT networks. The proposed architecture seamlessly binds
local IIoT networks, the blockchain overlay network, and the cloud
infrastructure together through two connectors, the \textit{blockchain
connector} and the \textit{cloud connector}, to construct the hierarchical
blockchain storage. The blockchain connector in the overlay network builds
blocks in blockchain from data generated in IIoT networks, and the cloud
connector resolves the blockchain synchronization issues between the overlay
network and the clouds. We also provide a case study to show the efficiency of
the proposed hierarchical blockchain storage in a practical Industrial IoT
case.
","['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']"
http://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,"  Blockchain has been widely deployed in various sectors, such as finance,
education, and public services. Since blockchain runs as an immutable
distributed ledger, it has decentralized mechanisms with persistency,
anonymity, and auditability, where transactions are jointly performed through
cryptocurrency-based consensus algorithms by worldwide distributed nodes. There
have been many survey papers reviewing the blockchain technologies from
different perspectives, e.g., digital currencies, consensus algorithms, and
smart contracts. However, none of them have focused on the blockchain data
management systems. To fill in this gap, we have conducted a comprehensive
survey on the data management systems, based on three typical types of
blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed
Acyclic Graph)-based blockchain. We categorize their data management mechanisms
into three layers: blockchain architecture, blockchain data structure, and
blockchain storage engine, where block architecture indicates how to record
transactions on a distributed ledger, blockchain data structure refers to the
internal structure of each block, and blockchain storage engine specifies the
storage form of data on the blockchain system. For each layer, the works
advancing the state-of-the-art are discussed together with technical
challenges. Furthermore, we lay out the future research directions for the
blockchain data management systems.
","['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']"
http://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,"A Consensus Algorithm Based on Risk Assessment Model for Permissioned
  Blockchain","  Blockchain technology enables stakeholders to conduct trusted data sharing
and exchange without a trusted centralized institution. These features make
blockchain applications attractive to enhance trustworthiness in very different
contexts. Due to unique design concepts and outstanding performance, blockchain
has become a popular research topic in industry and academia in recent years.
Every participant is anonymous in a permissionless blockchain represented by
cryptocurrency applications such as Bitcoin. In this situation, some special
incentive mechanisms are applied to permissionless blockchain, such as mined
native cryptocurrency to solve the trust issues of permissionless blockchain.
In many use cases, permissionless blockchain has bottlenecks in transaction
throughput performance, which restricts further application in the real world.
A permissioned blockchain can reach a consensus among a group of entities that
do not establish an entire trust relationship. Unlike permissionless
blockchains, the participants must be identified in permissioned blockchains.
By relying on the traditional crash fault-tolerant consensus protocols,
permissioned blockchains can achieve high transaction throughput and low
latency without sacrificing security. However, how to balance the security and
consensus efficiency is still the issue that needs to be solved urgently in
permissioned blockchains. As the core module of blockchain technology, the
consensus algorithm plays a vital role in the performance of the blockchain
system. Thus, this paper proposes a new consensus algorithm for permissioned
blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with
the decentralized design concept and the risk-node assessment mechanism to
address the unbalance issues of performance in speed, scalability, and
security.
","['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']"
http://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,"  Blockchain technology emerged with the advent of Bitcoin and rapidly
developed over the past few decades, becoming widely accepted and known by the
public. However, in the past decades, the massive adoption of blockchain
technology has yet to come. Rather than the scalability issue, the blockchain
application is challenged by its expensive usage cost. However, the high cost
of blockchain usage is deeply connected with the blockchain consensus and
security mechanism. The permissionless blockchain must maintain its high cost
for security against the 51% Attack. Chain users indirectly cover the cost as
coins are appointed for blockchain usage fees. This conflict prevents the
massive adoption of blockchain. Thus, blockchain must be improved to solve
those problems: 1. The cost of blockchain usage should be low enough. 2. The
blockchain should remain decentralized. 3. The scalability of blockchain must
meet the demand.
  In my thesis, new approaches are applied to solve the issues above. The key
contribution is the discovery of the useful PoW. It extends the Nakamoto PoW
with another usage of file data encoding during the same Nakamoto Consensus
computation to prove honest data preservation. Based on this theory, a
permissionless storage network is proposed as the new security engine for the
blockchain. It bridges the high blockchain security cost to the storage users
with real demands who are willing to pay for the storage resource. On the other
hand, the chain users can benefit from the low transaction fee. Meanwhile, we
also provide a scalability solution to shard the blockchain. It enables high
TPS and keeps decentralization. The solutions in this thesis provide the
answers to all the dependencies of the massive adoption.
",['Jia Kan']
http://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies
have been proposed based on blockchain. However, the performance of
permissionless blockchains restricts the widespread of cryptocurrency.
Recently, Libra was proposed by Facebook based on a permissioned blockchain,
i.e. the Libra blockchain. The vision of Libra is to become a global currency
supporting financial applications, but it is doubted whether the performance of
the Libra blockchain is able to support frequent micropayment scenarios. In
this paper, we propose a methodology to evaluate the performance of blockchain
platforms and conducted an experimental study on the Libra blockchain. The
results show that the Libra blockchain can only process about one thousand
transactions per second at most, and the performance drops significantly as the
number of validators increases. Although it outperforms permissionless
blockchain platforms, the performance of the Libra blockchain is still
unsatisfactory compared to other permissioned blockchains like Hyperledger
Fabric and needs to make effective improvements in order to support global
micropayment in the future.
","['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']"
http://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,"  Blockchain is an incrementally updated ledger maintained by distributed nodes
rather than centralized organizations. The current blockchain technology faces
scalability issues, which include two aspects: low transaction throughput and
high storage capacity costs. This paper studies the blockchain structure based
on state sharding technology, and mainly solves the problem of non-scalability
of block chain storage. This paper designs and implements the blockchain state
sharding scheme, proposes a specific state sharding data structure and
algorithm implementation, and realizes a complete blockchain structure so that
the blockchain has the advantages of high throughput, processing a large number
of transactions and saving storage costs. Experimental results show that a
blockchain network with more than 100,000 nodes can be divided into 1024
shards. A blockchain network with this structure can process 500,000
transactions in about 5 seconds. If the consensus time of the blockchain is
about 10 seconds, and the block generation time of the blockchain system of the
sharding mechanism is 15 seconds, the transaction throughput can reach 33,000
tx/sec. Experimental results show that the throughput of the proposed protocol
increases with the increase of the network node size. This confirms the
scalability of the blockchain structure based on sharding technology.
","['Xiangyu Wang', 'Ting Yang', 'Yu Wang']"
http://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,"  Blockchain's evolution during the past decade is astonishing: from bitcoin to
over 2.000 altcoins, and from decentralised electronic payments to transactions
programmable by smart contracts and complex tokens governed by decentralised
organisations. While the new generation of blockchain applications is still
evolving, blockchain's technical characteristics are also advancing. Yet,
immutability, a hitherto indisputable property according to which blockchain
data cannot be edited nor deleted, remains the cornerstone of blockchain's
security. Nevertheless, blockchain's immutability is being called into question
lately in the light of the new erasing requirements imposed by the GDPR's
``\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges
blockchain data to be editable in order restricted content redactions,
modifications or deletions to be applied when requested, blockchains compliance
with the regulation is indeed challenging, if not impracticable. Towards
resolving this contradiction, various methods and techniques for mutable
blockchains have been proposed in an effort to satisfy regulatory erasing
requirements while preserving blockchains' security. To this end, this work
aims to provide a comprehensive review on the state-of-the-art research
approaches, technical workarounds and advanced cryptographic techniques that
have been put forward to resolve this conflict and to discuss their potentials,
constraints and limitations when applied in the wild to either permissioned or
permissionless blockchains.
","['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']"
http://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain
  Transactions","  The interoperability across multiple blockchains would play a critical role
in future blockchain-based data management paradigm. Existing techniques either
work only for two blockchains or requires a centralized component to govern the
cross-blockchain transaction execution, neither of which would meet the
scalability requirement. This paper proposes a new distributed commit protocol,
namely \textit{cross-blockchain transaction} (CBT), for conducting transactions
across an arbitrary number of blockchains without any centralized component.
The key idea of CBT is to extend the two-phase commit protocol with a heartbeat
mechanism to ensure the liveness of CBT without introducing additional nodes or
blockchains. We have implemented CBT and compared it to the state-of-the-art
protocols, demonstrating CBT's low overhead (3.6\% between two blockchains,
less than $1\%$ among 32 or more blockchains) and high scalability (linear
scalability on up to 64-blockchain transactions). In addition, we developed a
graphic user interface for users to virtually monitor the status of the
cross-blockchain transactions.
","['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']"
http://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,"  Blockchain has been touted as a revolutionary technology. However, despite
the excitement, blockchain has not been adopted in many fields. Many are
hesitant to adopt blockchain technology due to privacy concerns, barriers to
use, or lack of practical use cases. In this work, we outline a potential
blockchain use case for tracking financial transactions across multiple
financial institutions. We show the downsides of traditional centralized
approaches and that blockchain approaches fail to give all the privacy and
accessibility required for this use case. Thus we propose a novel blockchain
architecture to support our use case. This novel architecture combines the ease
of use of public blockchains with the privacy of private blockchains by
allowing users to create personal blockchains. We believe this novel personal
blockchain architecture will lead to more blockchain adoption, particularly in
use cases handling private data.
","['Collin Connors', 'Dilip Sarkar']"
http://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,"  A key constraint that limits the implementation of blockchain in Internet of
Things (IoT) is its large storage requirement resulting from the fact that each
blockchain node has to store the entire blockchain. This increases the burden
on blockchain nodes, and increases the communication overhead for new nodes
joining the network since they have to copy the entire blockchain. In order to
reduce storage requirements without compromising on system security and
integrity, coded blockchains, based on error correcting codes with fixed rates
and lengths, have been recently proposed. This approach, however, does not fit
well with dynamic IoT networks in which nodes actively leave and join. In such
dynamic blockchains, the existing coded blockchain approaches lead to high
communication overheads for new joining nodes and may have high decoding
failure probability. This paper proposes a rateless coded blockchain with
coding parameters adjusted to network conditions. Our goals are to minimize
both the storage requirement at each blockchain node and the communication
overhead for each new joining node, subject to a target decoding failure
probability. We evaluate the proposed scheme in the context of real-world
Bitcoin blockchain and show that both storage and communication overhead are
reduced by 99.6\% with a maximum $10^{-12}$ decoding failure probability.
","['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']"
http://arxiv.org/abs/2001.02306v1,Cancer vaccine,2020-01-07T22:33:09Z,2020-01-07T22:33:09Z,"Examining Potential Usability and Health Beliefs Among Young Adults
  Using a Conversational Agent for HPV Vaccine Counseling","  The human papillomavirus (HPV) vaccine is the most effective way to prevent
HPV-related cancers. Integrating provider vaccine counseling is crucial to
improving HPV vaccine completion rates. Automating the counseling experience
through a conversational agent could help improve HPV vaccine coverage and
reduce the burden of vaccine counseling for providers. In a previous study, we
tested a simulated conversational agent that provided HPV vaccine counseling
for parents using the Wizard of OZ protocol. In the current study, we assessed
the conversational agent among young college adults (n=24), a population that
may have missed the HPV vaccine during their adolescence when vaccination is
recommended. We also administered surveys for system and voice usability, and
for health beliefs concerning the HPV vaccine. Participants perceived the agent
to have high usability that is slightly better or equivalent to other voice
interactive interfaces, and there is some evidence that the agent impacted
their beliefs concerning the harms, uncertainty, and risk denials for the HPV
vaccine. Overall, this study demonstrates the potential for conversational
agents to be an impactful tool for health promotion endeavors.
","['Muhammad Amith', 'Rebecca Lin', 'Rachel Cunningham', 'Qiwei Luna Wu', 'Lara S. Savas', 'Yang Gong', 'Julie A. Boom', 'Lu Tang', 'Cui Tao']"
http://arxiv.org/abs/1504.05383v1,Cancer vaccine,2015-04-21T10:54:10Z,2015-04-21T10:54:10Z,"HPV and cervical cancer in Moldova, epidemiological model with
  intervention cost vs benefit and effectiveness analysis","  Human papillomavirus, or HPV, is a sexually transmittable virus infection,
which is necessary risk factor for developing cervical cancer, first most
common type of cancer in working age women in Moldova. We observe both
behavioral change (sexuality increase) and demographical change (population
ageing). We used data since 1998 (Moldovan peace treaty) to adjust model
parameter and we project till around 2030 (for vaccination till 2050).
According to provided information, interdisciplinary model was proposed. It iss
set of deterministic differential equations. Stochasticity was introduced in
sexual partner change rates. The model has aggregated the most important paths
of infection, cancer development and prevention scenarios (more than 100
equations and 200 parameters). Moldovan cervical cancer perspective looks much
better, than in central western Europe countries, because of relatively young
society. In our setup, obligatory vaccination seems to not be so crucial (for
none of realistic scenarios increase of cancer cases is possible) for public
health, as in most countries in European Union. However, screening practice
could be verified in terms of efficiency, when cost benefit calculation would
be done. We propose more optimal screening guidelines (with prevention cost 5
-10k EUR per QALY), which could provide saving perspective in 10-15 year in
range 150-300k EUR yearly. Targeted vaccination could be also consider, because
costs are similar to high frequencies screening schema with the same cancer
cases projection. However, some positive side effects of vaccination as
reduction of pathogen circulation in society, will cause decrease of other
pathologies related to HPV like genital warts and other cancer.
",['Andrzej Jarynowski']
http://arxiv.org/abs/q-bio/0605046v3,Cancer vaccine,2006-05-29T06:53:50Z,2008-10-15T08:12:44Z,Different Strategies for Cancer Treatment: Mathematical Modeling,"  We formulate and analyze a mathematical model describing immune response to
avascular tumor under the influence of immunotherapy and chemotherapy and their
combinations as well as vaccine treatments. The effect of vaccine therapy is
considered as a parametric perturbation of the model. In the case of a weak
immune response, neither immunotherapy nor chemotherapy is found to cause tumor
regression to a small size, which would be below the clinically detectable
threshold. Numerical simulations show that the efficiency of vaccine therapy
depends on both the tumor size and the condition of immune system as well as on
the response of the organism to vaccination. In particular, we found that
vaccine therapy becomes more effective when used without time delay from a
prescribed date of vaccination after surgery and is ineffective without
preliminary treatment. For a strong immune response, our model predicts the
tumor remission under vaccine therapy. Our study of successive chemo/immuno,
immuno/chemo and concurrent chemoimmunotherapy shows that the chemo/immuno
sequence is more effective while concurrent chemoimmunotherapy is more sparing.
","['O. G. Isaeva', 'V. A. Osipov']"
http://arxiv.org/abs/2207.06257v1,Cancer vaccine,2022-07-13T14:56:01Z,2022-07-13T14:56:01Z,Stochastic and parameter analysis for an integrative cancer model,"  In a previous work, we presented a model that integrates cancer cell
differentiation and immunotherapy, analysing a particular therapy against
cancer stem cells by cytotoxic cell vaccines. As every biological system is
exposed to random fluctuations, it is important to study its stochasticity. The
influence of demographic and multiplicative noise in the system is carry out on
the parameters of reproduction and death in cancer cells. On the other hand, we
incorporated fluctuations by adding multiplicative noise. In both cases, we
analysed the dynamics for different values of the parameters involved. The
final amount of cancer cells decreases for different combinations of these
parameters and noise intensity is found.
","['Marcela Reale', 'David Margarit', 'Ariel Scagliotti', 'Lilia Romanelli']"
http://arxiv.org/abs/2411.00885v1,Cancer vaccine,2024-10-31T18:11:57Z,2024-10-31T18:11:57Z,"Revolutionizing Personalized Cancer Vaccines with NEO: Novel Epitope
  Optimization Using an Aggregated Feed Forward and Recurrent Neural Network
  with LSTM Architecture","  As cancer cases continue to rise, with a 2023 study from Zhejiang and Harvard
predicting a 31 percent increase in cases and a 21 percent increase in deaths
by 2030, the need to find more effective treatments for cancer is greater than
ever before. Traditional approaches to treating cancer, such as chemotherapy,
often kill healthy cells because of their lack of targetability. In contrast,
personalized cancer vaccines can utilize neoepitopes - distinctive peptides on
cancer cells that are often missed by the body's immune system - that have
strong binding affinities to a patient's MHC to provide a more targeted
treatment approach. The selection of optimal neoepitopes that elicit an immune
response is a time-consuming and costly process due to the required inputs of
modern predictive methods. This project aims to facilitate faster, cheaper, and
more accurate neoepitope binding predictions using Feed Forward Neural Networks
(FFNN) and Recurrent Neural Networks (RNN).
  To address this, NEO was created. NEO requires next-generation sequencing
data and uses a stacking ensemble method by calculating scores from
state-of-the-art models (MHCFlurry 1.6, NetMHCstabpan 1.0, and IEDB). The
model's architecture includes an FFNN and an RNN with LSTM layers capable of
analyzing both sequential and non-sequential data. The results from both models
are aggregated to produce predictions. Using this model, personalized cancer
vaccines can be produced with improved results (AUC = 0.9166, recall = 91.67
percent).
",['Nishanth Basava']
http://arxiv.org/abs/1607.08656v1,Cancer vaccine,2016-07-28T22:35:20Z,2016-07-28T22:35:20Z,Identifying Unvaccinated Individuals in Canada: A Predictive Model,"  Recently, the media and public health officials have become increasingly
aware of the rise in anti-vaccine sentiment. Vaccinations have numerous health
benefits for immunized individuals as well as for the general public through
herd immunity. Given the rise in immunization-preventable diseases, a
consequence of people opting out of their routine vaccinations, we determined
that Canadian health data can identify individuals over the age of 60 who chose
not to get vaccinated (80.1% negative predictive value) and individuals under
the age of 60 who have recently been vaccinated (96.4% positive predictive
value). Using the 2009-2014 Canadian Community Health Surveys (CCHS), a probit
model identified the variables that were most commonly associated with flu
vaccination outcomes. Of 1,381 variables, 47 with the most significant marginal
effects were selected, including the presence of diseases (e.g. diabetes and
cancer), behavioral characteristics (e.g. smoking and exercise), exposure to
the medical system (e.g. whether the individual gets a regular check-up), and a
person's living situation (e.g. having young children in the household). These
variables were then used to generate a Random Forest classification model,
trained on the 2009-2013 dataset, and tested on the 2014 dataset. We achieved
an overall accuracy of 87.8% between the two final models, each using 25
classification trees with bounded depth of 20 nodes, randomly selecting from
all 47 variables. With the two proposed policies, this model can be leveraged
to efficiently allocate vaccination promotion efforts. Additionally, it can be
applied to future surveys, only requiring 3.6% of the variables in the CCHS for
successful prediction.
","['Kevin Dick', 'Ardyn Nordstrom']"
http://arxiv.org/abs/2502.09659v1,Cancer vaccine,2025-02-12T06:30:31Z,2025-02-12T06:30:31Z,"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature
  using Large Language Models","  Motivation: An adjuvant is a chemical incorporated into vaccines that
enhances their efficacy by improving the immune response. Identifying adjuvant
names from cancer vaccine studies is essential for furthering research and
enhancing immunotherapies. However, the manual curation from the constantly
expanding biomedical literature poses significant challenges. This study
explores the automated recognition of vaccine adjuvant names using Large
Language Models (LLMs), specifically Generative Pretrained Transformers (GPT)
and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97
clinical trial records from AdjuvareDB and 290 abstracts annotated with the
Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in
zero-shot and few-shot learning paradigms with up to four examples per prompt.
Prompts explicitly targeted adjuvant names, testing the impact of contextual
information such as substances or interventions. Outputs underwent automated
and manual validation for accuracy and consistency. Results: GPT-4o attained
100% Precision across all situations while exhibiting notable improve in Recall
and F1-scores, particularly with incorporating interventions. On the VAC
dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,
surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o
reached an F1-score of 81.67% for three-shot prompting with interventions,
surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings
demonstrate that LLMs excel at identifying adjuvant names, including rare
variations of naming representation. This study emphasizes the capability of
LLMs to enhance cancer vaccine development by efficiently extracting insights.
Future work aims to broaden the framework to encompass various biomedical
literature and enhance model generalizability across various vaccines and
adjuvants.
","['Hasin Rehana', 'Jie Zheng', 'Leo Yeh', 'Benu Bansal', 'Nur Bengisu Çam', 'Christianah Jemiyo', 'Brett McGregor', 'Arzucan Özgür', 'Yongqun He', 'Junguk Hur']"
http://arxiv.org/abs/1602.08111v1,Cancer vaccine,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1710.06817v1,Cancer vaccine,2017-10-18T16:33:33Z,2017-10-18T16:33:33Z,"Using MRI Cell Tracking to Monitor Immune Cell Recruitment in Response
  to a Peptide-Based Cancer Vaccine","  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
  Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
  Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
  Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
","['Marie-Laurence Tremblay', 'Christa Davis', 'Chris V. Bowen', 'Olivia Stanley', 'Cathryn Parsons', 'Genevieve Weir', 'Mohan Karkada', 'Marianne M. Stanford', 'Kimberly D. Brewer']"
http://arxiv.org/abs/1303.4383v1,Cancer vaccine,2013-03-16T16:54:18Z,2013-03-16T16:54:18Z,"Hierarchical hydropathic evolution of influenza glycoproteins (N2, H3,
  A/H3N2) under relentless vaccination pressure","  Hemagglutinin (HA) and neuraminidase (NA) are highly variable envelope
glycoproteins. Here hydropathic analysis, previously applied to quantify common
flu (H1N1) evolution (1934-), is applied to the evolution of less common but
more virulent (avian derived) H3N2 (1968-), beginning with N2. Whereas N1
exhibited opposing migration and vaccination pressures, the dominant N2 trend
is due to vaccination, with only secondary migration interactions. Separation
and evaluation of these effects is made possible by the use of two distinct
hydropathic scales representing first-order and second-order thermodynamic
interactions. The evolutions of H1 and H3 are more complex, with larger
competing migration and vaccination effects. The linkages of H3 and N2
evolutionary trends are examined on two modular length scales, medium
(glycosidic) and large (corresponding to sialic acid interactions). The
hierarchical hydropathic results complement and greatly extend advanced
phylogenetic results obtained from similarity studies. They exhibit simple
quantitative trends that can be transferred to engineer oncolytic properties of
other viral proteins to treat recalcitrant cancers.
",['J. C. Phillips']
http://arxiv.org/abs/2209.07527v2,Cancer vaccine,2022-09-14T11:29:15Z,2022-10-28T07:42:08Z,"Improved proteasomal cleavage prediction with positive-unlabeled
  learning","  Accurate in silico modeling of the antigen processing pathway is crucial to
enable personalized epitope vaccine design for cancer. An important step of
such pathway is the degradation of the vaccine into smaller peptides by the
proteasome, some of which are going to be presented to T cells by the MHC
complex. While predicting MHC-peptide presentation has received a lot of
attention recently, proteasomal cleavage prediction remains a relatively
unexplored area in light of recent advancesin high-throughput mass
spectrometry-based MHC ligandomics. Moreover, as such experimental techniques
do not allow to identify regions that cannot be cleaved, the latest predictors
generate decoy negative samples and treat them as true negatives when training,
even though some of them could actually be positives. In this work, we thus
present a new predictor trained with an expanded dataset and the solid
theoretical underpinning of positive-unlabeled learning, achieving a new
state-of-the-art in proteasomal cleavage prediction. The improved predictive
capabilities will in turn enable more precise vaccine development improving the
efficacy of epitope-based vaccines. Pretrained models are available on GitHub
","['Emilio Dorigatti', 'Bernd Bischl', 'Benjamin Schubert']"
http://arxiv.org/abs/2306.13582v1,Cancer vaccine,2023-06-23T16:09:31Z,2023-06-23T16:09:31Z,"Heat shock proteins may be a missing link between febrile infection and
  cancer tumor rejection via autoantigen molecular mimicry","  Numerous epidemiological studies suggest febrile infections could confer
long-term immunity to certain types of cancers, though the precise mechanisms
for this phenomenon remain unclear. Systemic heat-shock responses to fever may
be key to understanding the overlapping outcomes of immune responses to
infection and cancer. To investigate this hypothesis, we performed epitope
discovery between heat-shock proteins (HSP) and cancer-associated antigens
(CAA) and annotated the results with experimentally validated epitopes in the
Immune Epitope Database (IEDB) (Vita et al., 2019). Further, epitopes were
matched with their homologs in human pathogens. Results identified 94 epitopes
shared between HSPs and CAAs, with experimental evidence of presentation at MHC
molecules and with high homology to several epitopes of human pathogens. The
identified epitopes can be used as candidates for designing cancer vaccines.
They may also be used to identify autoreactive antibodies or TCR specificities
that, as antibody drugs and cell therapies, would reproduce the effect of
febrile infection in conferring cancer immunity. Our results support the
hypothesis that the loss of self-tolerance to HSPs during febrile infection
confers tumor immunity through molecular mimicry.
",['Amin Zia']
http://arxiv.org/abs/1904.08514v2,Cancer vaccine,2019-04-17T21:50:03Z,2019-05-22T15:49:15Z,DeepNovoV2: Better de novo peptide sequencing with deep learning,"  Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level.
","['Rui Qiao', 'Ngoc Hieu Tran', 'Lei Xin', 'Baozhen Shan', 'Ming Li', 'Ali Ghodsi']"
http://arxiv.org/abs/1911.09765v1,Cancer vaccine,2019-11-21T21:49:16Z,2019-11-21T21:49:16Z,"Mixture survival models methodology: an application to cancer
  immunotherapy assessment in clinical trials","  Progress in immunotherapy revolutionized the treatment landscape for advanced
lung cancer, raising survival expectations beyond those that were historically
anticipated with this disease. In the present study, we describe the methods
for the adjustment of mixture parametric models of two populations for survival
analysis in the presence of long survivors. A methodology is proposed in
several five steps: first, it is proposed to use the multimodality test to
decide the number of subpopulations to be considered in the model, second to
adjust simple parametric survival models and mixture distribution models, to
estimate the parameters and to select the best model fitted the data, finally,
to test the hypotheses to compare the effectiveness of immunotherapies in the
context of randomized clinical trials. The methodology is illustrated with data
from a clinical trial that evaluates the effectiveness of the therapeutic
vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced
lung cancer. The mixture survival model allows estimating the presence of a
subpopulation of long survivors that is 44% for vaccinated patients. The
differences between the treated and control group were significant in both
subpopulations (population of short-term survival: p = 0.001, the population of
long-term survival: p = 0.0002). For cancer therapies, where a proportion of
patients achieves long-term control of the disease, the heterogeneity of the
population must be taken into account. Mixture parametric models may be more
suitable to detect the effectiveness of immunotherapies compared to standard
models.
","['Lizet Sanchez', 'Patricia Lorenzo-Luaces', 'Claudia Fonte', 'Agustin Lage']"
http://arxiv.org/abs/1607.07503v1,Cancer vaccine,2016-07-25T23:09:59Z,2016-07-25T23:09:59Z,Genomic data analysis in tree spaces,"  Recently, an elegant approach in phylogenetics was introduced by
Billera-Holmes-Vogtmann that allows a systematic comparison of different
evolutionary histories using the metric geometry of tree spaces. In many
problem settings one encounters heavily populated phylogenetic trees, where the
large number of leaves encumbers visualization and analysis in the relevant
evolutionary moduli spaces. To address this issue, we introduce tree
dimensionality reduction, a structured approach to reducing large phylogenetic
trees to a distribution of smaller trees. We prove a stability theorem ensuring
that small perturbations of the large trees are taken to small perturbations of
the resulting distributions.
  We then present a series of four biologically motivated applications to the
analysis of genomic data, spanning cancer and infectious disease. The first
quantifies how chemotherapy can disrupt the evolution of common leukemias. The
second examines a link between geometric information and the histologic grade
in relapsed gliomas, where longer relapse branches were specific to high grade
glioma. The third concerns genetic stability of xenograft models of cancer,
where heterogeneity at the single cell level increased with later mouse
passages. The last studies genetic diversity in seasonal influenza A virus. We
apply tree dimensionality reduction to 24 years of longitudinally collected
H3N2 hemagglutinin sequences, generating distributions of smaller trees
spanning between three and five seasons. A negative correlation is observed
between the influenza vaccine effectiveness during a season and the variance of
the distributions produced using preceding seasons' sequence data. We also show
how tree distributions relate to antigenic clusters and choice of influenza
vaccine. Our formalism exposes links between viral genomic data and clinical
observables such as vaccine selection and efficacy.
","['Sakellarios Zairis', 'Hossein Khiabanian', 'Andrew J. Blumberg', 'Raul Rabadan']"
http://arxiv.org/abs/1306.2898v1,Cancer vaccine,2013-06-12T17:06:40Z,2013-06-12T17:06:40Z,Defining a Simulation Strategy for Cancer Immunocompetence,"  Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours.
","['Grazziela P. Figueredo', 'Uwe Aickelin']"
http://arxiv.org/abs/1708.08160v1,Cancer vaccine,2017-08-28T01:38:06Z,2017-08-28T01:38:06Z,"Determining Positive Cancer Rescue Mutations in p53 Based Cancers by
  using Artificial Intelligence","  A mutation in a protein-coding gene in DNA can alter the protein structure
coded by the same gene. Structurally altered proteins usually lose their
functions and sometimes gain an undesirable function instead. These types of
mutations and their effects can result in genetic diseases or antibiotic
resistant bacteria, among other health issues. Important curing methods have
been developed for detecting mutations against AIDS as well as genetic
diseases. Another example is the influenza virus. The reasons why a vaccination
developed to fight against influenza does not work the following year are (a)
the mutation of its DNA and (b) the outbreak of the virus after it has been
mutated especially if it is a virus that escaped the vaccinations target. Due
to such reasons, it is highly important to know in advance the location of a
potential mutation in a protein as well as the problems it might cause the
medical sciences. In this study we have used artificial neural networks, which
are one of the latest artificial intelligence technologies, to determine the
effects of cancer mutations. The model we developed has given more successful
results compared to other methods. We foresee that our model will bring a new
dimension to medical research and the medical industry.
","['Kaan Aygen', 'Berkay Celik', 'Umut Eser']"
http://arxiv.org/abs/2505.06067v1,Cancer vaccine,2025-05-09T14:03:41Z,2025-05-09T14:03:41Z,"Oncolytic mechanisms and immunotherapeutic potential of Newcastle
  disease virus in cancer therapy","  Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian
paramyxovirus type 1), is a promising oncolytic agent that selectively targets
and destroys cancer cells while sparing normal tissues. Its oncoselectivity
exploits cancer-specific defects in antiviral defenses, particularly impaired
Type I interferon signaling, and dysregulated apoptotic pathways, enabling
robust viral replication and cytotoxicity in malignancies such as breast,
colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through
caspase activation and triggers immunogenic cell death via damage-associated
molecular patterns, stimulating potent antitumours immune responses.
Additionally, NDVs potential as a vaccine vector, expressing tumours-associated
antigens, offers prospects for prophylactic and therapeutic cancer
applications. This review provides a comprehensive analysis of NDVs morphology,
classification, and molecular biology, focusing on its viral entry and
replication mechanisms in host cells. It explores NDVs interactions with cancer
cells, emphasizing its ability to induce cytotoxicity and immune activation.
Understanding these mechanisms is critical for optimizing NDVs oncolytic
potential and advancing its clinical translation. Future directions include
enhancing NDV through genetic engineering, combining it with therapies like
immune checkpoint inhibitors, and developing personalized medicine approaches
tailored to tumours genomic profiles. These advancements position NDV as a
versatile therapeutic agent in oncolytic virotherapy.
","['Umar Ahmad', 'Surializa Harun', 'Moussa Moise Diagne', 'Syahril Abdullah', 'Khatijah Yusoff', 'Abhi Veerakumarasivam']"
http://arxiv.org/abs/2207.05964v1,Cancer vaccine,2022-07-13T05:05:53Z,2022-07-13T05:05:53Z,"Co-evolution of Vaccination Behavior and Perceived Vaccination Risk can
  lead to a Stag-Hunt like Game","  Voluntary vaccination is effective to prevent infectious diseases from
spreading. Both vaccination behavior and cognition of the vaccination risk play
important roles in individual vaccination decision making. However, it is not
clear how the co-evolution of the two shapes the population-wide vaccination
behavior. We establish a coupled dynamics of epidemic, vaccination behavior and
perceived vaccination risk with three different time scales. We assume that the
increase of vaccination level inhibits the rise of perceived vaccination risk,
and the increase of perceived vaccination risk inhibits the rise of vaccination
level. It is shown that the resulting vaccination behavior is similar to the
stag-hunt game, provided that the basic reproductive ratio is moderate and that
the epidemic dynamics evolves fast. This is in contrast with the previous view
that vaccination is a snowdrift like game. Furthermore, we find that epidemic
breaks out repeatedly and eventually leads to vaccine scares if these three
dynamics evolve on a similar time scale. And we propose some ways to promote
vaccination behavior, such as controlling side-effect bias and perceived
vaccination costs. Our work sheds light on epidemic control via vaccination by
taking into account the co-evolutionary dynamics of cognition and behavior.
","['Yuan Liu', 'Bin Wu']"
http://arxiv.org/abs/1902.01540v1,Cancer vaccine,2019-02-05T04:39:10Z,2019-02-05T04:39:10Z,Vaccination dilemma on an evolving social network,"  Vaccination is crucial for the control of epidemics. Yet it is a social
dilemma since non-vaccinators can benefit from the herd immunity created by the
vaccinators. Thus the optimum vaccination level is not reached via voluntary
vaccination at times. Intensive studies incorporate social networks to study
vaccination behavior, and it is shown that vaccination can be promoted on some
networks. The underlying network, however, is often assumed to be static,
neglecting the dynamical nature of social networks. We investigate the
vaccination behavior on dynamical social networks using both simulations and
mean-field approximations. We find that the more robust the
vaccinator-infected-non-vaccinator links are or the more fragile the
vaccinator-healthy-non-vaccinator links are, the higher the final vaccination
level is. This result is true for arbitrary rationality. Furthermore, we show
that, under strong selection, the vaccination level can be higher than that in
the well-mixed population. In addition, we show that vaccination on evolving
social network is equivalent to the vaccination in well mixed population with a
rescaled basic reproductive ratio. Our results highlight the dynamical nature
of social network on the vaccination behavior, and can be insightful for the
epidemic control.
","['Yuting Wei', 'Yaosen Lin', 'Bin Wu']"
http://arxiv.org/abs/2407.09982v1,Cultured meat,2024-04-30T13:35:18Z,2024-04-30T13:35:18Z,"Artificial intelligence and machine learning applications for cultured
  meat","  Cultured meat has the potential to provide a complementary meat industry with
reduced environmental, ethical, and health impacts. However, major
technological challenges remain which require time- and resource-intensive
research and development efforts. Machine learning has the potential to
accelerate cultured meat technology by streamlining experiments, predicting
optimal results, and reducing experimentation time and resources. However, the
use of machine learning in cultured meat is in its infancy. This review covers
the work available to date on the use of machine learning in cultured meat and
explores future possibilities. We address four major areas of cultured meat
research and development: establishing cell lines, cell culture media design,
microscopy and image analysis, and bioprocessing and food processing
optimization. This review aims to provide the foundation necessary for both
cultured meat and machine learning scientists to identify research
opportunities at the intersection between cultured meat and machine learning.
","['Michael E. Todhunter', 'Sheikh Jubair', 'Ruchika Verma', 'Rikard Saqe', 'Kevin Shen', 'Breanna Duffy']"
http://arxiv.org/abs/2401.02691v1,Cultured meat,2024-01-05T07:46:07Z,2024-01-05T07:46:07Z,"Scaffolding fundamentals and recent advances in sustainable scaffolding
  techniques for cultured meat development","  In cultured meat (CM) products the paramount significance lies in the
fundamental attributes like texture and sensory of the processed end product.
To cater to the tactile and gustatory preferences of real meat, the product
needs to be designed to incorporate its texture and sensory attributes.
Presently CM products are mainly grounded products like sausage, nugget,
frankfurter, burger patty, surimi, and steak with less sophistication and need
to mimic real meat to grapple with the traditional meat market. The existence
of fibrous microstructure in connective and muscle tissues has attracted
considerable interest in the realm of tissue engineering. Scaffolding plays an
important role in CM production by aiding cell adhesion, growth,
differentiation, and alignment. A wide array of scaffolding technologies has
been developed for implementation in the realm of biomedical research. In
recent years researchers also focus on edible scaffolding to ease the process
of CM. However, it is imperative to implement cutting edge technologies like 3D
scaffolds, 3D printing, electrospun nanofibers in order to advance the creation
of sustainable and edible scaffolding methods in CM production, with the
ultimate goal of replicating the sensory and nutritional attributes to mimic
real meat cut. This review discusses recent advances in scaffolding techniques
and biomaterials related to structured CM production and required advances to
create muscle fiber structures to mimic real meat.
  Keywords: Cultured meat, Scaffolding, Biomaterials, Edible scaffolding,
Electrospinning, 3D bioprinting, real meat.
","['AMM Nurul Alam', 'Chan-Jin Kim', 'So-Hee Kim', 'Swati Kumari', 'Eun-Yeong Lee', 'Young-Hwa Hwang', 'Seon-Tea Joo']"
http://arxiv.org/abs/1806.09912v1,Cultured meat,2018-06-26T11:16:15Z,2018-06-26T11:16:15Z,"Boiling, steaming or rinsing? (physics of the Chinese cuisine)","  Some physical aspects of Chinese cuisine are discussed. We start from the
cultural and historical particularities of the Chinese cuisine and technologies
of food production. What is the difference between raw and boiled meat? What is
the difference in the physical processes of heat transfer during steaming of
dumplings and their cooking in boiling water? Why is it possible to cook meat
stripes in a ""hot pot"" in ten seconds, while baking a turkey requires several
hours? This article is devoted to discussion of these questions.
","['Andrey Varlamov', 'Zheng Zhou', 'Yan Chen']"
http://arxiv.org/abs/1306.5104v1,Cultured meat,2013-06-21T11:30:13Z,2013-06-21T11:30:13Z,Preference for meat is not innate in dogs,"  Indian free ranging dogs live in a carbohydrate rich environment as
scavengers in and around human settlements. They rarely hunt and consequently
do not encounter rich sources of protein. Instead they have adapted to a diet
of primarily carbohydrates. As descendants of the exclusively carnivorous
wolves, they are subjected to the evolutionary load of a physiological demand
for proteins. To meet their protein needs they resort to a thumb rule, if it
smells like meat, eat it. Pups face high competition from group and non group
members and are in a phase of rapid growth with high protein demands. Following
the thumb rule, then they can acquire more protein at the cost of increased
competition and reduced supplementary non protein nutrition. However, if the
mother supplements their diet with protein rich regurgitates and milk, then the
pups can benefit by being generalists. Using a choice test in the field we show
that while adults have a clear preference for meat, pups have no such
preference, and they even eat degraded protein eagerly. Thus the thumb rule
used by adult dogs for efficient scavenging is not innate, and needs to be
learned. The thumb rule might be acquired by cultural transmission, through
exposure to meat in the regurgitate of the mother, or while accompanying her on
foraging trips.
","['Anandarup Bhadra', 'Anindita Bhadra']"
http://arxiv.org/abs/2308.02700v2,Cultured meat,2023-08-04T20:35:59Z,2023-08-23T20:01:55Z,"Simultaneous self-organization of arterial and venous networks driven by
  the physics of global power optimization","  Understanding of vascular organization is a long-standing problem in
quantitative biology and biophysics and is essential for the growth of large
cultured tissues. Approaches are needed that (1) make predictions of optimal
arteriovenous networks in order to understand the natural vasculatures that
originate from evolution (2) can design vasculature for 3D printing of cultured
tissues, meats, organoids and organs. I present a method for determining the
globally optimal structure of interlocking arterial and venous (arteriovenous)
networks. The core physics is comprised of the minimization of total power
associated with the whole vascular network, with penalties to stop arterial and
venous segments from intersecting. Specifically, the power needed for
Poiseuille flow through vessels and the metabolic power cost for blood
maintenance are optimized. Simultaneous determination of both arterial and
venous vasculatures is essential to avoid intersections between vessels that
would bypass the capillary network. As proof-of-concept, I examine the optimal
vascular structure for supplying square- and disk-like tissue shapes that would
be suitable for bioprinting in multi-well plates. Features in the trees are
driven by the bifurcation exponent and metabolic constant which affect whether
arteries and veins follow the same or different routes through the tissue. They
also affect the level of tortuosity in the vessels. The method could be used to
understand the distribution of blood vessels within organs, to form the core of
simulations, and combined with 3D printing to generate vasculatures for
arbitrary volumes of cultured tissue and cultured meat.
",['James P. Hague']
http://arxiv.org/abs/2306.13435v1,Cultured meat,2023-06-23T10:58:40Z,2023-06-23T10:58:40Z,"High-throughput design of cultured tissue moulds using a biophysical
  model","  The technique presented here identifies tethered mould designs, optimised for
growing cultured tissue with very highly-aligned cells. It is based on a
microscopic biophysical model for polarised cellular hydrogels. There is an
unmet need for tools to assist mould and scaffold designs for the growth of
cultured tissues with bespoke cell organisations, that can be used in
applications such as regenerative medicine, drug screening and cultured meat.
High-throughput biophysical calculations were made for a wide variety of
computer-generated moulds, with cell-matrix interactions and tissue-scale
forces simulated using a contractile-network dipole-orientation model.
Elongated moulds with central broadening and one of the following tethering
strategies are found to lead to highly-aligned cells: (1) tethers placed within
the bilateral protrusions resulting from an indentation on the short edge, to
guide alignment (2) tethers placed within a single vertex to shrink the
available space for misalignment. As such, proof-of-concept has been shown for
mould and tethered scaffold design based on a recently developed biophysical
model. The approach is applicable to a broad range of cell types that align in
tissues and is extensible for 3D scaffolds.
","['James P. Hague', 'Allison E. Andrews', 'Hugh Dickinson']"
http://arxiv.org/abs/2410.13685v1,Cultured meat,2024-10-17T15:47:12Z,2024-10-17T15:47:12Z,"Label-free prediction of fluorescence markers in bovine satellite cells
  using deep learning","  Assessing the quality of bovine satellite cells (BSCs) is essential for the
cultivated meat industry, which aims to address global food sustainability
challenges. This study aims to develop a label-free method for predicting
fluorescence markers in isolated BSCs using deep learning. We employed a
U-Net-based CNN model to predict multiple fluorescence signals from a single
bright-field microscopy image of cell culture. Two key biomarkers, DAPI and
Pax7, were used to determine the abundance and quality of BSCs. The image
pre-processing pipeline included fluorescence denoising to improve prediction
performance and consistency. A total of 48 biological replicates were used,
with statistical performance metrics such as Pearson correlation coefficient
and SSIM employed for model evaluation. The model exhibited better performance
with DAPI predictions due to uniform staining. Pax7 predictions were more
variable, reflecting biological heterogeneity. Enhanced visualization
techniques, including color mapping and image overlay, improved the
interpretability of the predictions by providing better contextual and
perceptual information. The findings highlight the importance of data
pre-processing and demonstrate the potential of deep learning to advance
non-invasive, label-free assessment techniques in the cultivated meat industry,
paving the way for reliable and actionable AI-driven evaluations.
","['Sania Sinha', 'Aarham Wasit', 'Won Seob Kim', 'Jongkyoo Kim', 'Jiyoon Yi']"
http://arxiv.org/abs/2202.13672v2,Cultured meat,2022-02-28T10:42:45Z,2022-06-08T06:23:38Z,Molecular and colloidal transport in bacterial cellulose hydrogels,"  Bacterial cellulose biofilms are complex networks of strong interwoven
nanofibers that control transport and protect bacterial colonies in the film.
Design of diverse applications of bacterial cellulose films also relies on
understanding and controlling transport through the fiber mesh, and transport
simulations of the films are most accurate when guided by experimental
characterization of the structures and the resultant diffusion inside.
Diffusion through such films is a function of their key microstructural length
scales, determining how molecules, as well as particles and microorganisms,
permeate them. We use microscopy to study the unique bacterial cellulose film
structure and quantify the mobility dynamics of various sizes of tracer
particles and macromolecules. Mobility is hindered within the films, as
confinement and local movement strongly depend on void size relative to
diffusing tracers. The biofilms have a naturally periodic structure of
alternating dense and porous layers of nanofiber mesh, and we tune the
magnitude of the spacing via fermentation conditions. Micron-sized particles
can diffuse through the porous layers, but can not penetrate the dense layers.
Tracer mobility in the porous layers is isotropic, indicating a largely random
pore structure there. Molecular diffusion through the whole film is only
slightly reduced by the structural tortuosity. Knowledge of transport
variations within bacterial cellulose networks can be used to guide design of
symbiotic cultures in these structures and enhance their use in applications
biomedical implants, wound dressings, lab-grown meat, and sensors.
","['Firoozeh Babayekhorasani', 'Maryam Hosseini', 'Patrick T. Spicer']"
http://arxiv.org/abs/2401.07875v1,Cultured meat,2024-01-15T18:08:54Z,2024-01-15T18:08:54Z,Safely and Autonomously Cutting Meat with a Collaborative Robot Arm,"  Labor shortages in the United States are impacting a number of industries
including the meat processing sector. Collaborative technologies that work
alongside humans while increasing production abilities may support the industry
by enhancing automation and improving job quality. However, existing automation
technologies used in the meat industry have limited collaboration potential,
low flexibility, and high cost. The objective of this work was to explore the
use of a robot arm to collaboratively work alongside a human and complete tasks
performed in a meat processing facility. Toward this objective, we demonstrated
proof-of-concept approaches to ensure human safety while exploring the capacity
of the robot arm to perform example meat processing tasks. In support of human
safety, we developed a knife instrumentation system to detect when the cutting
implement comes into contact with meat within the collaborative space. To
demonstrate the capability of the system to flexibly conduct a variety of basic
meat processing tasks, we developed vision and control protocols to execute
slicing, trimming, and cubing of pork loins. We also collected a subjective
evaluation of the actions from experts within the U.S. meat processing
industry. On average the experts rated the robot's performance as adequate.
Moreover, the experts generally preferred the cuts performed in collaboration
with a human worker to cuts completed autonomously, highlighting the benefits
of robotic technologies that assist human workers rather than replace them.
Video demonstrations of our proposed framework can be found here:
https://youtu.be/56mdHjjYMVc
","['Ryan Wright', 'Sagar Parekh', 'Robin White', 'Dylan P. Losey']"
http://arxiv.org/abs/2402.13439v1,Cultured meat,2024-02-21T00:16:08Z,2024-02-21T00:16:08Z,"Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada","  This paper investigates the demand for lamb, beef, pork, and poultry in
Canada, both at the national level and in disaggregated provinces, to identify
meat consumption patterns in different provinces. Meat consumption plays a
significant role in Canada's economy and is an important source of calories for
the population. However, meat demand faces several consumption challenges due
to logistic constraints, as a significant portion of the supply is imported
from other countries. Therefore, there is a need for a better understanding of
the causal relationships underlying lamb, beef, pork, and poultry consumption
in Canada. Until recently, there have been no attempts to estimate meat
consumption at the provincial level in Canada. Different Almost Ideal Demand
System (AIDS) models have been applied for testing specifications to circumvent
several econometric and theoretical problems. In particular, generalized AIDS
and its Quadratic extension QUAIDS methods have been estimated across each
province using the Iterative Linear Least Squares Estimator (ILLE) estimation
Method. Weekly retail meat consumption price and quantity data from 2019 to
2022 have been used for Canada and for each province namely Quebec, Maritime
provinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,
total West (Yukon, Northwest Territory and Nunavut), Alberta,
Manitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent
coefficients and demand elasticities estimates reveal patterns of substitution
and/or complementarity between the four categories of meat. Meat consumption
patterns differ across each province. Results show that the demand for the four
categories of meat is responsive to price changes. Overall, lamb expenditure
was found to be elastic and thus considered a luxury good during the study
period, while the other three categories are considered normal goods across
Canada.
",['Zakary Rodrigue Diakité']
http://arxiv.org/abs/2504.04872v1,Cultured meat,2025-04-07T09:27:37Z,2025-04-07T09:27:37Z,Simulating Persuasive Dialogues on Meat Reduction with Generative Agents,"  Meat reduction benefits human and planetary health, but social norms keep
meat central in shared meals. To date, the development of communication
strategies that promote meat reduction while minimizing social costs has
required the costly involvement of human participants at each stage of the
process. We present work in progress on simulating multi-round dialogues on
meat reduction between Generative Agents based on large language models (LLMs).
We measure our main outcome using established psychological questionnaires
based on the Theory of Planned Behavior and additionally investigate Social
Costs. We find evidence that our preliminary simulations produce outcomes that
are (i) consistent with theoretical expectations; and (ii) valid when compared
to data from previous studies with human participants. Generative agent-based
models are a promising tool for identifying novel communication strategies on
meat reduction-tailored to highly specific participant groups-to then be tested
in subsequent studies with human participants.
","['Georg Ahnert', 'Elena Wurth', 'Markus Strohmaier', 'Jutta Mata']"
http://arxiv.org/abs/2503.08664v1,Cultured meat,2025-03-11T17:50:59Z,2025-03-11T17:50:59Z,"MEAT: Multiview Diffusion Model for Human Generation on Megapixels with
  Mesh Attention","  Multiview diffusion models have shown considerable success in image-to-3D
generation for general objects. However, when applied to human data, existing
methods have yet to deliver promising results, largely due to the challenges of
scaling multiview attention to higher resolutions. In this paper, we explore
human multiview diffusion models at the megapixel level and introduce a
solution called mesh attention to enable training at 1024x1024 resolution.
Using a clothed human mesh as a central coarse geometric representation, the
proposed mesh attention leverages rasterization and projection to establish
direct cross-view coordinate correspondences. This approach significantly
reduces the complexity of multiview attention while maintaining cross-view
consistency. Building on this foundation, we devise a mesh attention block and
combine it with keypoint conditioning to create our human-specific multiview
diffusion model, MEAT. In addition, we present valuable insights into applying
multiview human motion videos for diffusion training, addressing the
longstanding issue of data scarcity. Extensive experiments show that MEAT
effectively generates dense, consistent multiview human images at the megapixel
level, outperforming existing multiview diffusion methods.
","['Yuhan Wang', 'Fangzhou Hong', 'Shuai Yang', 'Liming Jiang', 'Wayne Wu', 'Chen Change Loy']"
http://arxiv.org/abs/2208.13484v1,Cultured meat,2022-08-29T10:37:31Z,2022-08-29T10:37:31Z,"Pasture Intake Protects Against Commercial Diet-induced
  Lipopolysaccharide Production Facilitated by Gut Microbiota through
  Activating Intestinal Alkaline Phosphatase Enzyme in Meat Geese","  In-house feeding system (IHF, a low dietary fiber source) may cause altered
cecal microbiota composition and inflammatory responses in meat geese via
increased endotoxemia (lipopolysaccharides) with reduced intestinal alkaline
phosphatase (ALP) production. The effects of artificial pasture grazing system
(AGF, a high dietary fiber source) on modulating gut microbiota architecture
and gut barrier functions have not been investigated in meat geese. The
intestinal ALP functions to regulate gut microbial homeostasis and barrier
function appears to inhibit pro-inflammatory cytokines by reducing LPS-induced
reactive oxygen species (ROS) production. The purpose of our study was to
investigate whether this enzyme could play a critical role in attenuating ROS
generation and then ROS facilitated NF-\k{appa}B pathway-induced systemic
inflammation in meat geese. First, we assessed the impacts of IHF and AGF on
gut microbial composition via 16 sRNA sequencing in meat geese. In the gut
microbiota analysis, meat geese supplemented with pasture demonstrated a
significant reduction in microbial richness and diversity compared to IHF meat
geese demonstrating antimicrobial, antioxidation, and anti-inflammatory ability
of AGF system. Second host markers analysis through protein expression of serum
and cecal tissues and quantitative PCR of cecal tissues were evaluated. We
confirmed a significant increase in intestinal ALP-induced Nrf2 signaling
pathway representing LPS dephosphorylation mediated TLR4/MyD88 induced ROS
reduction mechanisms in AGF meat geese. Further, the correlation analysis of
top 44 host markers with gut microbiota shows that artificial pasture intake
induced gut barrier functions via reducing ROS-mediated NF-\k{appa}B
pathway-induced gut permeability, systemic inflammation, and aging phenotypes.
","['Qasim Ali', 'Sen Ma', 'Umar Farooq', 'Jiakuan Niu', 'Fen Li', 'Muhammad Abaidullah', 'Boshuai Liu', 'Shaokai La', 'Defeng Li', 'Zhichang Wang', 'Hao Sun', 'Yalei Cui', 'Yinghua Shi']"
http://arxiv.org/abs/2005.12671v1,Cultured meat,2020-04-12T15:43:14Z,2020-04-12T15:43:14Z,"Towards real time assessment of intramuscular fat content in meat using
  optical fibre-based optical coherence tomography","  We consider the use of optical coherence tomography (OCT) imaging to predict
the quality of meat. We find that intramuscular fat (IMF) absorbs infrared
light about nine times stronger than muscle, which enables us to estimate fat
content in intact meat samples. The method is made very efficient by extracting
relevant information from the three-dimensional high-resolution images
generated by OCT using principal component analysis (PCA). The principal
components are then used as regressors into a support vector regression (SVR)
prediction model. The SVR model is found to predict IMF content stably and
accurately, with an R^2 value of 0.94. Our study paves the way for automated,
contact-less, non-destructive, real time classification of the quality of meat
samples.
","['Abi Thampi', 'Sam Hitchman', 'Stéphane Coen', 'Frédérique Vanholsbeeck']"
http://arxiv.org/abs/2210.05358v2,Cultured meat,2022-10-06T15:03:23Z,2022-10-18T10:05:09Z,On estimating Armington elasticities for Japan's meat imports,"  By fully accounting for the distinct tariff regimes levied on imported meat,
we estimate substitution elasticities of Japan's two-stage import aggregation
functions for beef, chicken and pork. While the regression analysis crucially
depends on the price that consumers face, the post-tariff price of imported
meat depends not only on ad valorem duties but also on tariff rate quotas and
gate price system regimes. The effective tariff rate is consequently evaluated
by utilizing monthly transaction data. To address potential endogeneity
problems, we apply exchange rates that we believe to be independent of the
demand shocks for imported meat. The panel nature of the data allows us to
retrieve the first-stage aggregates via time dummy variables, free of demand
shocks, to be used as part of the explanatory variable and as an instrument in
the second-stage regression.
","['Satoshi Nakano', 'Kazuhiko Nishimura']"
http://arxiv.org/abs/2406.14259v1,Cultured meat,2024-06-20T12:28:47Z,2024-06-20T12:28:47Z,"MEAT: Median-Ensemble Adversarial Training for Improving Robustness and
  Generalization","  Self-ensemble adversarial training methods improve model robustness by
ensembling models at different training epochs, such as model weight averaging
(WA). However, previous research has shown that self-ensemble defense methods
in adversarial training (AT) still suffer from robust overfitting, which
severely affects the generalization performance. Empirically, in the late
phases of training, the AT becomes more overfitting to the extent that the
individuals for weight averaging also suffer from overfitting and produce
anomalous weight values, which causes the self-ensemble model to continue to
undergo robust overfitting due to the failure in removing the weight anomalies.
To solve this problem, we aim to tackle the influence of outliers in the weight
space in this work and propose an easy-to-operate and effective Median-Ensemble
Adversarial Training (MEAT) method to solve the robust overfitting phenomenon
existing in self-ensemble defense from the source by searching for the median
of the historical model weights. Experimental results show that MEAT achieves
the best robustness against the powerful AutoAttack and can effectively
allievate the robust overfitting. We further demonstrate that most defense
methods can improve robust generalization and robustness by combining with
MEAT.
","['Zhaozhe Hu', 'Jia-Li Yin', 'Bin Chen', 'Luojun Lin', 'Bo-Hao Chen', 'Ximeng Liu']"
http://arxiv.org/abs/2504.00066v1,Cultured meat,2025-03-31T16:16:58Z,2025-03-31T16:16:58Z,"Meat, Vegetable, Soup -- The First Successful Attempt to Classify
  Everything","  We present the results of a novel classification scheme for all items,
objects, concepts, and crucially -- things -- in the known and unknown
universe. Our definitions of meat, soup and vegetable are near-exhaustive and
represent a new era of scientific discovery within the rapidly-developing field
of Arbitrary Classification. While the definitions of vegetable (growing in the
ground), meat (growing in an animal) and soup (containing both vegetable and
meat) may appear simple at first, we discuss a range of complex cases in which
progress is rapidly being made, and provide definitions and clarifications for
as many objects as a weekend of typing will allow.
","['G. Weaver', 'M. J. Selfridge', 'J. M. Setchfield', 'F. Dresbach', 'V. Varma', 'J. Martinez Garcia', 'A. Moharana', 'J. Keegans', 'L. J. Adams']"
http://arxiv.org/abs/2203.11684v1,Cultured meat,2022-03-22T12:58:39Z,2022-03-22T12:58:39Z,Meta-attention for ViT-backed Continual Learning,"  Continual learning is a longstanding research topic due to its crucial role
in tackling continually arriving tasks. Up to now, the study of continual
learning in computer vision is mainly restricted to convolutional neural
networks (CNNs). However, recently there is a tendency that the newly emerging
vision transformers (ViTs) are gradually dominating the field of computer
vision, which leaves CNN-based continual learning lagging behind as they can
suffer from severe performance degradation if straightforwardly applied to
ViTs. In this paper, we study ViT-backed continual learning to strive for
higher performance riding on recent advances of ViTs. Inspired by mask-based
continual learning methods in CNNs, where a mask is learned per task to adapt
the pre-trained ViT to the new task, we propose MEta-ATtention (MEAT), i.e.,
attention to self-attention, to adapt a pre-trained ViT to new tasks without
sacrificing performance on already learned tasks. Unlike prior mask-based
methods like Piggyback, where all parameters are associated with corresponding
masks, MEAT leverages the characteristics of ViTs and only masks a portion of
its parameters. It renders MEAT more efficient and effective with less overhead
and higher accuracy. Extensive experiments demonstrate that MEAT exhibits
significant superiority to its state-of-the-art CNN counterparts, with 4.0~6.0%
absolute boosts in accuracy. Our code has been released at
https://github.com/zju-vipa/MEAT-TIL.
","['Mengqi Xue', 'Haofei Zhang', 'Jie Song', 'Mingli Song']"
http://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,"  Large language models (LLMs) face challenges in aligning with diverse
cultural values despite their remarkable performance in generation, which stems
from inherent monocultural biases and difficulties in capturing nuanced
cultural semantics. Existing methods struggle to adapt to unkown culture after
fine-tuning. Inspired by cultural geography across five continents, we propose
Cultural Palette, a multi-agent framework that redefines cultural alignment as
an adaptive ""color-blending"" process for country-specific adaptation. Our
approach harnesses cultural geography across five continents (Africa, America,
Asia, Europe, Oceania) through three key steps: First, we synthesize the
Pentachromatic Cultural Palette Dataset using GPT-4o, refining
continental-level dialogues with Hofstede cultural dimensions to establish
foundational cultural representations. Second, five continent-level alignment
agents form specialized cultural communities that generate region-specific
draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically
blend these cultural ""colors"" through attention-gated parameter merging, akin
to mixing pigments on a palette, resolving conflicts while preserving cultural
nuances to produce the final culturally-aligned response. Extensive experiments
across various countries demonstrate that Cultural Palette surpasses existing
baselines in cultural alignment.
","['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']"
http://arxiv.org/abs/1304.3546v1,Cultured meat,2013-04-12T06:30:33Z,2013-04-12T06:30:33Z,The Meat of the Matter: A thumb rule for scavenging dogs?,"  Animals that scavenge in and around human localities need to utilize a broad
range of resources. Preference for any one kind of food, under such
circumstances, might be inefficient. Indian free-ranging dogs, Canis lupus
familiaris are scavengers that are heavily dependent on humans for sustaining
their omnivorous diet. The current study suggests that because of evolutionary
load, these dogs, which are descendants of the decidedly carnivorous gray wolf,
still retain a preference for meat though they live on carbohydrate-rich
resources. The plasticity in their diet probably fosters efficient scavenging
in a competitive environment, while a thumb rule for preferentially acquiring
specific nutrients enables them to sequester proteins from the
carbohydrate-rich environment.
","['Anandarup Bhadra', 'Debottam Bhattacharjee', 'Manabi Paul', 'Anindita Bhadra']"
http://arxiv.org/abs/2007.13115v1,Gene therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/0810.0239v1,Gene therapy,2008-10-01T17:56:31Z,2008-10-01T17:56:31Z,"Stochastic models and numerical algorithms for a class of regulatory
  gene networks","  Regulatory gene networks contain generic modules like those involving
feedback loops, which are essential for the regulation of many biological
functions. We consider a class of self-regulated genes which are the building
blocks of many regulatory gene networks, and study the steady state
distributions of the associated Gillespie algorithm by providing efficient
numerical algorithms. We also study a regulatory gene network of interest in
synthetic biology and in gene therapy, using mean-field models with time
delays. Convergence of the related time-nonhomogeneous Markov chain is
established for a class of linear catalytic networks with feedback loops
","['Thomas Fournier', 'Jean-Pierre Gabriel', 'Christian Mazza', 'Jerome Pasquier', 'Jose Galbete', 'Nicolas Mermod']"
http://arxiv.org/abs/1902.00728v1,Gene therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/q-bio/0511020v1,Gene therapy,2005-11-15T07:47:51Z,2005-11-15T07:47:51Z,"Induction in myeloid leukemic cells of genes that are expressed in
  different normal tissues","  Using DNA microarray and cluster analysis of expressed genes in a cloned line
(M1-t-p53) of myeloid leukemic cells, we have analyzed the expression of genes
that are preferentially expressed in different normal tissues. Clustering of
547 highly expressed genes in these leukemic cells showed 38 genes
preferentially expressed in normal hematopoietic tissues and 122 other genes
preferentially expressed in different normal non-hematopoietic tissues
including neuronal tissues, muscle, liver and testis. We have also analyzed the
genes whose expression in the leukemic cells changed after activation of
wild-type p53 and treatment with the cytokine interleukin 6 (IL-6) or the
calcium mobilizer thapsigargin (TG). Out of 620 such genes in the leukemic
cells that were differentially expressed in normal tissues, clustering showed
80 genes that were preferentially expressed in hematopoietic tissues and 132
genes in different normal non-hematopietic tissues that also included neuronal
tissues, muscle, liver and testis. Activation of p53 and treatment with IL-6 or
TG induced different changes in the genes preferentially expressed in these
normal tissues. These myeloid leukemic cells thus express genes that are
expressed in normal non-hematopoietic tissues, and various treatments can
reprogram these cells to induce other such non-hematopoietic genes. The results
indicate that these leukemic cells share with normal hematopoietic stem cells
the plasticity of differentiation to different cell types. It is suggested that
this reprogramming to induce in malignant cells genes that are expressed in
different normal tissues may be of clinical value in therapy.
","['Joseph Lotem', 'Hila Benjamin', 'Dvir Netaneli', 'Eytan Domany', 'Leo Sachs']"
http://arxiv.org/abs/2403.01927v1,Gene therapy,2024-03-04T10:44:57Z,2024-03-04T10:44:57Z,"Advancing Gene Selection in Oncology: A Fusion of Deep Learning and
  Sparsity for Precision Gene Selection","  Gene selection plays a pivotal role in oncology research for improving
outcome prediction accuracy and facilitating cost-effective genomic profiling
for cancer patients. This paper introduces two gene selection strategies for
deep learning-based survival prediction models. The first strategy uses a
sparsity-inducing method while the second one uses importance based gene
selection for identifying relevant genes. Our overall approach leverages the
power of deep learning to model complex biological data structures, while
sparsity-inducing methods ensure the selection process focuses on the most
informative genes, minimizing noise and redundancy. Through comprehensive
experimentation on diverse genomic and survival datasets, we demonstrate that
our strategy not only identifies gene signatures with high predictive power for
survival outcomes but can also streamlines the process for low-cost genomic
profiling. The implications of this research are profound as it offers a
scalable and effective tool for advancing personalized medicine and targeted
cancer therapies. By pushing the boundaries of gene selection methodologies,
our work contributes significantly to the ongoing efforts in cancer genomics,
promising improved diagnostic and prognostic capabilities in clinical settings.
","['Akhila Krishna', 'Ravi Kant Gupta', 'Pranav Jeevan', 'Amit Sethi']"
http://arxiv.org/abs/2409.19115v1,Gene therapy,2024-09-27T19:44:20Z,2024-09-27T19:44:20Z,Identifying Key Genes in Cancer Networks Using Persistent Homology,"  Identifying driver genes is crucial for understanding oncogenesis and
developing targeted cancer therapies. Driver discovery methods using protein or
pathway networks rely on traditional network science measures, focusing on
nodes, edges, or community metrics. These methods can overlook the
high-dimensional interactions that cancer genes have within cancer networks.
This study presents a novel method using Persistent Homology to analyze the
role of driver genes in higher-order structures within Cancer Consensus
Networks derived from main cellular pathways. We integrate mutation data from
six cancer types and three biological functions: DNA Repair, Chromatin
Organization, and Programmed Cell Death. We systematically evaluated the impact
of gene removal on topological voids ($\beta_2$ structures) within the Cancer
Consensus Networks. Our results reveal that only known driver genes and
cancer-associated genes influence these structures, while passenger genes do
not. Although centrality measures alone proved insufficient to fully
characterize impact genes, combining higher-order topological analysis with
traditional network metrics can improve the precision of distinguishing between
drivers and passengers. This work shows that cancer genes play an important
role in higher-order structures, going beyond pairwise measures, and provides
an approach to distinguish drivers and cancer-associated genes from passenger
genes.
","['Rodrigo Henrique Ramos', 'Yago Augusto Bardelotte', 'Cynthia de Oliveira Lage Ferreira', 'Adenilso Simao']"
http://arxiv.org/abs/1612.09478v1,Gene therapy,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,"  Cancer is known as a disease mainly caused by gene alterations. Discovery of
mutated driver pathways or gene sets is becoming an important step to
understand molecular mechanisms of carcinogenesis. However, systematically
investigating commonalities and specificities of driver gene sets among
multiple cancer types is still a great challenge, but this investigation will
undoubtedly benefit deciphering cancers and will be helpful for personalized
therapy and precision medicine in cancer treatment. In this study, we propose
two optimization models to \emph{de novo} discover common driver gene sets
among multiple cancer types (ComMDP) and specific driver gene sets of one
certain or multiple cancer types to other cancers (SpeMDP), respectively. We
first apply ComMDP and SpeMDP to simulated data to validate their efficiency.
Then, we further apply these methods to 12 cancer types from The Cancer Genome
Atlas (TCGA) and obtain several biologically meaningful driver pathways. As
examples, we construct a common cancer pathway model for BRCA and OV, infer a
complex driver pathway model for BRCA carcinogenesis based on common driver
gene sets of BRCA with eight cancer types, and investigate specific driver
pathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)
versus other solid cancer types. In these processes more candidate cancer genes
are also found.
","['Junhua Zhang', 'Shihua Zhang']"
http://arxiv.org/abs/2311.06747v3,Gene therapy,2023-11-12T06:03:13Z,2024-11-25T22:37:38Z,Graph Frequency Features of Cancer Gene Co-Expression Networks,"  Complex gene interactions play a significant role in cancer progression,
driving cellular behaviors that contribute to tumor growth, invasion, and
metastasis. Gene co-expression networks model the functional connectivity
between genes under various biological conditions. Understanding the
system-level evolution of these networks in cancer is critical for elucidating
disease mechanisms and informing the development of targeted therapies. While
previous studies have primarily focused on structural differences between
cancer and normal cell co-expression networks, this study applies graph
frequency analysis to cancer transcriptomic signals defined on gene
co-expression networks, highlighting the graph spectral characteristics of
cancer systems. Using a range of graph frequency filters, we showed that cancer
cells display distinctive patterns in the graph frequency content of their gene
transcriptomic signals, effectively distinguishing between cancer types and
stages. The transformation of the original gene feature space into the graph
spectral space captured more intricate cancer properties, as validated by
significantly higher F-statistic scores for graph frequency-filtered gene
features compared to those in the original space.
","['Radwa Adel', 'Ercan Engin Kuruoglu']"
http://arxiv.org/abs/2411.12010v2,Gene therapy,2024-11-18T19:49:51Z,2024-12-11T11:52:24Z,"Active learning for efficient discovery of optimal gene combinations in
  the combinatorial perturbation space","  The advancement of novel combinatorial CRISPR screening technologies enables
the identification of synergistic gene combinations on a large scale. This is
crucial for developing novel and effective combination therapies, but the
combinatorial space makes exhaustive experimentation infeasible. We introduce
NAIAD, an active learning framework that efficiently discovers optimal gene
pairs capable of driving cells toward desired cellular phenotypes. NAIAD
leverages single-gene perturbation effects and adaptive gene embeddings that
scale with the training data size, mitigating overfitting in small-sample
learning while capturing complex gene interactions as more data is collected.
Evaluated on four CRISPR combinatorial perturbation datasets totaling over
350,000 genetic interactions, NAIAD, trained on small datasets, outperforms
existing models by up to 40\% relative to the second-best. NAIAD's
recommendation system prioritizes gene pairs with the maximum predicted
effects, resulting in the highest marginal gain in each AI-experiment round and
accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD
framework (https://github.com/NeptuneBio/NAIAD) improves the identification of
novel, effective gene combinations, enabling more efficient CRISPR library
design and offering promising applications in genomics research and therapeutic
development.
","['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']"
http://arxiv.org/abs/1111.1360v1,Gene therapy,2011-11-05T23:17:02Z,2011-11-05T23:17:02Z,"Magnetic Field-Assisted Gene Delivery: Achievements and Therapeutic
  Potential","  The discovery in the early 2000's that magnetic nanoparticles (MNPs)
complexed to nonviral or viral vectors can, in the presence of an external
magnetic field, greatly enhance gene transfer into cells has raised much
interest. This technique, called magnetofection, was initially developed mainly
to improve gene transfer in cell cultures, a simpler and more easily
controllable scenario than in vivo models. These studies provided evidence for
some unique capabilities of magnetofection. Progressively, the interest in
magnetofection expanded to its application in animal models and led to the
association of this technique with another technology, magnetic drug targeting
(MDT). This combination offers the possibility to develop more efficient and
less invasive gene therapy strategies for a number of major pathologies like
cancer, neurodegeneration and myocardial infarction. The goal of MDT is to
concentrate MNPs functionalized with therapeutic drugs, in target areas of the
body by means of properly focused external magnetic fields. The availability of
stable, nontoxic MNP-gene vector complexes now offers the opportunity to
develop magnetic gene targeting (MGT), a variant of MDT in which the gene
coding for a therapeutic molecule, rather than the molecule itself, is
delivered to a therapeutic target area in the body. This article will first
outline the principle of magnetofection, subsequently describing the properties
of the magnetic fields and MNPs used in this technique. Next, it will review
the results achieved by magnetofection in cell cultures. Last, the potential of
MGT for implementing minimally invasive gene therapy will be discussed.
","['José I. Schwerdt', 'Gerardo F. Goya', 'Pilar Calatayud', 'Claudia B. Hereñú', 'Paula C. Reggiani', 'Rodolfo G. Goya']"
http://arxiv.org/abs/2502.01689v1,Gene therapy,2025-02-02T15:43:20Z,2025-02-02T15:43:20Z,"scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological
  Profiling","  The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.
","['Yu-An Huang', 'Xiyue Cao', 'Zhu-Hong You', 'Yue-Chao Li', 'Xuequn Shang', 'Zhi-An Huang']"
http://arxiv.org/abs/1703.01900v1,Gene therapy,2017-03-01T02:09:50Z,2017-03-01T02:09:50Z,"Network-based Distance Metric with Application to Discover Disease
  Subtypes in Cancer","  While we once thought of cancer as single monolithic diseases affecting a
specific organ site, we now understand that there are many subtypes of cancer
defined by unique patterns of gene mutations. These gene mutational data, which
can be more reliably obtained than gene expression data, help to determine how
the subtypes develop, evolve, and respond to therapies. Different from dense
continuous-value gene expression data, which most existing cancer subtype
discovery algorithms use, somatic mutational data are extremely sparse and
heterogeneous, because there are less than 0.5\% mutated genes in discrete
value 1/0 out of 20,000 human protein-coding genes, and identical mutated genes
are rarely shared by cancer patients.
  Our focus is to search for cancer subtypes from extremely sparse and high
dimensional gene mutational data in discrete 1 and 0 values using unsupervised
learning. We propose a new network-based distance metric. We project cancer
patients' mutational profile into their gene network structure and measure the
distance between two patients using the similarity between genes and between
the gene vertexes of the patients in the network. Experimental results in
synthetic data and real-world data show that our approach outperforms the top
competitors in cancer subtype discovery. Furthermore, our approach can identify
cancer subtypes that cannot be detected by other clustering algorithms in real
cancer data.
","['Jipeng Qiang', 'Wei Ding', 'John Quackenbush', 'Ping Chen']"
http://arxiv.org/abs/1310.3528v1,Gene therapy,2013-10-13T23:30:57Z,2013-10-13T23:30:57Z,Evolution and Controllability of Cancer Networks: a Boolean Perspective,"  Cancer forms a robust system and progresses as stages over time typically
with increasing aggressiveness and worsening prognosis. Characterizing these
stages and identifying the genes driving transitions between them is critical
to understand cancer progression and to develop effective anti-cancer
therapies. Here, we propose a novel model of the 'cancer system' as a Boolean
state space in which a Boolean network, built from protein interaction and
gene-expression data from different stages of cancer, transits between Boolean
satisfiability states by ""editing"" interactions and ""flipping"" genes. The
application of our model (called BoolSpace) on three case studies - pancreatic
and breast tumours in human and post spinal-cord injury in rats - reveals
valuable insights into the phenomenon of cancer progression. In particular, we
notice that several of the genes flipped are serine/threonine kinases which act
as natural cellular switches and that different sets of genes are flipped
during the initial and final stages indicating a pattern to tumour progression.
We hypothesize that robustness of cancer partly stems from ""passing of the
baton"" between genes at different stages, and therefore an effective therapy
should target a ""cover set"" of these genes. A C/C++ implementation of BoolSpace
is freely available at: http://www.bioinformatics.org.au/tools-data
","['Sriganesh Srihari', 'Venkatesh Raman', 'Hon Wai Leong', 'Mark A. Ragan']"
http://arxiv.org/abs/2501.18794v1,Gene therapy,2025-01-30T23:03:03Z,2025-01-30T23:03:03Z,"Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models","  Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.
","['Matthew Neeley', 'Guantong Qi', 'Guanchu Wang', 'Ruixiang Tang', 'Dongxue Mao', 'Chaozhong Liu', 'Sasidhar Pasupuleti', 'Bo Yuan', 'Fan Xia', 'Pengfei Liu', 'Zhandong Liu', 'Xia Hu']"
http://arxiv.org/abs/1408.0083v1,Gene therapy,2014-08-01T05:36:59Z,2014-08-01T05:36:59Z,"Gene-level pharmacogenetic analysis on survival outcomes using
  gene-trait similarity regression","  Gene/pathway-based methods are drawing significant attention due to their
usefulness in detecting rare and common variants that affect disease
susceptibility. The biological mechanism of drug responses indicates that a
gene-based analysis has even greater potential in pharmacogenetics. Motivated
by a study from the Vitamin Intervention for Stroke Prevention (VISP) trial, we
develop a gene-trait similarity regression for survival analysis to assess the
effect of a gene or pathway on time-to-event outcomes. The similarity
regression has a general framework that covers a range of survival models, such
as the proportional hazards model and the proportional odds model. The
inference procedure developed under the proportional hazards model is robust
against model misspecification. We derive the equivalence between the
similarity survival regression and a random effects model, which further
unifies the current variance component-based methods. We demonstrate the
effectiveness of the proposed method through simulation studies. In addition,
we apply the method to the VISP trial data to identify the genes that exhibit
an association with the risk of a recurrent stroke. The TCN2 gene was found to
be associated with the recurrent stroke risk in the low-dose arm. This gene may
impact recurrent stroke risk in response to cofactor therapy.
","['Jung-Ying Tzeng', 'Wenbin Lu', 'Fang-Chi Hsu']"
http://arxiv.org/abs/1202.3015v2,Gene therapy,2012-02-14T12:24:09Z,2012-08-18T19:50:11Z,On dynamic network entropy in cancer,"  The cellular phenotype is described by a complex network of molecular
interactions. Elucidating network properties that distinguish disease from the
healthy cellular state is therefore of critical importance for gaining
systems-level insights into disease mechanisms and ultimately for developing
improved therapies. By integrating gene expression data with a protein
interaction network to induce a stochastic dynamics on the network, we here
demonstrate that cancer cells are characterised by an increase in the dynamic
network entropy, compared to cells of normal physiology. Using a fundamental
relation between the macroscopic resilience of a dynamical system and the
uncertainty (entropy) in the underlying microscopic processes, we argue that
cancer cells will be more robust to random gene perturbations. In addition, we
formally demonstrate that gene expression differences between normal and cancer
tissue are anticorrelated with local dynamic entropy changes, thus providing a
systemic link between gene expression changes at the nodes and their local
network dynamics. In particular, we also find that genes which drive
cell-proliferation in cancer cells and which often encode oncogenes are
associated with reductions in the dynamic network entropy. In summary, our
results support the view that the observed increased robustness of cancer cells
to perturbation and therapy may be due to an increase in the dynamic network
entropy that allows cells to adapt to the new cellular stresses. Conversely,
genes that exhibit local flux entropy decreases in cancer may render cancer
cells more susceptible to targeted intervention and may therefore represent
promising drug targets.
","['James West', 'Ginestra Bianconi', 'Simone Severini', 'Andrew Teschendorff']"
http://arxiv.org/abs/2007.03186v1,Gene therapy,2020-07-07T03:58:44Z,2020-07-07T03:58:44Z,"Advancing Drug Resistance Research Through Quantitative Modeling and
  Synthetic Biology","  Antimicrobial resistance is an emerging global health crisis that is
undermining advances in modern medicine and, if unmitigated, threatens to kill
10 million people per year worldwide by 2050. Research over the last decade has
demonstrated that the differences between genetically identical cells in the
same environment can lead to drug resistance. Fluctuations in gene expression,
modulated by gene regulatory networks, can lead to non-genetic heterogeneity
that results in the fractional killing of microbial populations causing drug
therapies to fail; this non-genetic drug resistance can enhance the probability
of acquiring genetic drug resistance mutations. Mathematical models of gene
networks can elucidate general principles underlying drug resistance, predict
the evolution of resistance, and guide drug resistance experiments in the
laboratory. Cells genetically engineered to carry synthetic gene networks
regulating drug resistance genes allow for controlled, quantitative experiments
on the role of non-genetic heterogeneity in the development of drug resistance.
In this perspective article, we emphasize the contributions that mathematical,
computational, and synthetic gene network models play in advancing our
understanding of antimicrobial resistance to discover effective therapies
against drug-resistant infections.
","['K. Farquhar', 'H. Flohr', 'D. A. Charlebois']"
http://arxiv.org/abs/1602.08111v1,Gene therapy,2015-12-15T05:15:51Z,2015-12-15T05:15:51Z,A Cancer Biotherapy Resource,"  Cancer Biotherapy (CB), as opposed to cancer chemotherapy, is the use of
macromolecular, biological agents instead of organic chemicals or drugs to
treat cancer. Biological agents usually have higher selectivity and have less
toxic side effects than chemical agents. The I.S.B.T.C., being the only major
information database for CB, seems lacking in some crucial information on
various cancer biotherapy regimens. It is thus necessary to have a
comprehensive curated CB database. The database accessible to cancer patients
and also should be a sounding board for scientific ideas by cancer researchers.
The database/web server has information about main families of cancer
biotherapy regimens to date, namely, Protein Kinase Inhibitors, Ras Pathway
Inhibitors, Cell-Cycle Active Agents, MAbs (monoclonal antibodies), ADEPT
(Antibody-Directed Enzyme Pro-Drug Therapy), Cytokines, Anti-Angiogenesis
Agents, Cancer Vaccines, Cell-based Immunotherapeutics, Gene Therapy,
Hematopoietic Growth Factors, Retinoids, and CAAT. For each biotherapy regimen,
we will extract the following attributes in populating the database: Cancer
type, Gene/s and gene product/s involved, Gene sequence, Organs affected,
Reference papers, Clinical phase/stage, Survival rate, Clinical test center
locations, Cost, Patient blogs, Researcher blogs, and Future work. The database
will be accessible to public through a website and had FAQs for making it
understandable to the laymen and discussion page for researchers to express
their views and ideas. In addition to information about the biotherapy
regimens, the website will link to other biologically significant databases
like structural proteomics, metabolomics, glycomics, and lipidomics databases,
as well as to news around the world regarding cancer therapy results. The
database attributes would be regularly updated for novel attributes as
discoveries are made.
","['Preety Priya', 'Vicente M. Reyes']"
http://arxiv.org/abs/1510.00815v1,Gene therapy,2015-10-03T13:09:36Z,2015-10-03T13:09:36Z,"Inferring synthetic lethal interactions from mutual exclusivity of
  genetic events in cancer","  Background: Synthetic lethality (SL) refers to the genetic interaction
between two or more genes where only their co-alteration (e.g. by mutations,
amplifications or deletions) results in cell death. In recent years, SL has
emerged as an attractive therapeutic strategy against cancer: by targeting the
SL partners of altered genes in cancer cells, these cells can be selectively
killed while sparing the normal cells. Consequently, a number of studies have
attempted prediction of SL interactions in human, a majority by extrapolating
SL interactions inferred through large-scale screens in model organisms.
However, these predicted SL interactions either do not hold in human cells or
do not include genes that are (frequently) altered in human cancers, and are
therefore not attractive in the context of cancer therapy.
  Results: Here, we develop a computational approach to infer SL interactions
directly from frequently altered genes in human cancers. It is based on the
observation that pairs of genes that are altered in a (significantly) mutually
exclusive manner in cancers are likely to constitute lethal combinations. Using
genomic copy-number and gene-expression data from four cancers, breast,
prostate, ovarian and uterine (total 3980 samples) from The Cancer Genome
Atlas, we identify 718 genes that are frequently amplified or upregulated, and
are likely to be synthetic lethal with six key DNA-damage response (DDR) genes
in these cancers. By comparing with published data on gene essentiality (~16000
genes) from ten DDR-deficient cancer cell lines, we show that our identified
genes are enriched among the top quartile of essential genes in these cell
lines, implying that our inferred genes are highly likely to be (synthetic)
lethal upon knockdown in these cell lines.
","['Sriganesh Srihari', 'Jitin Singla', 'Limsoon Wong', 'Mark A. Ragan']"
http://arxiv.org/abs/0803.0962v1,Gene therapy,2008-03-06T20:12:06Z,2008-03-06T20:12:06Z,Predicting synthetic rescues in metabolic networks,"  An important goal of medical research is to develop methods to recover the
loss of cellular function due to mutations and other defects. Many approaches
based on gene therapy aim to repair the defective gene or to insert genes with
compensatory function. Here, we propose an alternative, network-based strategy
that aims to restore biological function by forcing the cell to either bypass
the functions affected by the defective gene, or to compensate for the lost
function. Focusing on the metabolism of single-cell organisms, we
computationally study mutants that lack an essential enzyme, and thus are
unable to grow or have a significantly reduced growth rate. We show that
several of these mutants can be turned into viable organisms through additional
gene deletions that restore their growth rate. In a rather counterintuitive
fashion, this is achieved via additional damage to the metabolic network. Using
flux balance-based approaches, we identify a number of synthetically viable
gene pairs, in which the removal of one enzyme-encoding gene results in a
nonviable phenotype, while the deletion of a second enzyme-encoding gene
rescues the organism. The systematic network-based identification of
compensatory rescue effects may open new avenues for genetic interventions.
","['Adilson E. Motter', 'Natali Gulbahce', 'Eivind Almaas', 'Albert-Laszlo Barabasi']"
http://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,"Ethical Analysis on the Application of Neurotechnology for Human
  Augmentation in Physicians and Surgeons","  With the shortage of physicians and surgeons and increase in demand worldwide
due to situations such as the COVID-19 pandemic, there is a growing interest in
finding solutions to help address the problem. A solution to this problem would
be to use neurotechnology to provide them augmented cognition, senses and
action for optimal diagnosis and treatment. Consequently, doing so can
negatively impact them and others. We argue that applying neurotechnology for
human enhancement in physicians and surgeons can cause injustices, and harm to
them and patients. In this paper, we will first describe the augmentations and
neurotechnologies that can be used to achieve the relevant augmentations for
physicians and surgeons. We will then review selected ethical concerns
discussed within literature, discuss the neuroengineering behind using
neurotechnology for augmentation purposes, then conclude with an analysis on
outcomes and ethical issues of implementing human augmentation via
neurotechnology in medical and surgical practice.
","['Soaad Hossain', 'Syed Ishtiaque Ahmed']"
http://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
","['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']"
http://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,"Foundational guidelines for enhancing neurotechnology research and
  development through end-user involvement","  Neurotechnologies are increasingly becoming integrated with our everyday
lives, our bodies and our mental states. As the popularity and impact of
neurotechnology grows, so does our responsibility to ensure we understand its
particular implications on its end users, as well as broader ethical and
societal implications. Enabling end-users and stakeholders to participate in
the development of neurotechnology, from its earliest stages of conception,
will help us better navigate our design around these considerations and deliver
more impactful technologies. There are many terms and frameworks to articulate
the concept of involving end users in the technology development lifecycle, for
example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived
experience' and 'co-design'. Here we utilise the PPIE framework to develop
clear guidelines for implementing a robust involvement process of current and
future end-users in neurotechnology. We present best practice guidance for
researchers and engineers who are interested in developing and conducting a PPI
strategy for their neurotechnology. We provide advice from various online
sources to orient individual teams (and funders) to carve up their own approach
to meaningful involvement. After an introduction that coveys the tangible and
conceptual benefits of user involvement, we guide the reader to develop a
general strategy towards setting up their own process. We then help the reader
map out their relevant stakeholders and provide advice on how to consider user
diversity and representation. We also provide advice on how to quantify the
outcomes of the engagement, as well as a check-list to ensure transparency and
accountability at various stages. The aim is the establishment of gold-standard
methodologies for ensuring that patient and public insights are at the
forefront of our scientific inquiry and product development.
","['Amparo Güemes', 'Tiago da Silva Costa', 'Tamar Makin']"
http://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,"A Separation Principle for Discrete-Time Fractional-Order Dynamical
  Systems and its Implications to Closed-loop Neurotechnology","  Closed-loop neurotechnology requires the capability to predict the state
evolution and its regulation under (possibly) partial measurements. There is
evidence that neurophysiological dynamics can be modeled by fractional-order
dynamical systems. Therefore, we propose to establish a separation principle
for discrete-time fractional-order dynamical systems, which are inherently
nonlinear and are able to capture spatiotemporal relations that exhibit
non-Markovian properties. The separation principle states that the problems of
controller and state estimator design can be done independently of each other
while ensuring proper estimation and control in closed-loop setups. Lastly, we
illustrate, as proof-of-concept, the application of the separation principle
when designing controllers and estimators for these classes of systems in the
context of neurophysiological data. In particular, we rely on real data to
derive the models used to assess and regulate the evolution of closed-loop
neurotechnologies based on electroencephalographic data.
","['Sarthak Chatterjee', 'Orlando Romero', 'Sérgio Pequito']"
http://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,"  Interdisciplinary research among engineering, computer science, and
neuroscience to understand and utilize the human brain signals resulted in
advances and widespread applicability of wearable neurotechnology in adaptive
human-in-the-loop smart systems. Considering these advances, we envision that
future education will exploit the advances in wearable neurotechnology and move
toward more personalized smart classrooms where instructions and interactions
are tailored towards. students' individual strengths and needs. In this paper,
we discuss the future of smart classrooms and how advances in neuroscience,
machine learning, and embedded systems as key enablers will provide the
infrastructure for envisioned smart classrooms and personalized education along
with open challenges that are required to be addressed.
","['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']"
http://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,"A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology","  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
","['Bryce Allen Bagley', 'Claudia K Petritsch']"
http://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,"  Artificial intelligence (AI) is a fast-growing field focused on modeling and
machine implementation of various cognitive functions with an increasing number
of applications in computer vision, text processing, robotics, neurotechnology,
bio-inspired computing and others. In this chapter, we describe how AI methods
can be applied in the context of intracranial electroencephalography (iEEG)
research. IEEG data is unique as it provides extremely high-quality signals
recorded directly from brain tissue. Applying advanced AI models to these data
carries the potential to further our understanding of many fundamental
questions in neuroscience. At the same time, as an invasive technique, iEEG
lends itself well to long-term, mobile brain-computer interface applications,
particularly for communication in severely paralyzed individuals. We provide a
detailed overview of these two research directions in the application of AI
techniques to iEEG. That is, (1) the development of computational models that
target fundamental questions about the neurobiological nature of cognition
(AI-iEEG for neuroscience) and (2) applied research on monitoring and
identification of event-driven brain states for the development of clinical
brain-computer interface systems (AI-iEEG for neurotechnology). We explain key
machine learning concepts, specifics of processing and modeling iEEG data and
details of state-of-the-art iEEG-based neurotechnology and brain-computer
interfaces.
","['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']"
http://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about
  Neurotechnologies","  Teegi is an anthropomorphic and tangible avatar exposing a users' brain
activity in real time. It is connected to a device sensing the brain by means
of electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its
eyes along with the person being monitored. It also displays on its scalp the
associated EEG signals, thanks to a semi-spherical display made of LEDs.
Attendees can interact directly with Teegi -- e.g. move its limbs -- to
discover by themselves the underlying brain processes. Teegi can be used for
scientific outreach to introduce neurotechnologies in general and
brain-computer interfaces (BCI) in particular.
","['Jérémy Frey', 'Renaud Gervais', 'Thibault Lainé', 'Maxime Duluc', 'Hugo Germain', 'Stéphanie Fleck', 'Fabien Lotte', 'Martin Hachet']"
http://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in
  Neurotechnology","  Integrating smart algorithms on neural devices presents significant
opportunities for various brain disorders. In this paper, we review the latest
advancements in the development of three categories of intelligent neural
prostheses featuring embedded signal processing on the implantable or wearable
device. These include: 1) Neural interfaces for closed-loop symptom tracking
and responsive stimulation; 2) Neural interfaces for emerging network-related
conditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for
movement recovery following paralysis.
","['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']"
http://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,"Mining within-trial oscillatory brain dynamics to address the
  variability of optimized spatial filters","  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
  The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
","['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']"
http://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,"  Modeling dynamical systems is important in many disciplines, e.g., control,
robotics, or neurotechnology. Commonly the state of these systems is not
directly observed, but only available through noisy and potentially
high-dimensional observations. In these cases, system identification, i.e.,
finding the measurement mapping and the transition mapping (system dynamics) in
latent space can be challenging. For linear system dynamics and measurement
mappings efficient solutions for system identification are available. However,
in practical applications, the linearity assumptions does not hold, requiring
non-linear system identification techniques. If additionally the observations
are high-dimensional (e.g., images), non-linear system identification is
inherently hard. To address the problem of non-linear system identification
from high-dimensional observations, we combine recent advances in deep learning
and system identification. In particular, we jointly learn a low-dimensional
embedding of the observation by means of deep auto-encoders and a predictive
transition model in this low-dimensional space. We demonstrate that our model
enables learning good predictive models of dynamical systems from pixel
information only.
","['Niklas Wahlström', 'Thomas B. Schön', 'Marc Peter Deisenroth']"
http://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,"Algebraic identification of the effective connectivity of constrained
  geometric network models of neural signaling","  Cellular neural circuit and networks consisting of interconnected neurons and
glia are ulti- mately responsible for the information processing associated
with information processing in the brain. While there are major efforts aimed
at mapping the structural and (electro)physiological connectivity of brain
networks, such as the White House BRAIN Initiative aimed at the devel- opment
of neurotechnologies capable of high density neural recordings, theoretical and
compu- tational methods for analyzing and making sense of all this data seem to
be further behind. Here, we propose and provide a summary of an approach for
calculating effective connectivity from experimental observations of neuronal
network activity. The proposed method operates on network-level data, makes use
of all relevant prior knowledge, such as dynamical models of individual cells
in the network and the physical structural connectivity of the network, and is
broadly applicable to large classes of biological and non-biological networks.
","['Marius Buibas', 'Gabriel A. Silva']"
http://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field
integrating knowledge and methods from neurotechnology, artificial
intelligence, and quantum computing. The objective is to develop an enhanced
connectivity between the human brain and quantum computers for a variety of
disruptive applications. We foresee the emergence of hybrid classical-quantum
networks of wetware and hardware nodes, mediated by machine learning techniques
and brain-machine interfaces. QBraiNs will harness and transform in
unprecedented ways arts, science, technologies, and entrepreneurship, in
particular activities related to medicine, Internet of humans, intelligent
devices, sensorial experience, gaming, Internet of things, crypto trading, and
business.
","['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martín-Guerrero', 'E. Solano']"
http://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,"Using EEG-based brain connectivity for the study of brain dynamics in
  brain-computer interfaces","  The analysis of brain connectivity aims to understand the emergence of
functional networks into the brain. This information can be used in the process
of electroencephalographic (EEG) signal analysis and classification for a
braincomputer interface (BCI). These systems provide an alternative channel of
communication and control to people with motor impairments. In this article,
four strategies for using the brain connectivity in a BCI environment as a tool
to obtain a deeper understanding of the cerebral mechanisms are proposed, with
the principal aim of developing a scheme oriented to neuro-rehabilitation of
gait in combination with different neurotechnologies and exoskeletons. This
scheme would allow improving current schemes and/or to design new control
strategies, as well as rehabilitation approaches.
",['J. A. Gaxiola-Tirado']
http://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in
  Generative Models","  Generative adversarial networks (GANs) are able to generate high resolution
photo-realistic images of objects that ""do not exist."" These synthetic images
are rather difficult to detect as fake. However, the manner in which these
generative models are trained hints at a potential for information leakage from
the supplied training data, especially in the context of synthetic faces. This
paper presents experiments suggesting that identity information in face images
can flow from the training corpus into synthetic samples without any
adversarial actions when building or using the existing model. This raises
privacy-related questions, but also stimulates discussions of (a) the face
manifold's characteristics in the feature space and (b) how to create
generative models that do not inadvertently reveal identity information of real
subjects whose images were used for training. We used five different face
matchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology
MegaMatcher) and the StyleGAN2 synthesis model, and show that this identity
leakage does exist for some, but not all methods. So, can we say that these
synthetically generated faces truly do not exist? Databases of real and
synthetically generated faces are made available with this paper to allow full
replicability of the results discussed in this work.
","['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']"
http://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,"Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs","  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
","['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']"
http://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic
  contacts in Humans","  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey
electrophysiological activity within the brain to treat disorders such as
Epilepsy. In this stereotactic approach, leads are implanted through straight
trajectories to survey both cortical and sub-cortical activity. Visualizing the
recorded locations covering sulcal and gyral activity while staying true to the
cortical architecture is challenging due to the folded, three-dimensional
nature of the human cortex. To overcome this challenge, we developed a novel
visualization concept, allowing investigators to dynamically morph between the
subjects' cortical reconstruction and an inflated cortex representation. This
inflated view, in which gyri and sulci are viewed on a smooth surface, allows
better visualization of electrodes buried within the sulcus while staying true
to the underlying cortical architecture.
","['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']"
http://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,"Accelerated Algorithms for Source Orientation Detection (AORI) and
  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization","  This paper illustrates the development of two efficient source localization
algorithms for electroencephalography (EEG) data, aimed at enhancing real-time
brain signal reconstruction while addressing the computational challenges of
traditional methods. Accurate EEG source localization is crucial for
applications in cognitive neuroscience, neurorehabilitation, and brain-computer
interfaces (BCIs). To make significant progress toward precise source
orientation detection and improved signal reconstruction, we introduce the
Accelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and
the Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV
algorithm speeds up EEG source reconstruction by utilizing recursive covariance
matrix calculations, while AORI simplifies source orientation detection from
three dimensions to one, reducing computational load by 66% compared to
conventional methods. Using both simulated and real EEG data, we demonstrate
that these algorithms maintain high accuracy, with orientation errors below
0.2% and signal reconstruction accuracy within 2%. These findings suggest that
the proposed toolboxes represent a substantial advancement in the efficiency
and speed of EEG source localization, making them well-suited for real-time
neurotechnological applications.
","['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']"
http://arxiv.org/abs/2505.20509v1,Neurotechnology,2025-05-26T20:20:46Z,2025-05-26T20:20:46Z,"OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared
  Spectroscopy-based Brain Interfacing Cap","  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time
method for monitoring brain activity by measuring hemodynamic responses in the
cerebral cortex. However, existing systems are expensive, bulky, and limited to
clinical or research environments. This paper introduces OpenNIRScap, an
open-source, low-cost, and wearable fNIRS system designed to make real-time
brain monitoring more accessible in everyday environments. The device features
24 custom-designed sensor boards with dual-wavelength light emitters and
photodiode detectors, a central electrical control unit (ECU) with analog
multiplexing, and a real-time data processing pipeline. Bench validation and
pilot tests on volunteers have confirmed the ability of the system to capture
cognitively evoked hemodynamic responses, supporting its potential as an
affordable tool for cognitive monitoring and portable neurotechnology
applications. The hardware, software, and graphical user interface have all
been open-sourced and made publicly available at the following link:
https://github.com/tonykim07/fNIRS.
","['Tony Kim', 'Haotian Liu', 'Chiung-Ting Huang', 'Ingrid Wu', 'Xilin Liu']"
http://arxiv.org/abs/2505.24790v1,Neurotechnology,2025-05-30T16:52:44Z,2025-05-30T16:52:44Z,"Towards model-based design of causal manipulations of brain circuits
  with high spatiotemporal precision","  Recent advancements in neurotechnology enable precise spatiotemporal patterns
of microstimulations with single-cell resolution. The choice of perturbation
sites must satisfy two key criteria: efficacy in evoking significant responses
and selectivity for the desired target effects. This choice is currently based
on laborious trial-and-error procedures, unfeasible for sequences of multi-site
stimulations. Efficient methods to design complex perturbation patterns are
urgently needed. Can we design a spatiotemporal pattern of stimulation to steer
neural activity and behavior towards a desired target? We outline a method for
achieving this goal in two steps. First, we identify the most effective
perturbation sites, or hubs, only based on short observations of spontaneous
neural activity. Second, we provide an efficient method to design multi-site
stimulation patterns by combining approaches from nonlinear dynamical systems,
control theory and data-driven methods. We demonstrate the feasibility of our
approach using multi-site stimulation patterns in recurrent network models.
","['Anandita De', 'Roozbeh Kiani', 'Luca Mazzucato']"
http://arxiv.org/abs/2504.15291v1,Reusable launch vehicle,2025-04-08T22:15:13Z,2025-04-08T22:15:13Z,"Greenhouse Gas (GHG) Emissions Poised to Rocket: Modeling the
  Environmental Impact of LEO Satellite Constellations","  The proliferation of satellite megaconstellations in low Earth orbit (LEO)
represents a significant advancement in global broadband connectivity. However,
we urgently need to understand the potential environmental impacts,
particularly greenhouse gas (GHG) emissions associated with these
constellations. This study addresses a critical gap in modeling current and
future GHG emissions by developing a comprehensive open-source life cycle
assessment (LCA) methodology, applied to 10 launch vehicles and 15
megaconstellations. Our analysis reveals that the production of launch vehicles
and propellant combustion during launch events contribute most significantly to
overall GHG emissions, accounting for 72.6% of life cycle emissions. Among the
rockets analyzed, reusable vehicles like Falcon-9 and Starship demonstrate
95.4% lower production emissions compared to non-reusable alternatives,
highlighting the environmental benefits of reusability in space technology. The
findings underscore the importance of launch vehicle and satellite design
choices to minimize potential environmental impacts. The Open-source Rocket and
Constellation Lifecycle Emissions (ORACLE) repository is freely available and
aims to facilitate further research in this field. This study provides a
critical baseline for policymakers and industry stakeholders to develop
strategies for reducing the carbon footprint of the space industry, especially
satellite megaconstellations.
","['Rushil Kukreja', 'Edward J. Oughton', 'Richard Linares']"
http://arxiv.org/abs/2107.13513v2,Reusable launch vehicle,2021-04-19T00:15:27Z,2021-12-21T07:53:19Z,Feasibility Study For Multiply Reusable Space Launch System,"  A novel concept of orbital launch system in which all stages are reusable is
presented. The first two stages called Midpoint Delivery System (MPDS) deliver
the next stages to a midpoint. A midpoint is defined by an altitude of 100 $km$
to 120 $km$ and horizontal velocity of 2.8 $km/s$ to 3.2 $km/s$. MPDS stages
decelerate in the atmosphere and perform vertical landing on barges. These
stages can be reused daily for many years. The payload is delivered from the
midpoint to a 400 $km$ Low Earth Orbit by one or two stage rocket called
Midpoint to Orbit Delivery System (MPTO). All of MPTO engines are delivered to
LEO. These engines do not return to Earth themselves. They are returned to
Earth in packs of 50 to 100 by a Reentry Vehicle. Overall, the fully and
multiply reusable launch system should deliver payload to LEO for \$300 to
\$400 per $kg$
",['Mikhail Shubov']
http://arxiv.org/abs/2009.01664v1,Reusable launch vehicle,2020-09-03T13:48:54Z,2020-09-03T13:48:54Z,"Multidisciplinary Design Optimization of Reusable Launch Vehicles for
  Different Propellants and Objectives","  Identifying the optimal design of a new launch vehicle is most important
since design decisions made in the early development phase limit the vehicles'
later performance and determines the associated costs. Reusing the first stage
via retro-propulsive landing increases the complexity even more. Therefore, we
develop an optimization framework for partially reusable launch vehicles, which
enables multidisciplinary design studies. The framework contains suitable mass
estimates of all essential subsystems and a routine to calculate the needed
propellant for the ascent and landing maneuvers. For design optimization, the
framework can be coupled with a genetic algorithm. The overall goal is to
reveal the implications of different propellant combinations and objective
functions on the launcher's optimal design for various mission scenarios. The
results show that the optimization objective influences the most suitable
propellant choice and the overall launcher design, concerning staging, weight,
size, and rocket engine parameters. In terms of gross lift-off weight, liquid
hydrogen seems to be favorable. When optimizing for a minimum structural mass
or an expandable structural mass, hydrocarbon-based solutions show better
results. Finally, launch vehicles using a hydrocarbon fuel in the first stage
and liquid hydrogen in the upper stage are an appealing alternative, combining
both fuels' benefits.
","['Kai Dresia', 'Simon Jentzsch', 'Günther Waxenegger-Wilfing', 'Robson Hahn', 'Jan Deeken', 'Michael Oschwald', 'Fabio Mota']"
http://arxiv.org/abs/2405.01264v1,Reusable launch vehicle,2024-05-02T13:13:35Z,2024-05-02T13:13:35Z,"Model Predictive Guidance for Fuel-Optimal Landing of Reusable Launch
  Vehicles","  This paper introduces a landing guidance strategy for reusable launch
vehicles (RLVs) using a model predictive approach based on sequential convex
programming (SCP). The proposed approach devises two distinct optimal control
problems (OCPs): planning a fuel-optimal landing trajectory that accommodates
practical path constraints specific to RLVs, and determining real-time optimal
tracking commands. This dual optimization strategy allows for reduced
computational load through adjustable prediction horizon lengths in the
tracking task, achieving near closed-loop performance. Enhancements in model
fidelity for the tracking task are achieved through an alternative rotational
dynamics representation, enabling a more stable numerical solution of the OCP
and accounting for vehicle transient dynamics. Furthermore, modifications of
aerodynamic force in both planning and tracking phases are proposed, tailored
for thrust-vector-controlled RLVs, to reduce the fidelity gap without adding
computational complexity. Extensive 6-DOF simulation experiments validate the
effectiveness and improved guidance performance of the proposed algorithm.
","['Ki-Wook Jung', 'Sang-Don Lee', 'Cheol-Goo Jung', 'Chang-Hun Lee']"
http://arxiv.org/abs/2406.04185v1,Reusable launch vehicle,2024-06-06T15:41:12Z,2024-06-06T15:41:12Z,Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle,"  The trajectory optimization of the atmospheric entry of a reusable launch
vehicle is studied. The objective is to maximize the crossrange of the vehicle
subject to two control-inequality path constraints, two state-inequality path
constraints, and one mixed state-and-control inequality path constraint. In
order to determine the complex switching structure in the activity of the path
constraints, a recently developed method for solving state-path constrained
optimal control problems is used. This recently developed method is designed to
algorithmically locate the points of activation and deactivation in the path
constraints and partition the domain of the independent variable into
subdomains based on these activation and deactivation points. Additionally, in
a domain where a state-inequality path constraint is found to be active, the
method algorithmically determines and enforces the additional necessary
conditions that apply on the constrained arc. A multiple-domain formulation of
Legendre-Gauss-Radau direct collocation is then employed to transcribe the
optimal control problem into a large sparse nonlinear programming problem. Two
studies are performed which analyze a variety of problem formulations of the
hypersonic reusable launch vehicle. Key features of the constrained
trajectories are presented, and the method used is shown to obtain highly
accurate solutions with minimal user intervention.
","['Cale A. Byczkowski', 'Anil V. Rao']"
http://arxiv.org/abs/2503.11862v1,Reusable launch vehicle,2025-03-14T20:43:58Z,2025-03-14T20:43:58Z,"Ignition Point Reachability for Aerodynamically-Controlled Reusable
  Launch Vehicles","  We describe a successive convex programming (Sequential Convex Programming
(SCP)) based approach for estimate the set of points where a 5-degree of
freedom (5-DoF) reusable launch vehicle (RLV) returning to a landing site can
transition from aerodynamic to propulsive descent. Determining the set of
feasible ignition points that a RLV can use and then safely land is important
for mission planning and range safety. However, past trajectory optimization
approaches for RLVs consider substantially simplified versions of the vehicle
dynamics. Furthermore, prior reachability analysis methods either do not extend
to the full constraint set needed for an RLV or are too beset by the curse of
dimensionality to handle the full 5-DoF dynamics. To solve this problem, we
describe an algorithm that approximates the projection of a high dimensional
reachable set onto a low dimensional space. Instead of computing all parts of
the reachable space, we only calculate reachability in the projected space of
interest by using repeated trajectory optimization to sample the reachable
polytope in the reduced space. The optimization can take into account initial
and terminal constraints as well as state and control constraints. We show that
our algorithm is able to compute the projection of a reachable set into a low
dimensional space by calculating the feasible ignition points for a two-phase
aerodynamic/propulsive RLV landing trajectory, while also demonstrating the
aerodynamic divert enabled by our body and fin actuator model.
","['Benjamin Chung', 'Kazuya Echigo', 'Behçet Açıkmeşe']"
http://arxiv.org/abs/1409.1036v2,Reusable launch vehicle,2014-09-03T11:14:38Z,2015-02-20T08:26:39Z,EMMI - Electric Solar Wind Sail Facilitated Manned Mars Initiative,"  The novel propellantless electric solar wind sail concept promises efficient
low thrust transportation in the Solar System outside Earth's magnetosphere.
Combined with asteroid mining to provide water and synthetic cryogenic rocket
fuel in orbits of Earth and Mars, possibilities for affordable continuous
manned presence on Mars open up. Orbital fuel and water enable reusable
bidirectional Earth-Mars vehicles for continuous manned presence on Mars and
allow smaller fuel fraction of spacecraft than what is achievable by
traditional means. Water can also be used as radiation shielding of the manned
compartment, thus reducing the launch mass further. In addition, the presence
of fuel in the orbit of Mars provides the option for an all-propulsive landing,
thus potentially eliminating issues of heavy heat shields and augmenting the
capability of pinpoint landing. With this E-sail enabled scheme, the recurrent
cost of continuous bidirectional traffic between Earth and Mars might
ultimately approach the recurrent cost of running the International Space
Station, ISS.
","['Pekka Janhunen', 'Sini Merikallio', 'Mark Paton']"
http://arxiv.org/abs/1606.02387v1,Reusable launch vehicle,2016-06-08T03:42:41Z,2016-06-08T03:42:41Z,"Angle-of-Attack Modulation in Trajectory Tracking for a Reusable Launch
  Vehicle","  This paper deals with the problem of angle-of-attack modulation with the aim
of enhancing transient performance of entry guidance during bank reversals,
while compensating adverse effects of fast time-varying transient disturbances.
An extended single-input/single-output system is developed in the velocity
domain by means of a dynamic extension technique, and explicitly captures the
trajectory dynamics of angle-of-attack modulation. A normal form for this
extended system is derived for the sake of employing a feedback linearization
controller. Further, the control characteristics of angle-of-attack modulation
is found to be a non-minimum phase behavior under two common conditions in a
near- equilibrium glide flight. Therefore, the issue of angle-of-attack
modulation is formulated as robust output stabilization of the non-minimum
phase system. A disturbance observer-based feedback linearization technique is
used to design a robustly dynamical output-feedback controller for
angle-of-attack modulation, and an internal-state feedback controller for
bank-angle modulation is used to stabilize the unstable internal dynamics.
Numerical simulations are conducted to demonstrate that the performance of the
proposed method of angle-of-attack modulation is enhanced compared to the
existing shuttle method.
","['Ran Zhang', 'Huifeng Li', 'Rui Zhang']"
http://arxiv.org/abs/2310.05994v1,Reusable launch vehicle,2023-10-09T00:41:01Z,2023-10-09T00:41:01Z,Launch Vehicle High-Energy Performance Dataset,"  The choice of the launch vehicle is an important consideration during the
preliminary planning of interplanetary missions. The launch vehicle must be
highly reliable, capable of imparting sufficient energy to the spacecraft to
inject it on to an Earth-escape trajectory, and must fit within the cost
constraints of the mission. Over the recent past, the most commonly used
launchers for interplanetary missions include the Atlas V401, Atlas V551, Delta
IVH, and Falcon Heavy expendable version. The NASA Launch Vehicle Performance
website maintains a tool to help mission planners evaluate various launch
vehicles during mission studies. However, there is no comprehensive dataset
which can be used to quickly compare the launch performance and launch cost of
various options. The present study compiles a dataset of the high energy
performance of existing and planned launchers from open-source data and
performs a quantitative comparison of the launch performance and the launch
cost per kg. The Falcon Heavy expendable offers the lowest cost-per-kg for
high-energy launches, with only $0.075M per kg. The Vulcan Centaur offers
comparable performance to the Falcon Heavy. The results indicate Falcon Heavy
Expendable and the Vulcan Centaur will be the likely choice for several future
missions.
",['Athul Pradeepkumar Girija']
http://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A
  Quantum Reinforcement Learning Approach","  The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
","['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']"
http://arxiv.org/abs/2411.04073v1,Reusable launch vehicle,2024-11-06T17:50:32Z,2024-11-06T17:50:32Z,"Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles","  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
","['Eashwar Sathyamurthy', 'Jeffrey W. Herrmann', 'Shapour Azarm']"
http://arxiv.org/abs/2009.06495v1,Reusable launch vehicle,2020-09-14T14:58:12Z,2020-09-14T14:58:12Z,"Assembled Kinetic Impactor for Deflecting Asteroids via Combining the
  Spacecraft with the Launch Vehicle Final Stage","  Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the
asteroid from the impact trajectory is an important way to mitigate the threat.
A kinetic impactor remains to be the most feasible method to deflect the
asteroid. However, due to the constraint of the launch capability, an impactor
with the limited mass can only produce a very limited amount of velocity
increment for the asteroid. In order to improve the deflection efficiency of
the kinetic impactor strategy, this paper proposed a new concept called the
Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the
launch vehicle final stage. By making full use of the mass of the launch
vehicle final stage, the mass of the impactor will be increased, which will
cause the improvement of the deflection efficiency. According to the technical
data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu
are designed to demonstrate the power of the AKI concept. Simulation results
show that, compared with the Classic Kinetic Impactor (CKI, performs
spacecraft-rocket separation), the addition of the mass of the launch vehicle
final stage can increase the deflection distance to more than 3 times, and
reduce the launch lead-time by at least 15 years. With the requirement of the
same deflection distance, the addition of the mass of the launch vehicle final
stage can reduce the number of launches to 1/3 of that of the number of CKI
launches. The AKI concept makes it possible to defend Bennu-like large
asteroids by a no-nuclear technique within 10-year launch lead-time. At the
same time, for a single CZ-5, the deflection distance of a 140 m diameter
asteroid within 10-year launch lead-time, can be increased from less than 1
Earth radii to more than 1 Earth radii.
","['Yirui Wang', 'Mingtao Li', 'Zizheng Gong', 'Jianming Wang', 'Chuankui Wang', 'Binghong Zhou']"
http://arxiv.org/abs/2303.17869v1,Reusable launch vehicle,2023-03-31T08:06:20Z,2023-03-31T08:06:20Z,"Numerical Modelling and GNSS Observations of Ionospheric Depletions due
  to a Small-Lift Launch Vehicle","  Space launches produce ionospheric disturbances which can be observed through
measurements such as Global Navigation Satellite System signal delays. Here we
report observations and numerical simulations of the ionospheric depletion due
to a Small-Lift Launch Vehicle. The case examined was the launch of a Rocket
Lab Electron at 22:30 UTC on March 22, 2021. Despite the very small launch
vehicle, ground stations in the Chatham Islands measured decreases in
line-of-sight total electron content for navigation satellite signals following
the launch. General Circulation Model results indicated ionospheric depletions
which were comparable with these measurements. Line-of-sight measurements
showed a maximum decrease of $2.7$~TECU in vertical total electron content,
compared with a simulated decrease of $2.6$~TECU. Advection of the exhaust
plume due to its initial velocity and subsequent effects of neutral winds are
identified as some remaining challenges for this form of modelling.
","['G. W. Bowden', 'M. Brown']"
http://arxiv.org/abs/2205.05205v1,Reusable launch vehicle,2022-05-10T22:56:49Z,2022-05-10T22:56:49Z,An integrated debris environment assessment model,"  Launch behaviors are a key determinant of the orbital environment. Physical
and economic forces such as fragmentations and changing launch costs, or
policies like post-mission disposal (PMD) compliance requirements, will alter
the relative attractiveness of different orbits and lead operators to adjust
their launch behaviors. However, integrating models of adaptive launch behavior
with models of the debris environment remains an open challenge. We present a
statistical framework for integrating theoretically-grounded models of launch
behavior with evolutionary models of the low-Earth orbit (LEO) environment. We
implement this framework using data on satellite launches, the orbital
environment, launch vehicle prices, sectoral revenues, and government budgets
over 2007-2020. The data are combined with a multi-shell and multi-species
Particle-in-a-Box (PIB) model of the debris environment and a two-stage
budgeting model of commercial, civil government, and defense decisions to
allocate new launches across orbital shells. We demonstrate the framework's
capabilities in three counterfactual scenarios: unexpected fragmentation events
in highly-used regions, a sharp decrease in the cost of accessing lower parts
of LEO, and increasing compliance with 25-year PMD guidelines. Substitution
across orbits based on their evolving characteristics and the behavior of other
operators induces notable changes in the debris environment relative to models
without behavioral channels.
","['Akhil Rao', 'Francesca Letizia']"
http://arxiv.org/abs/2307.12642v1,Reusable launch vehicle,2023-07-24T09:32:54Z,2023-07-24T09:32:54Z,"Simultaneous Optimization of Launch Vehicle Stage and Trajectory
  Considering Operational Safety Constraints","  A conceptual design of a launch vehicle involves the optimization of
trajectory and stages considering its launch operations. This process
encompasses various disciplines, such as structural design, aerodynamics,
propulsion systems, flight control, and stage sizing. Traditional approaches
used for the conceptual design of a launch vehicle conduct the stage and
trajectory designs sequentially, often leading to high computational complexity
and suboptimal results. This paper presents an optimization framework that
addresses both trajectory optimization and staging in an integrated way. The
proposed framework aims to maximize the payload-to-liftoff mass ratio while
satisfying the constraints required for safe launch operations (e.g., the
impact points of burnt stages and fairing). A case study demonstrates the
advantage of the proposed framework compared to the traditional sequential
optimization approach.
","['Jaeyoul Ko', 'Jaewoo Kim', 'Jimin Choi', 'Jaemyung Ahn']"
http://arxiv.org/abs/2008.13239v1,Reusable launch vehicle,2020-08-30T18:44:18Z,2020-08-30T18:44:18Z,"Convex Optimization of Launch Vehicle Ascent Trajectory with Heat-Flux
  and Splash-Down Constraints","  This paper presents a convex programming approach to the optimization of a
multistage launch vehicle ascent trajectory, from the liftoff to the payload
injection into the target orbit, taking into account multiple nonconvex
constraints, such as the maximum heat flux after fairing jettisoning and the
splash-down of the burned-out stages. Lossless and successive convexification
are employed to convert the problem into a sequence of convex subproblems.
Virtual controls and buffer zones are included to ensure the recursive
feasibility of the process and a state-of-the-art method for updating the
reference solution is implemented to filter out undesired phenomena that may
hinder convergence. A hp pseudospectral discretization scheme is used to
accurately capture the complex ascent and return dynamics with a limited
computational effort. The convergence properties, computational efficiency, and
robustness of the algorithm are discussed on the basis of numerical results.
The ascent of the VEGA launch vehicle toward a polar orbit is used as case
study to discuss the interaction between the heat flux and splash-down
constraints. Finally, a sensitivity analysis of the launch vehicle carrying
capacity to different splash-down locations is presented.
","['Boris Benedikter', 'Alessandro Zavoli', 'Guido Colasurdo', 'Simone Pizzurro', 'Enrico Cavallini']"
http://arxiv.org/abs/1611.06925v1,Reusable launch vehicle,2016-11-21T18:13:27Z,2016-11-21T18:13:27Z,"Robust Design of H-infinity Controller for a Launch Vehicle Autopilot
  against Disturbances","  Atmospheric flight phase of a launch vehicle is utilized to evaluate the
performance of an H-infinity controller in the presence of disturbances.
Dynamics of the vehicle is linearly modeled using time-varying parameters. An
operating point was found to design a robust command tracker using H-infinity
control theory that guarantees a stable maneuver. At the end, the controller
was employed on the launch vehicle to assess the capability of control design
on the linearized aerospace vehicle. Experimental results illustrate the
excellent performance of the H-infinity controller and accurate tracking
implemented by the autopilot. Also the robustness of the entire system against
disturbances is demonstrated to be acceptable.
","['Antonio Graells', 'Francisco Carrabina']"
http://arxiv.org/abs/1611.05512v1,Reusable launch vehicle,2016-11-17T00:13:45Z,2016-11-17T00:13:45Z,"Unmatched Perturbation Accommodation for an Aerospace Launch Vehicle
  Autopilot Using Dynamic Sliding Manifolds","  Sliding mode control of a launch vehicle during its atmospheric flight phase
is studied in the presence of unmatched disturbances. Linear time-varying
dynamics of the aerospace vehicle is converted into a systematic formula and
then dynamic sliding manifold as an advanced method is used in order to
overcome the limited capability of conventional sliding manifolds in minimizing
the undesired effects of unmatched perturbations on the control system. At the
end, simulation results are evaluated and the performance of two approaches are
compared in terms of stability and robustness of the autopilot.
",['Mohammad Reza Saniee']
http://arxiv.org/abs/2307.16788v1,Reusable launch vehicle,2023-07-31T15:55:50Z,2023-07-31T15:55:50Z,Congestion Analysis for the DARPA OFFSET CCAST Swarm,"  The Defense Advanced Research Projects Agency (DARPA) OFFensive Swarm-Enabled
Tactics program's goal of launching 250 unmanned aerial and ground vehicles
from a limited sized launch zone was a daunting challenge. The swarm's aerial
vehicles were primarily multirotor platforms, which can efficiently be launched
en masse. Each field exercise expected the deployment of an even larger swarm.
While the launch zone's spatial area increased with each field exercise, the
relative space for each vehicle was not necessarily increased, considering the
increasing size of the swarm and the vehicles' associated GPS error; however,
safe mission deployment and execution were expected. At the same time,
achieving the mission goals required maximizing efficiency of the swarm's
performance by reducing congestion that blocked vehicles from completing tactic
assignments. Congestion analysis conducted before the final field exercise
focused on adjusting various constraints to optimize the swarm's deployment
without reducing safety. During the field exercise, data was collected that
permitted analyzing the number and durations of individual vehicle blockages'
impact on the resulting congestion. After the field exercise, additional
analyses used the mission plan to validate the use of simulation for analyzing
congestion.
","['Robert Brown', 'Julie A. Adams']"
http://arxiv.org/abs/1911.05639v1,Reusable launch vehicle,2019-11-13T17:16:51Z,2019-11-13T17:16:51Z,Design of a Ballistically-Launched Foldable Multirotor,"  The operation of multirotors in crowded environments requires a highly
reliable takeoff method, as failures during takeoff can damage more valuable
assets nearby. The addition of a ballistic launch system imposes a
deterministic path for the multirotor to prevent collisions with its
environment, as well as increases the multirotor's range of operation and
allows deployment from an unsteady platform. In addition, outfitting planetary
rovers or entry vehicles with such deployable multirotors has the potential to
greatly extend the data collection capabilities of a mission. A
proof-of-concept multirotor aircraft has been developed, capable of
transitioning from a ballistic launch configuration to a fully controllable
flight configuration in midair after launch. The transition is accomplished via
passive unfolding of the multirotor arms, triggered by a nichrome burn wire
release mechanism. The design is 3D printable, launches from a three-inch
diameter barrel, and has sufficient thrust to carry a significant payload. The
system has been fabricated and field tested from a moving vehicle up to 50mph
to successfully demonstrate the feasibility of the concept and experimentally
validate the design's aerodynamic stability and deployment reliability.
","['Daniel Pastor', 'Jacob Izraelevitz', 'Paul Nadan', 'Amanda Bouman', 'Joel Burdick', 'Brett Kennedy']"
http://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,"  Book chapter that summarizes recent research on agricultural robotics in
orchard management, including Robotic pruning, Robotic thinning, Robotic
spraying, Robotic harvesting, Robotic fruit transportation, and future trends.
","['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']"
http://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,"  Definition: The terms ""robotics in snow and ice"" refers to robotic systems
being studied, developed, and used in areas where water can be found in its
solid state. This specialized branch of field robotics investigates the impact
of extreme conditions related to cold environments on autonomous vehicles.
",['François Pomerleau']
http://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,"  Robot accidents are inevitable. Although rare, they have been happening since
assembly-line robots were first introduced in the 1960s. But a new generation
of social robots are now becoming commonplace. Often with sophisticated
embedded artificial intelligence (AI) social robots might be deployed as care
robots to assist elderly or disabled people to live independently. Smart robot
toys offer a compelling interactive play experience for children and
increasingly capable autonomous vehicles (AVs) the promise of hands-free
personal transport and fully autonomous taxis. Unlike industrial robots which
are deployed in safety cages, social robots are designed to operate in human
environments and interact closely with humans; the likelihood of robot
accidents is therefore much greater for social robots than industrial robots.
This paper sets out a draft framework for social robot accident investigation;
a framework which proposes both the technology and processes that would allow
social robot accidents to be investigated with no less rigour than we expect of
air or rail accident investigations. The paper also places accident
investigation within the practice of responsible robotics, and makes the case
that social robotics without accident investigation would be no less
irresponsible than aviation without air accident investigation.
","['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']"
http://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,"  This paper presents a deterministic algorithm for forming a given asymmetric
pattern in finite time by a set of autonomous, homogeneous, oblivious mobile
robots under the CORDA model. The robots are represented as points on the 2D
plane. There is no explicit communication between the robots. The robots
coordinate among themselves by observing the positions of the other robots on
the plane. Initially all the robots are assumed to be stationary. The robots
have local coordinate systems defined by Sense of Direction (SoD), orientation
or chirality and scale. Initially the robots are in asymmetric configuration.
We show that these robots can form any given asymmetric pattern in finite time.
","['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']"
http://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,"  The traditional distributed model of autonomous, homogeneous, mobile point
robots usually assumes that the robots do not create any visual obstruction for
the other robots, i.e., the robots are see through. In this paper, we consider
a slightly more realistic model, by incorporating the notion of obstructed
visibility (i.e., robots are not see through) for other robots. Under the new
model of visibility, a robot may not have the full view of its surroundings.
Many of the existing algorithms demand that each robot should have the complete
knowledge of the positions of other robots. Since, vision is the only mean of
their communication, it is required that the robots are in general position
(i.e., no three robots are collinear). We consider asynchronous robots. They
also do not have common chirality (or any agreement on a global coordinate
system). In this paper, we present a distributed algorithm for obtaining a
general position for the robots in finite time from any arbitrary
configuration. The algorithm also assures collision free motion for each robot.
This algorithm may also be used as a preprocessing module for many other
subsequent tasks performed by the robots.
","['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']"
http://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,"  Cuspidal robots can move from one inverse or direct kinematic solution to
another without ever passing through a singularity. These robots have remained
unknown because almost all industrial robots do not have this feature. However,
in fact, industrial robots are the exceptions. Some robots appeared recently in
the industrial market can be shown to be cuspidal but, surprisingly, almost
nobody knows it and robot users meet difficulties in planning trajectories with
these robots. This paper proposes a review on the fundamental and application
aspects of cuspidal robots. It addresses the important issues raised by these
robots for the design and planning of trajectories. The identification of all
cuspidal robots is still an open issue. This paper recalls in details the case
of serial robots with three joints but it also addresses robots with more
complex architectures such as 6-revolute-jointed robot and parallel robots. We
hope that this paper will help disseminate more widely knowledge on cuspidal
robots.
","['Philippe Wenger', 'Damien Chablat']"
http://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,"  Given a set of co-located mobile robots in an unknown anonymous graph, the
robots must relocate themselves in distinct graph nodes to solve the dispersion
problem. In this paper, we consider the dispersion problem for silent robots
\cite{gorain2024collaborative}, i.e., no direct, explicit communication between
any two robots placed in the nodes of an oriented $n$ node ring network. The
robots operate in synchronous rounds. The dispersion problem for silent mobile
robots has been studied in arbitrary graphs where the robots start from a
single source. In this paper, we focus on the dispersion problem for silent
mobile robots where robots can start from multiple sources. The robots have
unique labels from a range $[0,\;L]$ for some positive integer $L$. Any two
co-located robots do not have the information about the label of the other
robot. The robots have weak multiplicity detection capability, which means they
can determine if it is alone on a node. The robots are assumed to be able to
identify an increase or decrease in the number of robots present on a node in a
particular round. However, the robots can not get the exact number of increase
or decrease in the number of robots. We have proposed a deterministic
distributed algorithm that solves the dispersion of $k$ robots in an oriented
ring in $O(\log L+k)$ synchronous rounds with $O(\log L)$ bits of memory for
each robot. A lower bound $\Omega(\log L+k)$ on time for the dispersion of $k$
robots on a ring network is presented to establish the optimality of the
proposed algorithm.
","['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']"
http://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","  This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.
","['Pedro U. Lima', 'Luis M. M. Custodio']"
http://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,"Medical robotics: where we come from, where we are and where we could go","  This short note presents a viewpoint about medical robotics.
",['Jocelyne Troccaz']
http://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,"  In human-robot teams, humans often start with an inaccurate model of the
robot capabilities. As they interact with the robot, they infer the robot's
capabilities and partially adapt to the robot, i.e., they might change their
actions based on the observed outcomes and the robot's actions, without
replicating the robot's policy. We present a game-theoretic model of human
partial adaptation to the robot, where the human responds to the robot's
actions by maximizing a reward function that changes stochastically over time,
capturing the evolution of their expectations of the robot's capabilities. The
robot can then use this model to decide optimally between taking actions that
reveal its capabilities to the human and taking the best action given the
information that the human currently has. We prove that under certain
observability assumptions, the optimal policy can be computed efficiently. We
demonstrate through a human subject experiment that the proposed model
significantly improves human-robot team performance, compared to policies that
assume complete adaptation of the human to the robot.
","['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']"
http://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,"  In this work, we present an algorithm for robot replacement to increase the
operational time of a multi-robot payload transport system. Our system
comprises a group of nonholonomic wheeled mobile robots traversing on a known
trajectory. We design a multi-robot system with loosely coupled robots that
ensures the system lasts much longer than the battery life of an individual
robot. A system level optimization is presented, to decide on the operational
state (charging or discharging) of each robot in the system. The charging state
implies that the robot is not in a formation and is kept on charge whereas the
discharging state implies that the robot is a part of the formation. Robot
battery recharge hubs are present along the trajectory. Robots in the formation
can be replaced at these hub locations with charged robots using a replacement
mechanism. We showcase the efficacy of the proposed scheduling framework
through simulations and experiments with real robots.
","['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']"
http://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,"  Humans often assume that robots are rational. We believe robots take optimal
actions given their objective; hence, when we are uncertain about what the
robot's objective is, we interpret the robot's actions as optimal with respect
to our estimate of its objective. This approach makes sense when robots
straightforwardly optimize their objective, and enables humans to learn what
the robot is trying to achieve. However, our insight is that---when robots are
aware that humans learn by trusting that the robot actions are
rational---intelligent robots do not act as the human expects; instead, they
take advantage of the human's trust, and exploit this trust to more efficiently
optimize their own objective. In this paper, we formally model instances of
human-robot interaction (HRI) where the human does not know the robot's
objective using a two-player game. We formulate different ways in which the
robot can model the uncertain human, and compare solutions of this game when
the robot has conservative, optimistic, rational, and trusting human models. In
an offline linear-quadratic case study and a real-time user study, we show that
trusting human models can naturally lead to communicative robot behavior, which
influences end-users and increases their involvement.
","['Dylan P. Losey', 'Dorsa Sadigh']"
http://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,"Animation Techniques in Human-Robot Interaction User Studies: a
  Systematic Literature Review","  There are many different ways a robot can move in Human-Robot Interaction.
One way is to use techniques from film animation to instruct the robot to move.
This article is a systematic literature review of human-robot trials, pilots,
and evaluations that have applied techniques from animation to move a robot.
Through 27 articles, we find that animation techniques improves individual's
interaction with robots, improving individual's perception of qualities of a
robot, understanding what a robot intends to do, and showing the robot's state,
or possible emotion. Animation techniques also help people relate to robots
that do not resemble a human or robot. The studies in the articles show further
areas for research, such as applying animation principles in other types of
robots and situations, combining animation techniques with other modalities,
and testing robots moving with animation techniques over the long term.
","['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']"
http://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,"Robot Vitals and Robot Health: Towards Systematically Quantifying
  Runtime Performance Degradation in Robots Under Adverse Conditions","  This paper addresses the problem of automatically detecting and quantifying
performance degradation in remote mobile robots during task execution. A robot
may encounter a variety of uncertainties and adversities during task execution,
which can impair its ability to carry out tasks effectively and cause its
performance to degrade. Such situations can be mitigated or averted by timely
detection and intervention (e.g., by a remote human supervisor taking over
control in teleoperation mode). Inspired by patient triaging systems in
hospitals, we introduce the framework of ""robot vitals"" for estimating overall
""robot health"". A robot's vitals are a set of indicators that estimate the
extent of performance degradation faced by a robot at a given point in time.
Robot health is a metric that combines robot vitals into a single scalar value
estimate of performance degradation. Experiments, both in simulation and on a
real mobile robot, demonstrate that the proposed robot vitals and robot health
can be used effectively to estimate robot performance degradation during
runtime.
","['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']"
http://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,"Come Closer: The Effects of Robot Personality on Human Proxemics
  Behaviours","  Social Robots in human environments need to be able to reason about their
physical surroundings while interacting with people. Furthermore, human
proxemics behaviours around robots can indicate how people perceive the robots
and can inform robot personality and interaction design. Here, we introduce
Charlie, a situated robot receptionist that can interact with people using
verbal and non-verbal communication in a dynamic environment, where users might
enter or leave the scene at any time. The robot receptionist is stationary and
cannot navigate. Therefore, people have full control over their personal space
as they are the ones approaching the robot. We investigated the influence of
different apparent robot personalities on the proxemics behaviours of the
humans. The results indicate that different types of robot personalities,
specifically introversion and extroversion, can influence human proxemics
behaviours. Participants maintained shorter distances with the introvert robot
receptionist, compared to the extrovert robot. Interestingly, we observed that
human-robot proxemics were not the same as typical human-human interpersonal
distances, as defined in the literature. We therefore propose new proxemics
zones for human-robot interaction.
","['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']"
http://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,"  With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.
","['Rajashekhar V S', 'Gowdham Prabhakar']"
http://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,"  For a robot to be perfect and enter the everyday life of humans,like
computers did, it needs to move from special-purpose robots to general-purpose.
So, the idea of modularity is considered in this project.Thus, any type of task
that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be
achieved by adding a module to the robot.
","['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']"
http://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots
that can move from one inverse geometric solution to another without meeting a
singular confuguration. This feature was discovered quite recently and has then
been fascinating a lot of researchers. After a brief history of cuspidal
robots, the chapter provides the main features of cuspidal robots: explanation
of the non-singular change of posture, uniqueness domains, regions of feasible
paths, identification and classification of cuspidal robots. The chapter
focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel
robots is discussed in the end of this chapter.
",['Philippe Wenger']
http://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,"  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.
","['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']"
http://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,"  This paper shows how Graph Neural Networks can be used for learning
distributed coordination mechanisms in connected teams of robots. We capture
the relational aspect of robot coordination by modeling the robot team as a
graph, where each robot is a node, and edges represent communication links.
During training, robots learn how to pass messages and update internal states,
so that a target behavior is reached. As a proxy for more complex problems,
this short paper considers the problem where each robot must locally estimate
the algebraic connectivity of the team's network topology.
",['Amanda Prorok']
http://arxiv.org/abs/2304.06568v1,Smart contracts,2023-04-13T14:26:12Z,2023-04-13T14:26:12Z,"Smart Contract Upgradeability on the Ethereum Blockchain Platform: An
  Exploratory Study","  Context: Smart contracts are computerized self-executing contracts that
contain clauses, which are enforced once certain conditions are met. Smart
contracts are immutable by design and cannot be modified once deployed, which
ensures trustlessness. Despite smart contracts' immutability benefits,
upgrading contract code is still necessary for bug fixes and potential feature
improvements. In the past few years, the smart contract community introduced
several practices for upgrading smart contracts. Upgradeable contracts are
smart contracts that exhibit these practices and are designed with
upgradeability in mind. During the upgrade process, a new smart contract
version is deployed with the desired modification, and subsequent user requests
will be forwarded to the latest version (upgraded contract). Nevertheless,
little is known about the characteristics of the upgrading practices, how
developers apply them, and how upgrading impacts contract usage.
  Objectives: This paper aims to characterize smart contract upgrading patterns
and analyze their prevalence based on the deployed contracts that exhibit these
patterns. Furthermore, we intend to investigate the reasons why developers
upgrade contracts (e.g., introduce features, fix vulnerabilities) and how
upgrades affect the adoption and life span of a contract in practice.
  Method: We collect deployed smart contracts metadata and source codes to
identify contracts that exhibit certain upgrade patterns (upgradeable
contracts) based on a set of policies. Then we trace smart contract versions
for each upgradable contract and identify the changes in contract versions
using similarity and vulnerabilities detection tools. Finally, we plan to
analyze the impact of upgrading on contract usage based on the number of
transactions received and the lifetime of the contract version.
","['Ilham Qasse', 'Mohammad Hamdaqa', 'Björn Þór Jónsson']"
http://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,"  Smart contracts are immutable, verifiable, and autonomous pieces of code that
can be deployed and ran on blockchain networks like Ethereum. Due to the
immutability nature of blockchain, no change is possible on a deployed smart
contract or a verified transaction. On the other hand, there are millions of
dollars carried by smart contracts in Ethereum blockchain, and hence, a faulty
smart contract can lead to a huge monetary loss. Therefore, it is important for
smart contract developers to fully test and check the correctness of their code
before deploying it on the blockchain. In this paper, we propose a testing
mechanism for smart contracts in Solidity language, based on mutation testing.
We analyzed a comprehensive list of known bugs in Solidity smart contracts, and
designed 10 classes of mutation operators inspired by the real faults. Our
experimental results show that our proposed mutation operators can regenerate
10 of 15 famous faulty smart contracts, which have resulted in millions of
dollars loss. The results show the effectiveness of our proposed mutation
operators in detecting real faults in Solidity smart contracts. We have also
extended {\em Universal Mutator } tool with our mutation operators, so that it
can automatically generate mutants for smart contracts written in Solidity.
","['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']"
http://arxiv.org/abs/2001.10589v1,Smart contracts,2020-01-21T03:48:46Z,2020-01-21T03:48:46Z,"Blockchain Enabled Smart Contract Based Applications: Deficiencies with
  the Software Development Life Cycle Models","  With the recent popularity of Blockchain and other Distributed Ledger
Technologies (DLT), blockchain enabled smart contract applications has
attracted increased research focus. However, the immutability of the blocks,
where the smart contracts are stored, causes conflicts with the traditional
Software Development Life Cycle (SDLC) models usually followed by software
engineers. This clearly shows the unsuitability of the application of SDLC in
designing blockchain enabled smart contract based applications. This research
article addresses this current problem by first exploring the six traditional
SDLC models, clearly identifying the conflicts in a table with the application
of smart contracts and advocates that there is an urgent need to develop new
standard model(s) to address the arising issues. The concept of both block
immutability and contract is introduced. This is further set in a historical
context from legacy smart contracts and blockchain enabled smart contracts
extending to the difference between ""shallow smart contracts"" and ""deep smart
contracts"". To conclude, the traditional SDLC models are unsuitable for
blockchain enabled smart contract-based applications.
","['Mahdi H. Miraz', 'Maaruf Ali']"
http://arxiv.org/abs/1912.10370v1,Smart contracts,2019-12-22T01:52:54Z,2019-12-22T01:52:54Z,"An Overview on Smart Contracts: Challenges, Advances and Platforms","  Smart contract technology is reshaping conventional industry and business
processes. Being embedded in blockchains, smart contracts enable the
contractual terms of an agreement to be enforced automatically without the
intervention of a trusted third party. As a result, smart contracts can cut
down administration and save services costs, improve the efficiency of business
processes and reduce the risks. Although smart contracts are promising to drive
the new wave of innovation in business processes, there are a number of
challenges to be tackled.This paper presents a survey on smart contracts. We
first introduce blockchains and smart contracts. We then present the challenges
in smart contracts as well as recent technical advances. We also compare
typical smart contract platforms and give a categorization of smart contract
applications along with some representative examples.
","['Zibin Zheng', 'Shaoan Xie', 'Hong-Ning Dai', 'Weili Chen', 'Xiangping Chen', 'Jian Weng', 'Muhammad Imran']"
http://arxiv.org/abs/2101.08964v1,Smart contracts,2021-01-22T06:24:08Z,2021-01-22T06:24:08Z,Probabilistic Framework For Loss Distribution Of Smart Contract Risk,"  Smart contract risk can be defined as a financial risk of loss due to cyber
attacks on or contagious failures of smart contracts. Its quantification is of
paramount importance to technology platform providers as well as companies and
individuals when considering the deployment of this new technology. That is
why, as our primary contribution, we propose a structural framework of
aggregate loss distribution for smart contract risk under the assumption of a
tree-stars graph topology representing the network of interactions among smart
contracts and their users. Up to our knowledge, there exist no theoretical
frameworks or models of an aggregate loss distribution for smart contracts in
this setting. To achieve our goal, we contextualize the problem in the
probabilistic graph-theoretical framework using bond percolation models. We
assume that the smart contract network topology is represented by a random tree
graph of finite size, and that each smart contract is the center of a {random}
star graph whose leaves represent the users of the smart contract. We allow for
heterogeneous loss topology superimposed on this smart contract and user
topology and provide analytical results and instructive numerical examples.
","['Petar Jevtic', 'Nicolas Lanchier']"
http://arxiv.org/abs/2505.22619v1,Smart contracts,2025-05-28T17:40:21Z,2025-05-28T17:40:21Z,Smart Contracts for SMEs and Large Companies,"  Research on blockchains addresses multiple issues, with one being writing
smart contracts. In our previous research we described methodology and a tool
to generate, in automated fashion, smart contracts from BPMN models. The
generated smart contracts provide support for multi-step transactions that
facilitate repair/upgrade of smart contracts. In this paper we show how the
approach is used to support collaborations via smart contracts for companies
ranging from SMEs with little IT capabilities to companies with IT using
blockchain smart contracts. Furthermore, we also show how the approach is used
for certain applications to generate smart contracts by a BPMN modeler who does
not need any knowledge of blockchain technology or smart contract development -
thus we are hoping to facilitate democratization of smart contracts and
blockchain technology.
","['C. G. Liu', 'P. Bodorik', 'D. Jutla']"
http://arxiv.org/abs/1702.04467v1,Smart contracts,2017-02-15T05:38:37Z,2017-02-15T05:38:37Z,Adding Concurrency to Smart Contracts,"  Modern cryptocurrency systems, such as Ethereum, permit complex financial
transactions through scripts called smart contracts. These smart contracts are
executed many, many times, always without real concurrency. First, all smart
contracts are serially executed by miners before appending them to the
blockchain. Later, those contracts are serially re-executed by validators to
verify that the smart contracts were executed correctly by miners.
  Serial execution limits system throughput and fails to exploit today's
concurrent multicore and cluster architectures. Nevertheless, serial execution
appears to be required: contracts share state, and contract programming
languages have a serial semantics.
  This paper presents a novel way to permit miners and validators to execute
smart contracts in parallel, based on techniques adapted from software
transactional memory. Miners execute smart contracts speculatively in parallel,
allowing non-conflicting contracts to proceed concurrently, and ""discovering"" a
serializable concurrent schedule for a block's transactions, This schedule is
captured and encoded as a deterministic fork-join program used by validators to
re-execute the miner's parallel schedule deterministically but concurrently.
  Smart contract benchmarks run on a JVM with ScalaSTM show that a speedup of
of 1.33x can be obtained for miners and 1.69x for validators with just three
concurrent threads.
","['Thomas Dickerson', 'Paul Gazzillo', 'Maurice Herlihy', 'Eric Koskinen']"
http://arxiv.org/abs/1905.01467v3,Smart contracts,2019-05-04T09:58:49Z,2020-04-17T05:18:51Z,Defining Smart Contract Defects on Ethereum,"  Smart contracts are programs running on a blockchain. They are immutable to
change, and hence can not be patched for bugs once deployed. Thus it is
critical to ensure they are bug-free and well-designed before deployment. A
Contract defect is an error, flaw or fault in a smart contract that causes it
to produce an incorrect or unexpected result, or to behave in unintended ways.
The detection of contract defects is a method to avoid potential bugs and
improve the design of existing code. Since smart contracts contain numerous
distinctive features, such as the gas system. decentralized, it is important to
find smart contract specified defects. To fill this gap, we collected
smart-contract-related posts from Ethereum StackExchange, as well as real-world
smart contracts. We manually analyzed these posts and contracts; using them to
define 20 kinds of contract defects. We categorized them into indicating
potential security, availability, performance, maintainability and reusability
problems. To validate if practitioners consider these contract as harmful, we
created an online survey and received 138 responses from 32 different
countries. Feedback showed these contract defects are harmful and removing them
would improve the quality and robustness of smart contracts. We manually
identified our defined contract defects in 587 real world smart contract and
publicly released our dataset. Finally, we summarized 5 impacts caused by
contract defects. These help developers better understand the symptoms of the
defects and removal priority.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Daniel Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02663v2,Smart contracts,2020-09-06T07:38:45Z,2021-03-23T03:02:43Z,"DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing
  EVM Bytecode","  Smart contracts are Turing-complete programs running on the blockchain. They
are immutable and cannot be modified, even when bugs are detected. Therefore,
ensuring smart contracts are bug-free and well-designed before deploying them
to the blockchain is extremely important. A contract defect is an error, flaw
or fault in a smart contract that causes it to produce an incorrect or
unexpected result, or to behave in unintended ways. Detecting and removing
contract defects can avoid potential bugs and make programs more robust. Our
previous work defined 20 contract defects for smart contracts and divided them
into five impact levels. According to our classification, contract defects with
seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract
being controlled by attackers. In this paper, we propose DefectChecker, a
symbolic execution-based approach and tool to detect eight contract defects
that can cause unwanted behaviors of smart contracts on the Ethereum blockchain
platform. DefectChecker can detect contract defects from smart contracts
bytecode. We compare DefectChecker with key previous works, including Oyente,
Mythril and Securify by using an open-source dataset. Our experimental results
show that DefectChecker performs much better than these tools in terms of both
speed and accuracy. We also applied DefectChecker to 165,621 distinct smart
contracts on the Ethereum platform. We found that 25,815 of these smart
contracts contain at least one of the contract defects that belongs to impact
level 1-3, including some real-world attacks.
","['Jiachi Chen', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiapu Luo', 'Ting Chen']"
http://arxiv.org/abs/2009.02066v1,Smart contracts,2020-09-04T08:37:58Z,2020-09-04T08:37:58Z,A Framework and DataSet for Bugs in Ethereum Smart Contracts,"  Ethereum is the largest blockchain platform that supports smart contracts.
Users deploy smart contracts by publishing the smart contract's bytecode to the
blockchain. Since the data in the blockchain cannot be modified, even if these
contracts contain bugs, it is not possible to patch deployed smart contracts
with code updates. Moreover, there is currently neither a comprehensive
classification framework for Ethereum smart contract bugs, nor detailed
criteria for detecting bugs in smart contracts, making it difficult for
developers to fully understand the negative effects of bugs and design new
approaches to detect bugs. In this paper, to fill the gap, we first collect as
many smart contract bugs as possible from multiple sources and divide these
bugs into 9 categories by extending the IEEE Standard Classification for
Software Anomalies. Then, we design the criteria for detecting each kind of
bugs, and construct a dataset of smart contracts covering all kinds of bugs.
With our framework and dataset, developers can learn smart contract bugs and
develop new tools to detect and locate bugs in smart contracts. Moreover, we
evaluate the state-of-the-art tools for smart contract analysis with our
dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are
the most worthwhile combination of analysis tools. 2) There are still 10 kinds
of bugs that cannot be detected by any analysis tool.
","['Pengcheng Zhang', 'Feng Xiao', 'Xiapu Luo']"
http://arxiv.org/abs/2403.19805v2,Smart contracts,2024-03-28T19:36:53Z,2024-04-08T18:33:46Z,"Vulnerabilities of smart contracts and mitigation schemes: A
  Comprehensive Survey","  Ethereum smart contracts are highly powerful, immutable, and able to retain
massive amounts of tokens. However, smart contracts keep attracting attackers
to benefit from smart contract flaws and Ethereum unexpected behavior. Thus,
methodologies and tools have been proposed to help implement secure smart
contracts and to evaluate the security of smart contracts already deployed.
Most related surveys focus on tools without discussing the logic behind them.
in addition, they assess the tools based on papers rather than testing the
tools and collecting community feedback. Other surveys lack guidelines on how
to use tools specific to smart contract functionalities. This paper presents a
literature review combined with an experimental report that aims to assist
developers in developing secure smarts, with a novel emphasis on the challenges
and vulnerabilities introduced by NFT fractionalization by addressing the
unique risks of dividing NFT ownership into tradeable units called fractions.
It provides a list of frequent vulnerabilities and corresponding mitigation
solutions. In addition, it evaluates the community most widely used tools by
executing and testing them on sample smart contracts. Finally, a comprehensive
guide on implementing secure smart contracts is presented.
","['Wejdene Haouari', 'Abdelhakim Senhaji Hafid', 'Marios Fokaefs']"
http://arxiv.org/abs/1807.03932v2,Smart contracts,2018-07-11T02:32:54Z,2018-08-03T01:03:39Z,ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection,"  Decentralized cryptocurrencies feature the use of blockchain to transfer
values among peers on networks without central agency. Smart contracts are
programs running on top of the blockchain consensus protocol to enable people
make agreements while minimizing trusts. Millions of smart contracts have been
deployed in various decentralized applications. The security vulnerabilities
within those smart contracts pose significant threats to their applications.
Indeed, many critical security vulnerabilities within smart contracts on
Ethereum platform have caused huge financial losses to their users. In this
work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart
contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs
based on the ABI specifications of smart contracts, defines test oracles to
detect security vulnerabilities, instruments the EVM to log smart contracts
runtime behaviors, and analyzes these logs to report security vulnerabilities.
Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities
with high precision. In particular, our fuzzing tool successfully detects the
vulnerability of the DAO contract that leads to USD 60 million loss and the
vulnerabilities of Parity Wallet that have led to the loss of $30 million and
the freezing of USD 150 million worth of Ether.
","['Bo Jiang', 'Ye Liu', 'W. K. Chan']"
http://arxiv.org/abs/2005.11839v1,Smart contracts,2020-05-24T20:49:13Z,2020-05-24T20:49:13Z,"Tezla, an Intermediate Representation for Static Analysis of Michelson
  Smart Contracts","  This paper introduces Tezla, an intermediate representation of Michelson
smart contracts that eases the design of static smart contract analysers. This
intermediate representation uses a store and preserves the semantics, ow and
resource usage of the original smart contract. This enables properties like gas
consumption to be statically verified. We provide an automated decompiler of
Michelson smart contracts to Tezla. In order to support our claim about the
adequacy of Tezla, we develop a static analyser that takes advantage of the
Tezla representation of Michelson smart contracts to prove simple but
non-trivial properties.
","['João Santos Reis', 'Paul Crocker', 'Simão Melo de Sousa']"
http://arxiv.org/abs/2110.08983v1,Smart contracts,2021-10-18T02:25:54Z,2021-10-18T02:25:54Z,An Empirical Study of Protocols in Smart Contracts,"  Smart contracts are programs that are executed on a blockhain. They have been
used for applications in voting, decentralized finance, and supply chain
management. However, vulnerabilities in smart contracts have been abused by
hackers, leading to financial losses. Understanding state machine protocols in
smart contracts has been identified as important to catching common bugs,
improving documentation, and optimizing smart contracts. We analyze Solidity
smart contracts deployed on the Ethereum blockchain and study the prevalence of
protocols and protocol-based bugs, as well as opportunities for gas
optimizations.
","['Timothy Mou', 'Michael Coblenz', 'Jonathan Aldrich']"
http://arxiv.org/abs/1907.09208v1,Smart contracts,2019-07-22T10:03:23Z,2019-07-22T10:03:23Z,"Truffle tests for free -- Replaying Ethereum smart contracts for
  transparency","  The Ethereum blockchain is essentially a globally replicated public database.
Programs called smart contracts can access this database. Over 10 million smart
contracts have been deployed on the Ethereum blockchain. Executing a method of
a smart contract generates a transaction that is also stored on the blockchain.
There are over 1 billion Ethereum transactions to date. Smart contracts that
are transparent about their function are more successful than opaque contracts.
We have therefore developed a tool (ContractVis) to explore the transparency of
smart contracts. The tool generates a replay script for the historic
transactions of a smart contract. The script executes the transactions with the
same arguments as recorded on the blockchain, but in a minimal test
environment. Running a replay script provides insights into the contract, and
insights into the blockchain explorer that was used to retrieve the contract
and its history. We provide five concrete recommendations for blockchain
explorers like Etherscan to improve the transparency of smart contracts.
","['Pieter Hartel', 'Mark van Staalduinen']"
http://arxiv.org/abs/2412.20866v1,Smart contracts,2024-12-30T11:10:22Z,2024-12-30T11:10:22Z,"An Infrastructure for Systematically Collecting Smart Contract Lineages
  for Analyses","  Tracking the evolution of smart contracts is a significant challenge,
impeding on the advancement of research on smart contract analysis. Indeed, due
to the inherent immutability of the underlying blockchain technology, each
smart contract update results in a deployment at a new address, breaking the
links between versions. Existing platforms like Etherscan lack the capability
to trace the predecessor-successor relationships within a smart contract
lineage, further hindering empirical research on contract evolution.
  We address this challenge for the research community towards building a
reliable dataset of linked versions for various smart contracts, i.e.,
lineages: we introduce SCLineage, an automated infrastructure that accurately
identifies and collects smart contract lineages by leveraging proxy contracts.
We present SCLineageSet, an up-to-date, open-source dataset that facilitates
extensive research on smart contract evolution. We illustrate the applicability
of our proposal in software engineering research through a case study that
explores the evaluation of Locality-Sensitive Hashing (LSH) for forming
contract lineages. This example underscores how SCLineage provides valuable
insights for future research in the field.
","['Fatou Ndiaye Mbodji', 'Vinny Adjibi', 'Gervais Mendy', 'Moustapha Awwalou Diouf', 'Jacques Klein', 'Tegawende Bissyande']"
http://arxiv.org/abs/2207.13827v1,Smart contracts,2022-07-27T23:36:22Z,2022-07-27T23:36:22Z,Declarative Smart Contracts,"  This paper presents DeCon, a declarative programming language for
implementing smart contracts and specifying contract-level properties. Driven
by the observation that smart contract operations and contract-level properties
can be naturally expressed as relational constraints, DeCon models each smart
contract as a set of relational tables that store transaction records. This
relational representation of smart contracts enables convenient specification
of contract properties, facilitates run-time monitoring of potential property
violations, and brings clarity to contract debugging via data provenance.
Specifically, a DeCon program consists of a set of declarative rules and
violation query rules over the relational representation, describing the smart
contract implementation and contract-level properties, respectively. We have
developed a tool that can compile DeCon programs into executable Solidity
programs, with instrumentation for run-time property monitoring. Our case
studies demonstrate that DeCon can implement realistic smart contracts such as
ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal
overhead of DeCon compared to the open-source reference implementation,
incurring 14% median gas overhead for execution, and another 16% median gas
overhead for run-time verification.
","['Haoxian Chen', 'Gerald Whitters', 'Mohammad Javad Amiri', 'Yuepeng Wang', 'Boon Thau Loo']"
http://arxiv.org/abs/2307.00549v1,Smart contracts,2023-07-02T12:05:43Z,2023-07-02T12:05:43Z,"Abusing the Ethereum Smart Contract Verification Services for Fun and
  Profit","  Smart contracts play a vital role in the Ethereum ecosystem. Due to the
prevalence of kinds of security issues in smart contracts, the smart contract
verification is urgently needed, which is the process of matching a smart
contract's source code to its on-chain bytecode for gaining mutual trust
between smart contract developers and users. Although smart contract
verification services are embedded in both popular Ethereum browsers (e.g.,
Etherscan and Blockscout) and official platforms (i.e., Sourcify), and gain
great popularity in the ecosystem, their security and trustworthiness remain
unclear. To fill the void, we present the first comprehensive security analysis
of smart contract verification services in the wild. By diving into the
detailed workflow of existing verifiers, we have summarized the key security
properties that should be met, and observed eight types of vulnerabilities that
can break the verification. Further, we propose a series of detection and
exploitation methods to reveal the presence of vulnerabilities in the most
popular services, and uncover 19 exploitable vulnerabilities in total. All the
studied smart contract verification services can be abused to help spread
malicious smart contracts, and we have already observed the presence of using
this kind of tricks for scamming by attackers. It is hence urgent for our
community to take actions to detect and mitigate security issues related to
smart contract verification, a key component of the Ethereum smart contract
ecosystem.
","['Pengxiang Ma', 'Ningyu He', 'Yuhua Huang', 'Haoyu Wang', 'Xiapu Luo']"
http://arxiv.org/abs/1710.06372v1,Smart contracts,2017-10-17T16:39:23Z,2017-10-17T16:39:23Z,Blockchain-based Smart Contracts: A Systematic Mapping Study,"  An appealing feature of blockchain technology is smart contracts. A smart
contract is executable code that runs on top of the blockchain to facilitate,
execute and enforce an agreement between untrusted parties without the
involvement of a trusted third party. In this paper, we conduct a systematic
mapping study to collect all research that is relevant to smart contracts from
a technical perspective. The aim of doing so is to identify current research
topics and open challenges for future studies in smart contract research. We
extract 24 papers from different scientific databases. The results show that
about two thirds of the papers focus on identifying and tackling smart contract
issues. Four key issues are identified, namely, codifying, security, privacy
and performance issues. The rest of the papers focuses on smart contract
applications or other smart contract related topics. Research gaps that need to
be addressed in future studies are provided.
","['Maher Alharby', 'Aad van Moorsel']"
http://arxiv.org/abs/1908.03707v1,Smart contracts,2019-08-10T07:50:33Z,2019-08-10T07:50:33Z,Mutation Testing for Ethereum Smart Contract,"  Smart contract is a special program that manages digital assets on
blockchain. It is difficult to recover the loss if users make transactions
through buggy smart contracts, which cannot be directly fixed. Hence, it is
important to ensure the correctness of smart contracts before deploying them.
This paper proposes a systematic framework to mutation testing for smart
contracts on Ethereum, which is currently the most popular open blockchain for
deploying and running smart contracts. Fifteen novel mutation operators have
been designed for Ethereum Smart Contracts (ESC), in terms of keyword, global
variable/function, variable unit, and error handling. An empirical study on 26
smart contracts in four Ethereum DApps has been conducted to evaluate the
effectiveness of mutation testing. The experimental results show that our
approach can outperform the coverage-based approach on defect detection rate
(96.01% vs. 55.68%). The ESC mutation operators are effective to reveal real
defects and we found 117 out of 729 real bug reports are related to our
operators. These show the great potential of using mutation testing for quality
assurance of ESC.
","['Haoran Wu', 'Xingya Wang', 'Jiehui Xu', 'Weiqin Zou', 'Lingming Zhang', 'Zhenyu Chen']"
http://arxiv.org/abs/2007.13115v1,Stem-cell therapy,2020-07-26T12:22:16Z,2020-07-26T12:22:16Z,"Challenges in constructing genetic instruments for pharmacologic
  therapies","  The genes that encode the targets of most therapies do not have rare variants
with large-effect or common variants with moderate effects on the biomarker
reflecting the pharmacologic action of the corresponding therapy. Therefore,
providing genetic target validation for most therapies is challenging. Novel
methods are being developed to combine multiple variants in the gene encoding
the target of a therapy that are weakly associated with the biomarker
reflecting the pharmacologic action of that therapy into a genetic score that
can be used as an adequate instrumental variable. We describe one approach to
solve this important problem.
","['B. A. Ference', 'G. Davey Smith', 'M. V. Holmes', 'A. L. Catapano', 'K. K. Ray', 'S. J. Nicholls']"
http://arxiv.org/abs/1811.06262v3,Stem-cell therapy,2018-11-15T10:00:11Z,2019-10-09T11:47:59Z,"In Silico Implementation of Evolutionary Paradigm in Therapy Design:
  Towards Anti-Cancer Therapy as Darwinian Process","  In here presented in silico study we suggest a way how to implement the
evolutionary principles into anti-cancer therapy design. We hypothesize that
instead of its ongoing supervised adaptation, the therapy may be constructed as
a self-sustaining evolutionary process in a dynamic fitness landscape
established implicitly by evolving cancer cells, microenvironment and the
therapy itself. For these purposes, we replace a unified therapy with the
`therapy species', which is a population of heterogeneous elementary therapies,
and propose a way how to turn the toxicity of the elementary therapy into its
fitness in a way conforming to evolutionary causation. As a result, not only
the therapies govern the evolution of different cell phenotypes, but the cells'
resistances govern the evolution of the therapies as well. We illustrate the
approach by the minimalistic ad hoc evolutionary model. Its results indicate
that the resistant cells could bias the evolution towards more toxic elementary
therapies by inhibiting the less toxic ones. As the evolutionary causation of
cancer drug resistance has been intensively studied for a few decades, we refer
to cancer as a special case to illustrate purely theoretical analysis.
","['Branislav Brutovsky', 'Denis Horvath']"
http://arxiv.org/abs/2411.16362v2,Stem-cell therapy,2024-11-25T13:15:31Z,2024-12-04T16:30:46Z,"Optimal switching strategies in multi-drug therapies for chronic
  diseases","  Antimicrobial resistance is a threat to public health with millions of deaths
linked to drug resistant infections every year. To mitigate resistance, common
strategies that are used are combination therapies and therapy switching.
However, the stochastic nature of pathogenic mutation makes the optimization of
these strategies challenging. Here, we propose a two-scale stochastic model
that considers the effective evolution of therapies in a multidimensional
efficacy space, where each dimension represents the efficacy of a specific drug
in the therapy. The diffusion of therapies within this space is subject to
stochastic resets, representing therapy switches. The boundaries of the space,
inferred from coarser pathogen-host dynamics, can be either reflecting or
absorbing. Reflecting boundaries impede full recovery of the host, while
absorbing boundaries represent the development of antimicrobial resistance,
leading to therapy failure. We derive analytical expressions for the average
absorption times, accounting for both continuous and discrete genomic changes
using the frameworks of Langevin and Master equations, respectively. These
expressions allow us to evaluate the relevance of times between drug-switches
and the number of simultaneous drugs in relation to typical timescales for drug
resistance development. We also explore realistic scenarios where therapy
constraints are imposed to the number of administered therapies and/or their
costs, finding non-trivial optimal drug-switching protocols that maximize the
time before antimicrobial resistance develops while reducing therapy costs.
","['Juan Magalang', 'Javier Aguilar', 'Jose Perico Esguerra', 'Édgar Roldán', 'Daniel Sanchez-Taltavull']"
http://arxiv.org/abs/2102.03061v1,Stem-cell therapy,2021-02-05T08:54:38Z,2021-02-05T08:54:38Z,Applications of Artificial Intelligence in Particle Radiotherapy,"  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
","['Chao Wu', 'Dan Nguyen', 'Jan Schuemann', 'Andrea Mairani', 'Yuehu Pu', 'Steve Jiang']"
http://arxiv.org/abs/2204.05877v1,Stem-cell therapy,2022-04-12T15:14:12Z,2022-04-12T15:14:12Z,Computational model for tumor response to adoptive cell transfer therapy,"  One of the barriers to the development of effective adoptive cell transfer
therapies (ACT), specifically for genetically engineered T-cell receptors
(TCRs), and chimeric antigen receptor (CAR) T-cells, is target antigen
heterogeneity. It is thought that intratumor heterogeneity is one of the
leading determinants of therapeutic resistance and treatment failure. While
understanding antigen heterogeneity is important for effective therapeutics, a
good therapy strategy could enhance the therapy efficiency. In this work we
introduce an agent-based model to rationalize the outcomes of two types of ACT
therapies over heterogeneous tumors: antigen specific ACT therapy and
multi-antigen recognition ACT therapy. We found that one dose of antigen
specific ACT therapy should be expected to reduce the tumor size as well as its
growth rate, however it may not be enough to completely eliminate it. A second
dose also reduced the tumor size as well as the tumor growth rate, but, due to
the intratumor heterogeneity, it turned out to be less effective than the
previous dose. Moreover, an interesting emergent phenomenon results from the
simulations, namely the formation of a shield-like structure of cells with low
oncoprotein expression. This shield turns out to protect cells with high
oncoprotein expression. On the other hand, our studies suggest that the earlier
the multi-antigen recognition ACT therapy is applied, the more efficient it
turns. In fact, it could completely eliminate the tumor. Based on our results,
it is clear that a proper therapeutic strategy could enhance the therapies
outcomes. In that direction, our computational approach provides a framework to
model treatment combinations in different scenarios and explore the
characteristics of successful and unsuccessful treatments.
","['Luciana Melina Luque', 'Carlos Manuel Carlevaro', 'Enrique Rodríguez-Lomba', 'Enrique Lomba']"
http://arxiv.org/abs/2412.06600v2,Stem-cell therapy,2024-12-09T15:49:18Z,2024-12-12T05:15:09Z,"Advancing Music Therapy: Integrating Eastern Five-Element Music Theory
  and Western Techniques with AI in the Novel Five-Element Harmony System","  In traditional medical practices, music therapy has proven effective in
treating various psychological and physiological ailments. Particularly in
Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in
traditional Chinese medicine, possesses profound cultural significance and
unique therapeutic philosophies. With the rapid advancement of Information
Technology and Artificial Intelligence, applying these modern technologies to
FEMT could enhance the personalization and cultural relevance of the therapy
and potentially improve therapeutic outcomes. In this article, we developed a
music therapy system for the first time by applying the theory of the five
elements in music therapy to practice. This innovative approach integrates
advanced Information Technology and Artificial Intelligence with Five-Element
Music Therapy (FEMT) to enhance personalized music therapy practices. As
traditional music therapy predominantly follows Western methodologies, the
unique aspects of Eastern practices, specifically the Five-Element theory from
traditional Chinese medicine, should be considered. This system aims to bridge
this gap by utilizing computational technologies to provide a more
personalized, culturally relevant, and therapeutically effective music therapy
experience.
","['Yubo Zhou', 'Weizhen Bian', 'Kaitai Zhang', 'Xiaohan Gu']"
http://arxiv.org/abs/q-bio/0608028v3,Stem-cell therapy,2006-08-15T14:04:22Z,2013-05-02T22:25:31Z,"Incubation periods under various anti-retroviral therapies in
  homogeneous mixing and age-structured dynamical models: A theoretical
  approach","  With the launch of second line anti-retroviral therapy for HIV infected
individuals, there has been an increased expectation on surviving period of
people with HIV. We consider previously well-known models in HIV epidemiology
where the parameter for incubation period is used as one of the important
components to explain the dynamics of the variables. Such models are extended
here to explain the dynamics with respect to a given therapy that prolongs life
of an HIV infected individual. A deconvolution method is demonstrated for
estimation of parameters in the situations when no-therapy and multiple
therapies are given to the infected population. The models and deconvolution
method are extended in order to study the impact of therapy in age-structured
populations. A generalization for a situation when n-types of therapies are
available is given. Models are demonstrated using hypothetical data and
sensitivity of the parameters are also computed.
",['Arni S. R. Srinivasa Rao']
http://arxiv.org/abs/1504.07642v1,Stem-cell therapy,2015-04-28T20:05:31Z,2015-04-28T20:05:31Z,"Infinitesimal Perturbation Analysis for Personalized Cancer Therapy
  Design","  We use a Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution
under intermittent androgen suppression (IAS) to study a threshold-based policy
for therapy design. IAS is currently one of the most widely used treatments for
advanced prostate cancer. Patients undergoing IAS are submitted to cycles of
treatment (in the form of androgen deprivation) and off-treatment periods in an
alternating manner. One of the main challenges in IAS is to optimally design a
therapy scheme, i.e., to determine when to discontinue and recommence androgen
suppression. The level of prostate specific antigen (PSA) in a patient's serum
is frequently monitored to determine when the patient will be taken off therapy
and when therapy will resume. The threshold-based policy we propose is
parameterized by lower and upper PSA threshold values and is associated with a
cost metric that combines clinically relevant measures of therapy success.
Using Infinitesimal Perturbation Analysis (IPA), we derive unbiased gradient
estimators of this cost metric with respect to the controllable PSA threshold
values based on actual data and show how these estimators can be used to
adaptively adjust controllable parameters so as to improve therapy outcomes
based on the cost metric defined.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1409.1928v1,Stem-cell therapy,2014-09-05T20:00:17Z,2014-09-05T20:00:17Z,Neutron Therapy in the 21st Century,"  The question of whether or not neutron therapy works has been answered. It is
a qualified yes, as is the case with all of radiation therapy. But, neutron
therapy has not kept pace with the rest of radiation therapy in terms of beam
delivery techniques. Modern photon and proton based external beam radiotherapy
routinely implements image-guidance, beam intensity-modulation and
3-dimensional treatment planning. The current iteration of fast neutron
radiotherapy does not. Addressing these deficiencies, however, is not a matter
of technology or understanding, but resources. The future of neutron therapy
lies in better understanding the interaction processes of radiation with living
tissue. A combination of radiobiology and computer simulations is required in
order to optimize the use of neutron therapy. The questions that need to be
answered are: Can we connect the macroscopic with the microscopic? What is the
optimum energy? What is the optimum energy spectrum? Can we map the sensitivity
of the various tissues of the human body and use that knowledge to our
advantage? And once we gain a better understanding of the above radiobiological
issues will we be able to capitalize on this understanding by precisely and
accurately delivering fast neutrons in a manner comparable to what is now
possible with photons and protons? This presentation will review the
accomplishments to date. It will then lay out the questions that need to be
answered for neutron therapy to truly be a 21st Century therapy.
","['Thomas K. Kroc', 'James S. Welsh']"
http://arxiv.org/abs/1602.02077v1,Stem-cell therapy,2016-02-05T16:03:43Z,2016-02-05T16:03:43Z,Cancer and electromagnetic radiation therapy: Quo Vadis?,"  In oncology, treating cancer with a beam of photons is a well established
therapeutic technique, developed over 100 years, and today over 50% of cancer
patients will undergo traditional X-ray radiotherapy. However, ionizing
radiation therapy is not the only option, as the high-energy photons delivering
their cell-killing radiation energy into cancerous tumor can lead to
significant damage to healthy tissues surrounding the tumor, located throughout
the beam's path. Therefore, in nowadays, advances in ionizing radiation therapy
are competitive to non-ionizing ones, as for example the laser light based
therapy, resulting in a synergism that has revolutionized medicine. The use of
non-invasive or minimally invasive (e.g. through flexible endoscopes)
therapeutic procedures in the management of patients represents a very
interesting treatment option. Moreover, as the major breakthrough in cancer
management is the individualized patient treatment, new biophotonic techniques,
e.g. photo-activated drug carriers, help the improvement of treatment efficacy
and/or normal tissue toxicity. Additionally, recent studies support that laser
technology progresses could revolutionize cancer proton therapy, by reducing
the cost of the needed installations. The aim of this review is to present some
laser-based future objectives for cancer radiation therapy, aiming to address
the relevant advances in the ionizing and non-ionizing radiation therapy, i.e.
protons and heavy ions therapy, as well as photodynamic targeted and molecular
therapies.
",['Mersini Makropoulou']
http://arxiv.org/abs/1603.00895v1,Stem-cell therapy,2016-03-02T21:24:40Z,2016-03-02T21:24:40Z,Personalized Cancer Therapy Design: Robustness vs. Optimality,"  Intermittent Androgen Suppression (IAS) is a treatment strategy for delaying
or even preventing time to relapse of advanced prostate cancer. IAS consists of
alternating cycles of therapy (in the form of androgen suppression) and
off-treatment periods. The level of prostate specific antigen (PSA) in a
patient's serum is frequently monitored to determine when the patient will be
taken off therapy and when therapy will resume. In spite of extensive recent
clinical experience with IAS, the design of an ideal protocol for any given
patient remains one of the main challenges associated with effectively
implementing this therapy. We use a threshold-based policy for optimal IAS
therapy design that is parameterized by lower and upper PSA threshold values
and is associated with a cost metric that combines clinically relevant measures
of therapy success. We apply Infinitesimal Perturbation Analysis (IPA) to a
Stochastic Hybrid Automaton (SHA) model of prostate cancer evolution under IAS
and derive unbiased estimators of the cost metric gradient with respect to
various model and therapy parameters. These estimators are subsequently used
for system analysis. By evaluating sensitivity estimates with respect to
several model parameters, we identify critical parameters and demonstrate that
relaxing the optimality condition in favor of increased robustness to modeling
errors provides an alternative objective to therapy design for at least some
patients.
","['Julia L. Fleck', 'Christos G. Cassandras']"
http://arxiv.org/abs/1902.00728v1,Stem-cell therapy,2019-02-02T14:34:13Z,2019-02-02T14:34:13Z,"New combinational therapies for cancer using modern statistical
  mechanics","  We investigate a new dynamical system that describes tumor-host interaction.
The equation that describes the untreated tumor growth is based on
non-extensive statistical mechanics. Recently, this model has been shown to fit
successfully exponential, Gompertz, logistic, and power-law tumor growths. We
have been able to include as many hallmarks of cancer as possible. We study
also the dynamic response of cancer under therapy. Using our model, we can make
predictions about the different outcomes when we change the parameters, and/or
the initial conditions. We can determine the importance of different factors to
influence tumor growth. We discover synergistic therapeutic effects of
different treatments and drugs. Cancer is generally untreatable using
conventional monotherapy. We consider conventional therapies, oncogene-targeted
therapies, tumor-suppressors gene-targeted therapies, immunotherapies,
anti-angiogenesis therapies, virotherapy, among others. We need therapies with
the potential to target both tumor cells and the tumors' microenvironment.
Drugs that target oncogenes and tumor-suppressor genes can be effective in the
treatment of some cancers. However, most tumors do reoccur. We have found that
the success of the new therapeutic agents can be seen when used in combination
with other cancer-cell-killing therapies. Our results have allowed us to design
a combinational therapy that can lead to the complete eradication of cancer.
","['Jorge A. González', 'M. Acanda', 'Z. Akhtar', 'D. Andrews', 'J. I. Azqueta', 'E. Bass', 'A. Bellorín', 'J. Couso', 'Mónica A. García-Ñustes', 'Y. Infante', 'S. Jiménez', 'L. Lester', 'L. Maldonado', 'Juan F. Marín', 'L. Pineda', 'I. Rodríguez', 'C. C. Tamayo', 'D. Valdes', 'L. Vázquez']"
http://arxiv.org/abs/2203.05383v2,Stem-cell therapy,2022-03-10T14:17:07Z,2022-06-16T11:29:06Z,"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset
  of Stuttering","  Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
","['Sebastian P. Bayerl', 'Alexander Wolff von Gudenberg', 'Florian Hönig', 'Elmar Nöth', 'Korbinian Riedhammer']"
http://arxiv.org/abs/2112.07717v1,Stem-cell therapy,2021-12-14T19:37:20Z,2021-12-14T19:37:20Z,"Deterministic and Stochastic in-host Tuberculosis Models for
  Bacterium-directed and Host-directed Therapy Combination","  Mycobacterium tuberculosis infection can involve all immune system components
and can result in different disease outcomes. The antibiotic TB drugs require
strict adherence to prevent both disease relapse and mutation of drug- and
multidrug-resistant strains. To overcome the constraints of pathogen-directed
therapy, host-directed therapy has attracted more attention in recent years as
an adjunct therapy to enhance host immunity to fight against this intractable
pathogen. The goal of this paper is to investigate in-host tuberculosis models
to provide insights into therapy development. Focusing on therapy-targeting
parameters, the parameter regions for different disease outcomes are identified
from an established ODE model. Interestingly, the ODE model also demonstrates
that the immune responses can both benefit and impede disease progression,
depending on the number of bacteria engulfed and released by macrophages. We
then develop two It\^{o} SDE models, which consider the impact of demographic
variations at the cellular level and environmental variations during therapies
along with demographic variations. The SDE model with demographic variation
suggests that stochastic fluctuations at the cellular level have significant
influences on (1) the T-cell population in all parameter regions, (2) the
bacterial population when parameters located in the region with multiple
disease outcomes, and (3) the uninfected macrophage population in the parameter
region representing active disease. Further, considering environmental
variations from therapies, the second SDE model suggests that disease
progression can slow down if therapies (1) can have fast return rates and (2)
can bring parameter values into the disease clearance regions.
",['Wenjing Zhang']
http://arxiv.org/abs/2404.10310v1,Stem-cell therapy,2024-04-16T06:37:19Z,2024-04-16T06:37:19Z,"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A
  Deep Learning Approach","  Several therapy routines require deep breathing exercises as a key component
and patients undergoing such therapies must perform these exercises regularly.
Assessing the outcome of a therapy and tailoring its course necessitates
monitoring a patient's compliance with the therapy. While therapy compliance
monitoring is routine in a clinical environment, it is challenging to do in an
at-home setting. This is so because a home setting lacks access to specialized
equipment and skilled professionals needed to effectively monitor the
performance of a therapy routine by a patient. For some types of therapies,
these challenges can be addressed with the use of consumer-grade hardware, such
as earphones and smartphones, as practical solutions. To accurately monitor
breathing exercises using wireless earphones, this paper proposes a framework
that has the potential for assessing a patient's compliance with an at-home
therapy. The proposed system performs real-time detection of breathing phases
and channels with high accuracy by processing a $\mathbf{500}$ ms audio signal
through two convolutional neural networks. The first network, called a channel
classifier, distinguishes between nasal and oral breathing, and a pause. The
second network, called a phase classifier, determines whether the audio segment
is from inhalation or exhalation. According to $k$-fold cross-validation, the
channel and phase classifiers achieved a maximum F1 score of $\mathbf{97.99\%}$
and $\mathbf{89.46\%}$, respectively. The results demonstrate the potential of
using commodity earphones for real-time breathing channel and phase detection
for breathing therapy compliance monitoring.
","['Hassam Khan Wazir', 'Zaid Waghoo', 'Vikram Kapila']"
http://arxiv.org/abs/2410.18329v1,Stem-cell therapy,2024-10-23T23:51:53Z,2024-10-23T23:51:53Z,"When Group Spirit Meets Personal Journeys: Exploring Motivational
  Dynamics and Design Opportunities in Group Therapy","  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in
treating various mental disorders. Technology-facilitated mental health therapy
improves client engagement through methods like digitization or gamification.
However, these innovations largely cater to individual therapy, ignoring the
potential of group therapy-a treatment for multiple clients concurrently, which
enables individual clients to receive various perspectives in the treatment
process and also addresses the scarcity of healthcare practitioners to reduce
costs. Notwithstanding its cost-effectiveness and unique social dynamics that
foster peer learning and community support, group therapy, such as group CBT,
faces the issue of attrition. While existing medical work has developed
guidelines for therapists, such as establishing leadership and empathy to
facilitate group therapy, understanding about the interactions between each
stakeholder is still missing. To bridge this gap, this study examined a group
CBT program called the Serigaya Methamphetamine Relapse Prevention Program
(SMARPP) as a case study to understand stakeholder coordination and
communication, along with factors promoting and hindering continuous engagement
in group therapy. In-depth interviews with eight facilitators and six former
clients from SMARPP revealed the motivators and demotivators for
facilitator-facilitator, client-client, and facilitator-client communications.
Our investigation uncovers the presence of discernible conflicts between
clients' intrapersonal motivation as well as interpersonal motivation in the
context of group therapy through the lens of self-determination theory. We
discuss insights and research opportunities for the HCI community to mediate
such tension and enhance stakeholder communication in future
technology-assisted group therapy settings.
","['Shixian Geng', 'Ginshi Shimojima', 'Chi-Lan Yang', 'Zefan Sramek', 'Shunpei Norihama', 'Ayumi Takano', 'Simo Hosio', 'Koji Yatani']"
http://arxiv.org/abs/1804.08990v1,Stem-cell therapy,2018-04-24T12:36:09Z,2018-04-24T12:36:09Z,Therapy Control and Patient Safety for Proton Therapy,"  This contribution describes general concepts for control and safety systems
in proton therapy. These concepts are illustrated by concrete examples
implemented in the Proscan facility at PSI.
",['Martin Grossmann']
http://arxiv.org/abs/2011.00285v1,Stem-cell therapy,2020-10-31T14:33:16Z,2020-10-31T14:33:16Z,"On The Relationship Between The Energy, Energy Spread And Distal Slope
  for Proton Therapy Observed in GEANT4","  In proton therapy both the energy, which determines the range, and the distal
slope, which reflects the rate at which the protons decelerate, are of import
if we are to ensure accurate dose deposition and maximum tissue sparing. This
publication describes a Geant4 model and presents a two-dimensional polynomial
relationship between energy, the energy spread and the distal slope for beams
with Gaussian energy spectra for proton therapy. This simple polynomial
relationship will be useful for non-invasive or minimally invasive near
real-time monitoring of the energy and energy spread of a proton therapy beam.
","['Tim Fulcher', 'Richard A Amos', 'Hywel Owen', 'Rob Edgecock']"
http://arxiv.org/abs/1812.04900v1,Stem-cell therapy,2018-12-12T11:18:19Z,2018-12-12T11:18:19Z,"Model of a Data Mining System for Personalized Therapy of Speech
  Disorders","  Lately, the children with speech disorder have more and more become object of
specialists attention and investment in speech disorder therapy are increasing
The development and use of information technology in order to assist and follow
speech disorder therapy allowed researchers to collect a considerable volume of
data. The aim of this paper is to present a data mining system designed to be
associated with TERAPERS system in order to provide information based on which
one could improve the process of personalized therapy of speech disorders.
","['Mirela Danubianu', 'Stefan Gheorghe Pentiuc', 'Iolanda Tobolcea', 'Tiberiu Socaciu']"
http://arxiv.org/abs/2209.03812v1,Stem-cell therapy,2022-09-08T13:32:30Z,2022-09-08T13:32:30Z,"Optimal personalized therapies in colon-cancer induced immune response
  using a Fokker-Planck framework","  In this paper, a new stochastic framework to determine optimal combination
therapies in colon cancer-induced immune response is presented. The dynamics of
colon cancer is described through an It\""o stochastic process, whose
probability density function evolution is governed by the Fokker-Planck
equation. An open-loop control optimization problem is proposed to determine
the optimal combination therapies. Numerical results with combination therapies
comprising of the chemotherapy drug \ind{Doxorubicin} and immunotherapy drug
IL-2 validate the proposed framework.
","['Souvik Roy', 'Suvra Pal']"
