Index: data/arxiv.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>id,technology,published,updated,title,summary,authors\r\nhttp://arxiv.org/abs/2409.18125v3,3D printing,2024-09-26T17:59:11Z,2025-04-27T06:50:23Z,\"LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with\r\n  3D-awareness\",\"  Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced\r\ntheir proficiency in 2D visual understanding tasks, enabling them to\r\neffectively process and understand images and videos. However, the development\r\nof LMMs with 3D scene understanding capabilities has been hindered by the lack\r\nof large-scale 3D vision-language datasets and powerful 3D encoders. In this\r\npaper, we introduce a simple yet effective framework called LLaVA-3D.\r\nLeveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D\r\nefficiently adapts LLaVA for 3D scene understanding without compromising 2D\r\nunderstanding capabilities. To achieve this, we utilize the 3D position\r\nembeddings to enhance the 2D CLIP Patches with 3D spatial context information\r\nand construct 3D patches. By integrating the 3D position embeddings into 2D\r\nLMMs and employing joint 2D and 3D vision-language instruction tuning, we\r\nestablish a unified architecture for both 2D visual understanding and 3D scene\r\nunderstanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding\r\naccurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from\r\nthese 3D patches, without relying on the time-consuming off-the-shelf 3D\r\nsegmentors. Experimental results show that LLaVA-3D converges 3.5x faster than\r\nexisting 3D LMMs when trained on 3D vision-language datasets. Moreover,\r\nLLaVA-3D not only achieves state-of-the-art performance across various 3D tasks\r\nbut also maintains comparable 2D visual understanding and vision-language\r\nconversation capabilities with LLaVA.\r\n\",\"['Chenming Zhu', 'Tai Wang', 'Wenwei Zhang', 'Jiangmiao Pang', 'Xihui Liu']\"\r\nhttp://arxiv.org/abs/2303.15780v1,3D printing,2023-03-28T07:50:45Z,2023-03-28T07:50:45Z,Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion,\"  We propose a high-quality 3D-to-3D conversion method, Instruct 3D-to-3D. Our\r\nmethod is designed for a novel task, which is to convert a given 3D scene to\r\nanother scene according to text instructions. Instruct 3D-to-3D applies\r\npretrained Image-to-Image diffusion models for 3D-to-3D conversion. This\r\nenables the likelihood maximization of each viewpoint image and high-quality 3D\r\ngeneration. In addition, our proposed method explicitly inputs the source 3D\r\nscene as a condition, which enhances 3D consistency and controllability of how\r\nmuch of the source 3D scene structure is reflected. We also propose dynamic\r\nscaling, which allows the intensity of the geometry transformation to be\r\nadjusted. We performed quantitative and qualitative evaluations and showed that\r\nour proposed method achieves higher quality 3D-to-3D conversions than baseline\r\nmethods.\r\n\",\"['Hiromichi Kamata', 'Yuiko Sakuma', 'Akio Hayakawa', 'Masato Ishii', 'Takuya Narihira']\"\r\nhttp://arxiv.org/abs/2401.13923v2,3D printing,2024-01-25T03:42:00Z,2024-03-17T08:51:45Z,Towards 3D Molecule-Text Interpretation in Language Models,\"  Language Models (LMs) have greatly influenced diverse domains. However, their\r\ninherent limitation in comprehending 3D molecular structures has considerably\r\nconstrained their potential in the biomolecular domain. To bridge this gap, we\r\nfocus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\r\nLanguage Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\r\n3D molecules by equipping the LM with a 3D molecular encoder. This integration\r\nis achieved by a 3D molecule-text projector, bridging the 3D molecular\r\nencoder's representation space and the LM's input space. Moreover, to enhance\r\n3D-MoLM's ability of cross-modal molecular understanding and instruction\r\nfollowing, we meticulously curated a 3D molecule-centric instruction tuning\r\ndataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\r\ninstruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\r\nand LM. It significantly surpasses existing baselines on downstream tasks,\r\nincluding molecule-text retrieval, molecule captioning, and more challenging\r\nopen-text molecular QA tasks, especially focusing on 3D-dependent properties.\r\nWe release our codes and datasets at https://github.com/lsh0520/3D-MoLM.\r\n\",\"['Sihang Li', 'Zhiyuan Liu', 'Yanchen Luo', 'Xiang Wang', 'Xiangnan He', 'Kenji Kawaguchi', 'Tat-Seng Chua', 'Qi Tian']\"\r\nhttp://arxiv.org/abs/2503.24229v1,3D printing,2025-03-31T15:42:10Z,2025-03-31T15:42:10Z,\"Pre-training with 3D Synthetic Data: Learning 3D Point Cloud Instance\r\n  Segmentation from 3D Synthetic Scenes\",\"  In the recent years, the research community has witnessed growing use of 3D\r\npoint cloud data for the high applicability in various real-world applications.\r\nBy means of 3D point cloud, this modality enables to consider the actual size\r\nand spatial understanding. The applied fields include mechanical control of\r\nrobots, vehicles, or other real-world systems. Along this line, we would like\r\nto improve 3D point cloud instance segmentation which has emerged as a\r\nparticularly promising approach for these applications. However, the creation\r\nof 3D point cloud datasets entails enormous costs compared to 2D image\r\ndatasets. To train a model of 3D point cloud instance segmentation, it is\r\nnecessary not only to assign categories but also to provide detailed\r\nannotations for each point in the large-scale 3D space. Meanwhile, the increase\r\nof recent proposals for generative models in 3D domain has spurred proposals\r\nfor using a generative model to create 3D point cloud data. In this work, we\r\npropose a pre-training with 3D synthetic data to train a 3D point cloud\r\ninstance segmentation model based on generative model for 3D scenes represented\r\nby point cloud data. We directly generate 3D point cloud data with Point-E for\r\ninserting a generated data into a 3D scene. More recently in 2025, although\r\nthere are other accurate 3D generation models, even using the Point-E as an\r\nearly 3D generative model can effectively support the pre-training with 3D\r\nsynthetic data. In the experimental section, we compare our pre-training method\r\nwith baseline methods indicated improved performance, demonstrating the\r\nefficacy of 3D generative models for 3D point cloud instance segmentation.\r\n\",\"['Daichi Otsuka', 'Shinichi Mae', 'Ryosuke Yamada', 'Hirokatsu Kataoka']\"\r\nhttp://arxiv.org/abs/2009.09633v1,3D printing,2020-09-21T06:26:39Z,2020-09-21T06:26:39Z,3D-FUTURE: 3D Furniture shape with TextURE,\"  The 3D CAD shapes in current 3D benchmarks are mostly collected from online\r\nmodel repositories. Thus, they typically have insufficient geometric details\r\nand less informative textures, making them less attractive for comprehensive\r\nand subtle research in areas such as high-quality 3D mesh and texture recovery.\r\nThis paper presents 3D Furniture shape with TextURE (3D-FUTURE): a\r\nrichly-annotated and large-scale repository of 3D furniture shapes in the\r\nhousehold scenario. At the time of this technical report, 3D-FUTURE contains\r\n20,240 clean and realistic synthetic images of 5,000 different rooms. There are\r\n9,992 unique detailed 3D instances of furniture with high-resolution textures.\r\nExperienced designers developed the room scenes, and the 3D CAD shapes in the\r\nscene are used for industrial production. Given the well-organized 3D-FUTURE,\r\nwe provide baseline experiments on several widely studied tasks, such as joint\r\n2D instance segmentation and 3D object pose estimation, image-based 3D shape\r\nretrieval, 3D object reconstruction from a single image, and texture recovery\r\nfor 3D shapes, to facilitate related future researches on our database.\r\n\",\"['Huan Fu', 'Rongfei Jia', 'Lin Gao', 'Mingming Gong', 'Binqiang Zhao', 'Steve Maybank', 'Dacheng Tao']\"\r\nhttp://arxiv.org/abs/2401.04099v1,3D printing,2024-01-08T18:56:33Z,2024-01-08T18:56:33Z,AGG: Amortized Generative 3D Gaussians for Single Image to 3D,\"  Given the growing need for automatic 3D content creation pipelines, various\r\n3D representations have been studied to generate 3D objects from a single\r\nimage. Due to its superior rendering efficiency, 3D Gaussian splatting-based\r\nmodels have recently excelled in both 3D reconstruction and generation. 3D\r\nGaussian splatting approaches for image to 3D generation are often\r\noptimization-based, requiring many computationally expensive score-distillation\r\nsteps. To overcome these challenges, we introduce an Amortized Generative 3D\r\nGaussian framework (AGG) that instantly produces 3D Gaussians from a single\r\nimage, eliminating the need for per-instance optimization. Utilizing an\r\nintermediate hybrid representation, AGG decomposes the generation of 3D\r\nGaussian locations and other appearance attributes for joint optimization.\r\nMoreover, we propose a cascaded pipeline that first generates a coarse\r\nrepresentation of the 3D data and later upsamples it with a 3D Gaussian\r\nsuper-resolution module. Our method is evaluated against existing\r\noptimization-based 3D Gaussian frameworks and sampling-based pipelines\r\nutilizing other 3D representations, where AGG showcases competitive generation\r\nabilities both qualitatively and quantitatively while being several orders of\r\nmagnitude faster. Project page: https://ir1d.github.io/AGG/\r\n\",\"['Dejia Xu', 'Ye Yuan', 'Morteza Mardani', 'Sifei Liu', 'Jiaming Song', 'Zhangyang Wang', 'Arash Vahdat']\"\r\nhttp://arxiv.org/abs/2303.09036v2,3D printing,2023-03-16T02:18:41Z,2023-08-07T08:10:55Z,Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation,\"  Generating images with both photorealism and multiview 3D consistency is\r\ncrucial for 3D-aware GANs, yet existing methods struggle to achieve them\r\nsimultaneously. Improving the photorealism via CNN-based 2D super-resolution\r\ncan break the strict 3D consistency, while keeping the 3D consistency by\r\nlearning high-resolution 3D representations for direct rendering often\r\ncompromises image quality. In this paper, we propose a novel learning strategy,\r\nnamely 3D-to-2D imitation, which enables a 3D-aware GAN to generate\r\nhigh-quality images while maintaining their strict 3D consistency, by letting\r\nthe images synthesized by the generator's 3D rendering branch to mimic those\r\ngenerated by its 2D super-resolution branch. We also introduce 3D-aware\r\nconvolutions into the generator for better 3D representation learning, which\r\nfurther improves the image generation quality. With the above strategies, our\r\nmethod reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats,\r\nrespectively, at 512x512 resolution, largely outperforming existing 3D-aware\r\nGANs using direct 3D rendering and coming very close to the previous\r\nstate-of-the-art method that leverages 2D super-resolution. Project website:\r\nhttps://seanchenxy.github.io/Mimic3DWeb.\r\n\",\"['Xingyu Chen', 'Yu Deng', 'Baoyuan Wang']\"\r\nhttp://arxiv.org/abs/2403.12438v1,3D printing,2024-03-19T04:51:38Z,2024-03-19T04:51:38Z,Precise-Physics Driven Text-to-3D Generation,\"  Text-to-3D generation has shown great promise in generating novel 3D content\r\nbased on given text prompts. However, existing generative methods mostly focus\r\non geometric or visual plausibility while ignoring precise physics perception\r\nfor the generated 3D shapes. This greatly hinders the practicality of generated\r\n3D shapes in real-world applications. In this work, we propose Phy3DGen, a\r\nprecise-physics-driven text-to-3D generation method. By analyzing the solid\r\nmechanics of generated 3D shapes, we reveal that the 3D shapes generated by\r\nexisting text-to-3D generation methods are impractical for real-world\r\napplications as the generated 3D shapes do not conform to the laws of physics.\r\nTo this end, we leverage 3D diffusion models to provide 3D shape priors and\r\ndesign a data-driven differentiable physics layer to optimize 3D shape priors\r\nwith solid mechanics. This allows us to optimize geometry efficiently and learn\r\nprecise physics information about 3D shapes at the same time. Experimental\r\nresults demonstrate that our method can consider both geometric plausibility\r\nand precise physics perception, further bridging 3D virtual modeling and\r\nprecise physical worlds.\r\n\",\"['Qingshan Xu', 'Jiao Liu', 'Melvin Wong', 'Caishun Chen', 'Yew-Soon Ong']\"\r\nhttp://arxiv.org/abs/2010.11504v1,3D printing,2020-10-22T07:45:09Z,2020-10-22T07:45:09Z,3D Meta-Registration: Learning to Learn Registration of 3D Point Clouds,\"  Deep learning-based point cloud registration models are often generalized\r\nfrom extensive training over a large volume of data to learn the ability to\r\npredict the desired geometric transformation to register 3D point clouds. In\r\nthis paper, we propose a meta-learning based 3D registration model, named 3D\r\nMeta-Registration, that is capable of rapidly adapting and well generalizing to\r\nnew 3D registration tasks for unseen 3D point clouds. Our 3D Meta-Registration\r\ngains a competitive advantage by training over a variety of 3D registration\r\ntasks, which leads to an optimized model for the best performance on the\r\ndistribution of registration tasks including potentially unseen tasks.\r\nSpecifically, the proposed 3D Meta-Registration model consists of two modules:\r\n3D registration learner and 3D registration meta-learner. During the training,\r\nthe 3D registration learner is trained to complete a specific registration task\r\naiming to determine the desired geometric transformation that aligns the source\r\npoint cloud with the target one. In the meantime, the 3D registration\r\nmeta-learner is trained to provide the optimal parameters to update the 3D\r\nregistration learner based on the learned task distribution. After training,\r\nthe 3D registration meta-learner, which is learned with the optimized coverage\r\nof distribution of 3D registration tasks, is able to dynamically update 3D\r\nregistration learners with desired parameters to rapidly adapt to new\r\nregistration tasks. We tested our model on synthesized dataset ModelNet and\r\nFlyingThings3D, as well as real-world dataset KITTI. Experimental results\r\ndemonstrate that 3D Meta-Registration achieves superior performance over other\r\nprevious techniques (e.g. FlowNet3D).\r\n\",\"['Lingjing Wang', 'Yu Hao', 'Xiang Li', 'Yi Fang']\"\r\nhttp://arxiv.org/abs/2502.08503v1,3D printing,2025-02-12T15:34:45Z,2025-02-12T15:34:45Z,Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?,\"  In this work, we identify the \"\"2D-Cheating\"\" problem in 3D LLM evaluation,\r\nwhere these tasks might be easily solved by VLMs with rendered images of point\r\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\r\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\r\nreference, propose principles for better assessing genuine 3D understanding. We\r\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\r\nevaluating 3D LLMs.\r\n\",\"['Jiahe Jin', 'Yanheng He', 'Mingyan Yang']\"\r\nhttp://arxiv.org/abs/1901.08373v2,3D printing,2019-01-24T12:11:05Z,2019-09-14T08:12:30Z,\"Three-dimensional Backbone Network for 3D Object Detection in Traffic\r\n  Scenes\",\"  The task of detecting 3D objects in traffic scenes has a pivotal role in many\r\nreal-world applications. However, the performance of 3D object detection is\r\nlower than that of 2D object detection due to the lack of powerful 3D feature\r\nextraction methods. To address this issue, this study proposes a 3D backbone\r\nnetwork to acquire comprehensive 3D feature maps for 3D object detection. It\r\nprimarily consists of sparse 3D convolutional neural network operations in the\r\npoint cloud. The 3D backbone network can inherently learn 3D features from the\r\nraw data without compressing the point cloud into multiple 2D images. The\r\nsparse 3D convolutional neural network takes full advantage of the sparsity in\r\nthe 3D point cloud to accelerate computation and save memory, which makes the\r\n3D backbone network feasible in a real-world application. Empirical experiments\r\nwere conducted on the KITTI benchmark and comparable results were obtained with\r\nrespect to the state-of-the-art performance for 3D object detection.\r\n\",\"['Xuesong Li', 'Jose Guivant', 'Ngaiming Kwok', 'Yongzhi Xu', 'Ruowei Li', 'Hongkun Wu']\"\r\nhttp://arxiv.org/abs/2011.11534v4,3D printing,2020-11-23T16:48:35Z,2022-04-19T05:59:10Z,Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation,\"  Whole-body 3D human mesh estimation aims to reconstruct the 3D human body,\r\nhands, and face simultaneously. Although several methods have been proposed,\r\naccurate prediction of 3D hands, which consist of 3D wrist and fingers, still\r\nremains challenging due to two reasons. First, the human kinematic chain has\r\nnot been carefully considered when predicting the 3D wrists. Second, previous\r\nworks utilize body features for the 3D fingers, where the body feature barely\r\ncontains finger information. To resolve the limitations, we present Hand4Whole,\r\nwhich has two strong points over previous works. First, we design Pose2Pose, a\r\nmodule that utilizes joint features for 3D joint rotations. Using Pose2Pose,\r\nHand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints\r\nlargely contribute to 3D wrist rotations in the human kinematic chain. Second,\r\nHand4Whole discards the body feature when predicting 3D finger rotations. Our\r\nHand4Whole is trained in an end-to-end manner and produces much better 3D hand\r\nresults than previous whole-body 3D human mesh estimation methods. The codes\r\nare available here at https://github.com/mks0601/Hand4Whole_RELEASE.\r\n\",\"['Gyeongsik Moon', 'Hongsuk Choi', 'Kyoung Mu Lee']\"\r\nhttp://arxiv.org/abs/2111.03098v1,3D printing,2021-11-04T18:30:37Z,2021-11-04T18:30:37Z,\"Voxel-based 3D Detection and Reconstruction of Multiple Objects from a\r\n  Single Image\",\"  Inferring 3D locations and shapes of multiple objects from a single 2D image\r\nis a long-standing objective of computer vision. Most of the existing works\r\neither predict one of these 3D properties or focus on solving both for a single\r\nobject. One fundamental challenge lies in how to learn an effective\r\nrepresentation of the image that is well-suited for 3D detection and\r\nreconstruction. In this work, we propose to learn a regular grid of 3D voxel\r\nfeatures from the input image which is aligned with 3D scene space via a 3D\r\nfeature lifting operator. Based on the 3D voxel features, our novel\r\nCenterNet-3D detection head formulates the 3D detection as keypoint detection\r\nin the 3D space. Moreover, we devise an efficient coarse-to-fine reconstruction\r\nmodule, including coarse-level voxelization and a novel local PCA-SDF shape\r\nrepresentation, which enables fine detail reconstruction and one order of\r\nmagnitude faster inference than prior methods. With complementary supervision\r\nfrom both 3D detection and reconstruction, one enables the 3D voxel features to\r\nbe geometry and context preserving, benefiting both tasks.The effectiveness of\r\nour approach is demonstrated through 3D detection and reconstruction in single\r\nobject and multiple object scenarios.\r\n\",\"['Feng Liu', 'Xiaoming Liu']\"\r\nhttp://arxiv.org/abs/2206.09221v1,3D printing,2022-06-18T15:21:24Z,2022-06-18T15:21:24Z,\"3D Face Parsing via Surface Parameterization and 2D Semantic\r\n  Segmentation Network\",\"  Face parsing assigns pixel-wise semantic labels as the face representation\r\nfor computers, which is the fundamental part of many advanced face\r\ntechnologies. Compared with 2D face parsing, 3D face parsing shows more\r\npotential to achieve better performance and further application, but it is\r\nstill challenging due to 3D mesh data computation. Recent works introduced\r\ndifferent methods for 3D surface segmentation, while the performance is still\r\nlimited. In this paper, we propose a method based on the \"\"3D-2D-3D\"\" strategy to\r\naccomplish 3D face parsing. The topological disk-like 2D face image containing\r\nspatial and textural information is transformed from the sampled 3D face data\r\nthrough the face parameterization algorithm, and a specific 2D network called\r\nCPFNet is proposed to achieve the semantic segmentation of the 2D parameterized\r\nface data with multi-scale technologies and feature aggregation. The 2D\r\nsemantic result is then inversely re-mapped to 3D face data, which finally\r\nachieves the 3D face parsing. Experimental results show that both CPFNet and\r\nthe \"\"3D-2D-3D\"\" strategy accomplish high-quality 3D face parsing and outperform\r\nstate-of-the-art 2D networks as well as 3D methods in both qualitative and\r\nquantitative comparisons.\r\n\",\"['Wenyuan Sun', 'Ping Zhou', 'Yangang Wang', 'Zongpu Yu', 'Jing Jin', 'Guangquan Zhou']\"\r\nhttp://arxiv.org/abs/2211.14091v2,3D printing,2022-11-25T13:21:59Z,2022-12-11T03:35:39Z,Language-Assisted 3D Feature Learning for Semantic Scene Understanding,\"  Learning descriptive 3D features is crucial for understanding 3D scenes with\r\ndiverse objects and complex structures. However, it is usually unknown whether\r\nimportant geometric attributes and scene context obtain enough emphasis in an\r\nend-to-end trained 3D scene understanding network. To guide 3D feature learning\r\ntoward important geometric attributes and scene context, we explore the help of\r\ntextual scene descriptions. Given some free-form descriptions paired with 3D\r\nscenes, we extract the knowledge regarding the object relationships and object\r\nattributes. We then inject the knowledge to 3D feature learning through three\r\nclassification-based auxiliary tasks. This language-assisted training can be\r\ncombined with modern object detection and instance segmentation methods to\r\npromote 3D semantic scene understanding, especially in a label-deficient\r\nregime. Moreover, the 3D feature learned with language assistance is better\r\naligned with the language features, which can benefit various 3D-language\r\nmultimodal tasks. Experiments on several benchmarks of 3D-only and 3D-language\r\ntasks demonstrate the effectiveness of our language-assisted 3D feature\r\nlearning. Code is available at\r\nhttps://github.com/Asterisci/Language-Assisted-3D.\r\n\",\"['Junbo Zhang', 'Guofan Fan', 'Guanghan Wang', 'Zhengyuan Su', 'Kaisheng Ma', 'Li Yi']\"\r\nhttp://arxiv.org/abs/2308.08769v1,3D printing,2023-08-17T03:52:15Z,2023-08-17T03:52:15Z,\"Chat-3D: Data-efficiently Tuning Large Language Model for Universal\r\n  Dialogue of 3D Scenes\",\"  3D scene understanding has gained significant attention due to its wide range\r\nof applications. However, existing methods for 3D scene understanding are\r\nlimited to specific downstream tasks, which hinders their practicality in\r\nreal-world applications. This paper presents Chat-3D, which combines the 3D\r\nvisual perceptual ability of pre-trained 3D representations and the impressive\r\nreasoning and conversation capabilities of advanced LLMs to achieve the first\r\nuniversal dialogue systems for 3D scenes. Specifically, we align 3D\r\nrepresentations into the feature space of LLMs, thus enabling LLMs to perceive\r\nthe 3D world. Given the scarcity of 3D scene-text data, we propose a\r\nthree-stage training strategy to efficiently utilize the available data for\r\nbetter alignment. To enhance the reasoning ability and develop a user-friendly\r\ninteraction scheme, we further construct a high-quality object-centric 3D\r\ninstruction dataset and design an associated object-centric prompt. Our\r\nexperiments show that Chat-3D achieves an impressive ability to comprehend\r\ndiverse instructions for 3D scenes, engage in intricate spatial reasoning, and\r\nincorporate external knowledge into its responses. Chat-3D achieves a 75.6%\r\nrelative score compared with GPT-4 on the constructed instruction dataset.\r\n\",\"['Zehan Wang', 'Haifeng Huang', 'Yang Zhao', 'Ziang Zhang', 'Zhou Zhao']\"\r\nhttp://arxiv.org/abs/2403.15383v2,3D printing,2024-03-22T17:59:01Z,2024-05-15T06:56:30Z,ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars,\"  Real-world applications often require a large gallery of 3D assets that share\r\na consistent theme. While remarkable advances have been made in general 3D\r\ncontent creation from text or image, synthesizing customized 3D assets\r\nfollowing the shared theme of input 3D exemplars remains an open and\r\nchallenging problem. In this work, we present ThemeStation, a novel approach\r\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\r\nassets based on given few exemplars with two goals: 1) unity for generating 3D\r\nassets that thematically align with the given exemplars and 2) diversity for\r\ngenerating 3D assets with a high degree of variations. To this end, we design a\r\ntwo-stage framework that draws a concept image first, followed by a\r\nreference-informed 3D modeling stage. We propose a novel dual score\r\ndistillation (DSD) loss to jointly leverage priors from both the input\r\nexemplars and the synthesized concept image. Extensive experiments and user\r\nstudies confirm that ThemeStation surpasses prior works in producing diverse\r\ntheme-aware 3D models with impressive quality. ThemeStation also enables\r\nvarious applications such as controllable 3D-to-3D generation.\r\n\",\"['Zhenwei Wang', 'Tengfei Wang', 'Gerhard Hancke', 'Ziwei Liu', 'Rynson W. H. Lau']\"\r\nhttp://arxiv.org/abs/2406.05132v3,3D printing,2024-06-07T17:59:59Z,2025-03-20T23:06:14Z,\"3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and\r\n  Less Hallucination\",\"  The integration of language and 3D perception is crucial for embodied agents\r\nand robots that comprehend and interact with the physical world. While large\r\nlanguage models (LLMs) have demonstrated impressive language understanding and\r\ngeneration capabilities, their adaptation to 3D environments (3D-LLMs) remains\r\nin its early stages. A primary challenge is a lack of large-scale datasets with\r\ndense grounding between language and 3D scenes. We introduce 3D-GRAND, a\r\npioneering large-scale dataset comprising 40,087 household scenes paired with\r\n6.2 million densely-grounded scene-language instructions. Our results show that\r\ninstruction tuning with 3D-GRAND significantly enhances grounding capabilities\r\nand reduces hallucinations in 3D-LLMs. As part of our contributions, we propose\r\na comprehensive benchmark 3D-POPE to systematically evaluate hallucination in\r\n3D-LLMs, enabling fair comparisons of models. Our experiments highlight a\r\nscaling effect between dataset size and 3D-LLM performance, emphasizing the\r\nimportance of large-scale 3D-text datasets for embodied AI research. Our\r\nresults demonstrate early signals for effective sim-to-real transfer,\r\nindicating that models trained on large synthetic data can perform well on\r\nreal-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied\r\nAI community with resources and insights to lead to more reliable and\r\nbetter-grounded 3D-LLMs. Project website: https://3d-grand.github.io\r\n\",\"['Jianing Yang', 'Xuweiyi Chen', 'Nikhil Madaan', 'Madhavan Iyengar', 'Shengyi Qian', 'David F. Fouhey', 'Joyce Chai']\"\r\nhttp://arxiv.org/abs/1802.10271v1,3D printing,2018-02-28T06:02:55Z,2018-02-28T06:02:55Z,\"Multimodal Sensor-Based Semantic 3D Mapping for a Large-Scale\r\n  Environment\",\"  Semantic 3D mapping is one of the most important fields in robotics, and has\r\nbeen used in many applications, such as robot navigation, surveillance, and\r\nvirtual reality. In general, semantic 3D mapping is mainly composed of 3D\r\nreconstruction and semantic segmentation. As these technologies evolve, there\r\nhas been great progress in semantic 3D mapping in recent years. Furthermore,\r\nthe number of robotic applications requiring semantic information in 3D mapping\r\nto perform high-level tasks has increased, and many studies on semantic 3D\r\nmapping have been published. Existing methods use a camera for both 3D\r\nreconstruction and semantic segmentation. However, this is not suitable for\r\nlarge-scale environments and has the disadvantage of high computational\r\ncomplexity. To address this problem, we propose a multimodal sensor-based\r\nsemantic 3D mapping system using a 3D Lidar combined with a camera. In this\r\nstudy, we build a 3D map by estimating odometry based on a global positioning\r\nsystem (GPS) and an inertial measurement unit (IMU), and use the latest 2D\r\nconvolutional neural network (CNN) for semantic segmentation. To build a\r\nsemantic 3D map, we integrate the 3D map with semantic information by using\r\ncoordinate transformation and Bayes' update scheme. In order to improve the\r\nsemantic 3D map, we propose a 3D refinement process to correct wrongly\r\nsegmented voxels and remove traces of moving vehicles in the 3D map. Through\r\nexperiments on challenging sequences, we demonstrate that our method\r\noutperforms state-of-the-art methods in terms of accuracy and intersection over\r\nunion (IoU). Thus, our method can be used for various applications that require\r\nsemantic information in 3D map.\r\n\",\"['Jongmin Jeong', 'Tae Sung Yoon', 'Jin Bae Park']\"\r\nhttp://arxiv.org/abs/2209.10020v2,3D printing,2022-09-20T22:04:31Z,2024-02-18T11:59:16Z,Towards 3D VR-Sketch to 3D Shape Retrieval,\"  Growing free online 3D shapes collections dictated research on 3D retrieval.\r\nActive debate has however been had on (i) what the best input modality is to\r\ntrigger retrieval, and (ii) the ultimate usage scenario for such retrieval. In\r\nthis paper, we offer a different perspective towards answering these questions\r\n-- we study the use of 3D sketches as an input modality and advocate a\r\nVR-scenario where retrieval is conducted. Thus, the ultimate vision is that\r\nusers can freely retrieve a 3D model by air-doodling in a VR environment. As a\r\nfirst stab at this new 3D VR-sketch to 3D shape retrieval problem, we make four\r\ncontributions. First, we code a VR utility to collect 3D VR-sketches and\r\nconduct retrieval. Second, we collect the first set of $167$ 3D VR-sketches on\r\ntwo shape categories from ModelNet. Third, we propose a novel approach to\r\ngenerate a synthetic dataset of human-like 3D sketches of different abstract\r\nlevels to train deep networks. At last, we compare the common multi-view and\r\nvolumetric approaches: We show that, in contrast to 3D shape to 3D shape\r\nretrieval, volumetric point-based approaches exhibit superior performance on 3D\r\nsketch to 3D shape retrieval due to the sparse and abstract nature of 3D\r\nVR-sketches. We believe these contributions will collectively serve as enablers\r\nfor future attempts at this problem. The VR interface, code and datasets are\r\navailable at https://tinyurl.com/3DSketch3DV.\r\n\",\"['Ling Luo', 'Yulia Gryaditskaya', 'Yongxin Yang', 'Tao Xiang', 'Yi-Zhe Song']\"\r\nhttp://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,\"Does an artificial intelligence perform market manipulation with its own\r\n  discretion? -- A genetic algorithm learns in an artificial market simulation\",\"  Who should be charged with responsibility for an artificial intelligence\r\nperforming market manipulation have been discussed. In this study, I\r\nconstructed an artificial intelligence using a genetic algorithm that learns in\r\nan artificial market simulation, and investigated whether the artificial\r\nintelligence discovers market manipulation through learning with an artificial\r\nmarket simulation despite a builder of artificial intelligence has no intention\r\nof market manipulation. As a result, the artificial intelligence discovered\r\nmarket manipulation as an optimal investment strategy. This result suggests\r\nnecessity of regulation, such as obligating builders of artificial intelligence\r\nto prevent artificial intelligence from performing market manipulation.\r\n\",['Takanobu Mizuta']\r\nhttp://arxiv.org/abs/2304.02924v1,Artificial intelligence,2023-04-06T08:26:38Z,2023-04-06T08:26:38Z,The Governance of Physical Artificial Intelligence,\"  Physical artificial intelligence can prove to be one of the most important\r\nchallenges of the artificial intelligence. The governance of physical\r\nartificial intelligence would define its responsible intelligent application in\r\nthe society.\r\n\",\"['Yingbo Li', 'Anamaria-Beatrice Spulber', 'Yucong Duan']\"\r\nhttp://arxiv.org/abs/2102.12076v1,Artificial intelligence,2021-02-24T05:43:44Z,2021-02-24T05:43:44Z,\"Perspective: Purposeful Failure in Artificial Life and Artificial\r\n  Intelligence\",\"  Complex systems fail. I argue that failures can be a blueprint characterizing\r\nliving organisms and biological intelligence, a control mechanism to increase\r\ncomplexity in evolutionary simulations, and an alternative to classical fitness\r\noptimization. Imitating biological successes in Artificial Life and Artificial\r\nIntelligence can be misleading; imitating failures offers a path towards\r\nunderstanding and emulating life it in artificial systems.\r\n\",['Lana Sinapayen']\r\nhttp://arxiv.org/abs/1105.1534v1,Artificial intelligence,2011-05-08T16:47:35Z,2011-05-08T16:47:35Z,Taking the redpill: Artificial Evolution in native x86 systems,\"  In analogon to successful artificial evolution simulations as Tierra or\r\navida, this text presents a way to perform artificial evolution in a native x86\r\nsystem. The implementation of the artificial chemistry and first results of\r\nstatistical experiments are presented.\r\n\",['Thomas Sperl']\r\nhttp://arxiv.org/abs/1509.01213v1,Artificial intelligence,2015-07-01T16:26:21Z,2015-07-01T16:26:21Z,Impact of Artificial Intelligence on Economic Theory,\"  Artificial intelligence has impacted many aspects of human life. This paper\r\nstudies the impact of artificial intelligence on economic theory. In particular\r\nwe study the impact of artificial intelligence on the theory of bounded\r\nrationality, efficient market hypothesis and prospect theory.\r\n\",['Tshilidzi Marwala']\r\nhttp://arxiv.org/abs/2101.02179v1,Artificial intelligence,2020-12-27T23:45:03Z,2020-12-27T23:45:03Z,The case for psychometric artificial general intelligence,\"  A short review of the literature on measurement and detection of artificial\r\ngeneral intelligence is made. Proposed benchmarks and tests for artificial\r\ngeneral intelligence are critically evaluated against multiple criteria. Based\r\non the findings, the most promising approaches are identified and some useful\r\ndirections for future work are proposed.\r\n\",['Mark McPherson']\r\nhttp://arxiv.org/abs/0906.2824v1,Artificial intelligence,2009-06-15T23:47:28Z,2009-06-15T23:47:28Z,What Does Artificial Life Tell Us About Death?,\"  Short philosophical essay\r\n\",['Carlos Gershenson']\r\nhttp://arxiv.org/abs/1304.3846v1,Artificial intelligence,2013-04-13T20:44:25Z,2013-04-13T20:44:25Z,\"Proceedings of the Thirteenth Conference on Uncertainty in Artificial\r\n  Intelligence (1997)\",\"  This is the Proceedings of the Thirteenth Conference on Uncertainty in\r\nArtificial Intelligence, which was held in Providence, RI, August 1-3, 1997\r\n\",\"['Dan Geiger', 'Prakash Shenoy']\"\r\nhttp://arxiv.org/abs/1304.3851v1,Artificial intelligence,2013-04-13T21:03:12Z,2013-04-13T21:03:12Z,\"Proceedings of the Ninth Conference on Uncertainty in Artificial\r\n  Intelligence (1993)\",\"  This is the Proceedings of the Ninth Conference on Uncertainty in Artificial\r\nIntelligence, which was held in Washington, DC, July 9-11, 1993\r\n\",\"['David Heckerman', 'E. Mamdani']\"\r\nhttp://arxiv.org/abs/1304.3859v1,Artificial intelligence,2013-04-13T21:37:12Z,2013-04-13T21:37:12Z,\"Proceedings of the Second Conference on Uncertainty in Artificial\r\n  Intelligence (1986)\",\"  This is the Proceedings of the Second Conference on Uncertainty in Artificial\r\nIntelligence, which was held in Philadelphia, PA, August 8-10, 1986\r\n\",\"['Laveen Kanal', 'John Lemmer']\"\r\nhttp://arxiv.org/abs/1311.0716v1,Artificial intelligence,2013-10-30T14:19:49Z,2013-10-30T14:19:49Z,Artificial Intelligence in Humans,\"  In this paper, I put forward that in many instances, thinking mechanisms are\r\nequivalent to artificial intelligence modules programmed into the human mind.\r\n\",['Michael Swan Laufer']\r\nhttp://arxiv.org/abs/1810.06018v1,Artificial intelligence,2018-10-14T11:40:30Z,2018-10-14T11:40:30Z,\"AAAI FSS-18: Artificial Intelligence in Government and Public Sector\r\n  Proceedings\",\"  Proceedings of the AAAI Fall Symposium on Artificial Intelligence in\r\nGovernment and Public Sector, Arlington, Virginia, USA, October 18-20, 2018\r\n\",\"['Frank Stein', 'Alun Preece', 'Mihai Boicu']\"\r\nhttp://arxiv.org/abs/2010.00543v1,Artificial intelligence,2020-10-01T16:57:40Z,2020-10-01T16:57:40Z,\"Artificial Creations: Ascription, Ownership, Time-Specific Monopolies\",\"  Creativity has always been synonymous with humans. No other living species\r\ncould boast of creativity as humans could. Even the smartest computers thrived\r\nonly on the ingenious imaginations of its coders. However, that is steadily\r\nchanging with highly advanced artificially intelligent systems that demonstrate\r\nincredible capabilities to autonomously (i.e., with minimal or no human input)\r\nproduce creative products that would ordinarily deserve intellectual property\r\nstatus if created by a human. These systems could be called artificial creators\r\nand their creative products artificial creations. The use of artificial\r\ncreators is likely to become a part of mainstream production practices in the\r\ncreative and innovation industries sooner than we realize. When they do,\r\nintellectual property regimes (that are inherently designed to reward human\r\ncreativity) must be sufficiently prepared to aptly respond to the phenomenon of\r\nwhat could be called artificial creativity. Needless to say, any such response\r\nmust be guided by considerations of public welfare. This study analyzes what\r\nthat response ought to look like by revisiting the determinants of intellectual\r\nproperty and critiquing its nature and modes. This understanding of\r\nintellectual property is then applied to investigate the determinants of\r\nintellectual property in artificial creations so as to determine the intrinsic\r\njustifications for intellectual property rewards for artificial creativity, and\r\naccordingly, develop general modalities for granting intellectual property\r\nstatus to artificial creations. Finally, the treatment of artificial works\r\n(i.e., copyrightable artificial creations) and artificial inventions (i.e.,\r\npatentable artificial creations) by current intellectual property regimes is\r\ncritiqued, and specific modalities for granting intellectual property status to\r\nartificial works and artificial inventions are developed.\r\n\",['Raj Shekhar']\r\nhttp://arxiv.org/abs/2110.01831v1,Artificial intelligence,2021-10-05T05:58:23Z,2021-10-05T05:58:23Z,\"The Artificial Scientist: Logicist, Emergentist, and Universalist\r\n  Approaches to Artificial General Intelligence\",\"  We attempt to define what is necessary to construct an Artificial Scientist,\r\nexplore and evaluate several approaches to artificial general intelligence\r\n(AGI) which may facilitate this, conclude that a unified or hybrid approach is\r\nnecessary and explore two theories that satisfy this requirement to some\r\ndegree.\r\n\",\"['Michael Timothy Bennett', 'Yoshihiro Maruyama']\"\r\nhttp://arxiv.org/abs/2404.03499v1,Artificial intelligence,2024-04-04T14:57:32Z,2024-04-04T14:57:32Z,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,\"  Artificial Intelligence applications gradually move outside the safe walls of\r\nresearch labs and invade our daily lives. This is also true for Machine\r\nLearning methods on Knowledge Graphs, which has led to a steady increase in\r\ntheir application since the beginning of the 21st century. However, in many\r\napplications, users require an explanation of the Artificial Intelligences\r\ndecision. This led to increased demand for Comprehensible Artificial\r\nIntelligence. Knowledge Graphs epitomize fertile soil for Comprehensible\r\nArtificial Intelligence, due to their ability to display connected data, i.e.\r\nknowledge, in a human- as well as machine-readable way. This survey gives a\r\nshort history to Comprehensible Artificial Intelligence on Knowledge Graphs.\r\nFurthermore, we contribute by arguing that the concept Explainable Artificial\r\nIntelligence is overloaded and overlapping with Interpretable Machine Learning.\r\nBy introducing the parent concept Comprehensible Artificial Intelligence, we\r\nprovide a clear-cut distinction of both concepts while accounting for their\r\nsimilarities. Thus, we provide in this survey a case for Comprehensible\r\nArtificial Intelligence on Knowledge Graphs consisting of Interpretable Machine\r\nLearning on Knowledge Graphs and Explainable Artificial Intelligence on\r\nKnowledge Graphs. This leads to the introduction of a novel taxonomy for\r\nComprehensible Artificial Intelligence on Knowledge Graphs. In addition, a\r\ncomprehensive overview of the research on Comprehensible Artificial\r\nIntelligence on Knowledge Graphs is presented and put into the context of the\r\ntaxonomy. Finally, research gaps in the field of Comprehensible Artificial\r\nIntelligence on Knowledge Graphs are identified for future research.\r\n\",\"['Simon Schramm', 'Christoph Wehner', 'Ute Schmid']\"\r\nhttp://arxiv.org/abs/2104.13155v2,Artificial intelligence,2021-04-27T13:03:25Z,2021-05-07T18:34:10Z,\"Watershed of Artificial Intelligence: Human Intelligence, Machine\r\n  Intelligence, and Biological Intelligence\",\"  This article reviews the \"\"Once learning\"\" mechanism that was proposed 23 years\r\nago and the subsequent successes of \"\"One-shot learning\"\" in image classification\r\nand \"\"You Only Look Once - YOLO\"\" in objective detection. Analyzing the current\r\ndevelopment of Artificial Intelligence (AI), the proposal is that AI should be\r\nclearly divided into the following categories: Artificial Human Intelligence\r\n(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological\r\nIntelligence (ABI), which will also be the main directions of theory and\r\napplication development for AI. As a watershed for the branches of AI, some\r\nclassification standards and methods are discussed: 1) Human-oriented,\r\nmachine-oriented, and biological-oriented AI R&D; 2) Information input\r\nprocessed by Dimensionality-up or Dimensionality-reduction; 3) The use of\r\none/few or large samples for knowledge learning.\r\n\",\"['Li Weigang', 'Liriam Enamoto', 'Denise Leyi Li', 'Geraldo Pereira Rocha Filho']\"\r\nhttp://arxiv.org/abs/2111.11295v1,Artificial intelligence,2021-11-08T00:10:49Z,2021-11-08T00:10:49Z,\"Artificial Intelligence Technology analysis using Artificial\r\n  Intelligence patent through Deep Learning model and vector space model\",\"  Thanks to rapid development of artificial intelligence technology in recent\r\nyears, the current artificial intelligence technology is contributing to many\r\npart of society. Education, environment, medical care, military, tourism,\r\neconomy, politics, etc. are having a very large impact on society as a whole.\r\nFor example, in the field of education, there is an artificial intelligence\r\ntutoring system that automatically assigns tutors based on student's level. In\r\nthe field of economics, there are quantitative investment methods that\r\nautomatically analyze large amounts of data to find investment laws to create\r\ninvestment models or predict changes in financial markets. As such, artificial\r\nintelligence technology is being used in various fields. So, it is very\r\nimportant to know exactly what factors have an important influence on each\r\nfield of artificial intelligence technology and how the relationship between\r\neach field is connected. Therefore, it is necessary to analyze artificial\r\nintelligence technology in each field. In this paper, we analyze patent\r\ndocuments related to artificial intelligence technology. We propose a method\r\nfor keyword analysis within factors using artificial intelligence patent data\r\nsets for artificial intelligence technology analysis. This is a model that\r\nrelies on feature engineering based on deep learning model named KeyBERT, and\r\nusing vector space model. A case study of collecting and analyzing artificial\r\nintelligence patent data was conducted to show how the proposed model can be\r\napplied to real world problems.\r\n\",\"['Yongmin Yoo', 'Dongjin Lim', 'Kyungsun Kim']\"\r\nhttp://arxiv.org/abs/cs/0701087v2,Artificial intelligence,2007-01-13T16:50:37Z,2007-04-27T18:00:27Z,Artificiality in Social Sciences,\"  This text provides with an introduction to the modern approach of\r\nartificiality and simulation in social sciences. It presents the relationship\r\nbetween complexity and artificiality, before introducing the field of\r\nartificial societies which greatly benefited from the computer power fast\r\nincrease, gifting social sciences with formalization and experimentation tools\r\npreviously owned by \"\"hard\"\" sciences alone. It shows that as \"\"a new way of doing\r\nsocial sciences\"\", artificial societies should undoubtedly contribute to a\r\nrenewed approach in the study of sociality and should play a significant part\r\nin the elaboration of original theories of social phenomena.\r\n\",['Jean-Philippe Rennard']\r\nhttp://arxiv.org/abs/0901.0317v1,Artificial intelligence,2009-01-03T17:35:49Z,2009-01-03T17:35:49Z,Design of a P System based Artificial Graph Chemistry,\"  Artificial Chemistries (ACs) are symbolic chemical metaphors for the\r\nexploration of Artificial Life, with specific focus on the origin of life. In\r\nthis work we define a P system based artificial graph chemistry to understand\r\nthe principles leading to the evolution of life-like structures in an AC set up\r\nand to develop a unified framework to characterize and classify symbolic\r\nartificial chemistries by devising appropriate formalism to capture semantic\r\nand organizational information. An extension of P system is considered by\r\nassociating probabilities with the rules providing the topological framework\r\nfor the evolution of a labeled undirected graph based molecular reaction\r\nsemantics.\r\n\",['Janardan Misra']\r\nhttp://arxiv.org/abs/1412.6703v2,Artificial intelligence,2014-12-20T22:35:48Z,2014-12-23T16:17:35Z,\"Quantifying Natural and Artificial Intelligence in Robots and Natural\r\n  Systems with an Algorithmic Behavioural Test\",\"  One of the most important aims of the fields of robotics, artificial\r\nintelligence and artificial life is the design and construction of systems and\r\nmachines as versatile and as reliable as living organisms at performing high\r\nlevel human-like tasks. But how are we to evaluate artificial systems if we are\r\nnot certain how to measure these capacities in living systems, let alone how to\r\ndefine life or intelligence? Here I survey a concrete metric towards measuring\r\nabstract properties of natural and artificial systems, such as the ability to\r\nreact to the environment and to control one's own behaviour.\r\n\",['Hector Zenil']\r\nhttp://arxiv.org/abs/2308.13791v1,Augmented reality,2023-08-26T07:33:23Z,2023-08-26T07:33:23Z,Handwritten image augmentation,\"  In this paper, we introduce Handwritten augmentation, a new data augmentation\r\nfor handwritten character images. This method focuses on augmenting handwritten\r\nimage data by altering the shape of input characters in training. The proposed\r\nhandwritten augmentation is similar to position augmentation, color\r\naugmentation for images but a deeper focus on handwritten characters.\r\nHandwritten augmentation is data-driven, easy to implement, and can be\r\nintegrated with CNN-based optical character recognition models. Handwritten\r\naugmentation can be implemented along with commonly used data augmentation\r\ntechniques such as cropping, rotating, and yields better performance of models\r\nfor handwritten image datasets developed using optical character recognition\r\nmethods.\r\n\",['Mahendran N']\r\nhttp://arxiv.org/abs/2104.00722v1,Augmented reality,2021-04-01T19:00:17Z,2021-04-01T19:00:17Z,GABO: Graph Augmentations with Bi-level Optimization,\"  Data augmentation refers to a wide range of techniques for improving model\r\ngeneralization by augmenting training examples. Oftentimes such methods require\r\ndomain knowledge about the dataset at hand, spawning a plethora of recent\r\nliterature surrounding automated techniques for data augmentation. In this work\r\nwe apply one such method, bilevel optimization, to tackle the problem of graph\r\nclassification on the ogbg-molhiv dataset. Our best performing augmentation\r\nachieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which\r\nmakes it the most effective augmenter for this classifier on the leaderboard.\r\nThis framework combines a GIN layer augmentation generator with a bias\r\ntransformation and outperforms the same classifier augmented using the\r\nstate-of-the-art FLAG augmentation.\r\n\",\"['Heejung W. Chung', 'Avoy Datta', 'Chris Waites']\"\r\nhttp://arxiv.org/abs/2501.18648v2,Augmented reality,2025-01-29T16:38:57Z,2025-03-21T18:17:47Z,\"Multimodal Large Language Models for Image, Text, and Speech Data\r\n  Augmentation: A Survey\",\"  In the past five years, research has shifted from traditional Machine\r\nLearning (ML) and Deep Learning (DL) approaches to leveraging Large Language\r\nModels (LLMs) , including multimodality, for data augmentation to enhance\r\ngeneralization, and combat overfitting in training deep convolutional neural\r\nnetworks. However, while existing surveys predominantly focus on ML and DL\r\ntechniques or limited modalities (text or images), a gap remains in addressing\r\nthe latest advancements and multi-modal applications of LLM-based methods. This\r\nsurvey fills that gap by exploring recent literature utilizing multimodal LLMs\r\nto augment image, text, and audio data, offering a comprehensive understanding\r\nof these processes. We outlined various methods employed in the LLM-based\r\nimage, text and speech augmentation, and discussed the limitations identified\r\nin current approaches. Additionally, we identified potential solutions to these\r\nlimitations from the literature to enhance the efficacy of data augmentation\r\npractices using multimodal LLMs. This survey serves as a foundation for future\r\nresearch, aiming to refine and expand the use of multimodal LLMs in enhancing\r\ndataset quality and diversity for deep learning applications. (Surveyed Paper\r\nGitHub Repo: https://github.com/WSUAgRobotics/data-aug-multi-modal-llm.\r\nKeywords: LLM data augmentation, Grok text data augmentation, DeepSeek image\r\ndata augmentation, Grok speech data augmentation, GPT audio augmentation, voice\r\naugmentation, DeepSeek for data augmentation, DeepSeek R1 text data\r\naugmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM,\r\nText Augmentation using LLM, LLM data augmentation for deep learning\r\napplications)\r\n\",\"['Ranjan Sapkota', 'Shaina Raza', 'Maged Shoman', 'Achyut Paudel', 'Manoj Karkee']\"\r\nhttp://arxiv.org/abs/2312.05520v1,Augmented reality,2023-12-09T10:24:59Z,2023-12-09T10:24:59Z,Augmenty: A Python Library for Structured Text Augmentation,\"  Augmnety is a Python library for structured text augmentation. It is built on\r\ntop of spaCy and allows for augmentation of both the text and its annotations.\r\nAugmenty provides a wide range of augmenters which can be combined in a\r\nflexible manner to create complex augmentation pipelines. It also includes a\r\nset of primitives that can be used to create custom augmenters such as word\r\nreplacement augmenters. This functionality allows for augmentations within a\r\nrange of applications such as named entity recognition (NER), part-of-speech\r\ntagging, and dependency parsing.\r\n\",['Kenneth Enevoldsen']\r\nhttp://arxiv.org/abs/2101.05469v1,Augmented reality,2021-01-14T05:59:23Z,2021-01-14T05:59:23Z,Text Augmentation in a Multi-Task View,\"  Traditional data augmentation aims to increase the coverage of the input\r\ndistribution by generating augmented examples that strongly resemble original\r\nsamples in an online fashion where augmented examples dominate training. In\r\nthis paper, we propose an alternative perspective -- a multi-task view (MTV) of\r\ndata augmentation -- in which the primary task trains on original examples and\r\nthe auxiliary task trains on augmented examples. In MTV data augmentation, both\r\noriginal and augmented samples are weighted substantively during training,\r\nrelaxing the constraint that augmented examples must resemble original data and\r\nthereby allowing us to apply stronger levels of augmentation. In empirical\r\nexperiments using four common data augmentation techniques on three benchmark\r\ntext classification datasets, we find that the MTV leads to higher and more\r\nrobust performance improvements than traditional augmentation.\r\n\",\"['Jason Wei', 'Chengyu Huang', 'Shiqi Xu', 'Soroush Vosoughi']\"\r\nhttp://arxiv.org/abs/2203.06172v2,Augmented reality,2022-03-11T18:57:27Z,2022-03-15T15:36:24Z,Deep AutoAugment,\"  While recent automated data augmentation methods lead to state-of-the-art\r\nresults, their design spaces and the derived data augmentation strategies still\r\nincorporate strong human priors. In this work, instead of fixing a set of\r\nhand-picked default augmentations alongside the searched data augmentations, we\r\npropose a fully automated approach for data augmentation search named Deep\r\nAutoAugment (DeepAA). DeepAA progressively builds a multi-layer data\r\naugmentation pipeline from scratch by stacking augmentation layers one at a\r\ntime until reaching convergence. For each augmentation layer, the policy is\r\noptimized to maximize the cosine similarity between the gradients of the\r\noriginal and augmented data along the direction with low variance. Our\r\nexperiments show that even without default augmentations, we can learn an\r\naugmentation policy that achieves strong performance with that of previous\r\nworks. Extensive ablation studies show that the regularized gradient matching\r\nis an effective search method for data augmentation policies. Our code is\r\navailable at: https://github.com/MSU-MLSys-Lab/DeepAA .\r\n\",\"['Yu Zheng', 'Zhi Zhang', 'Shen Yan', 'Mi Zhang']\"\r\nhttp://arxiv.org/abs/2301.03174v2,Augmented reality,2023-01-09T05:21:17Z,2023-02-27T10:40:22Z,Augmented Quaternion and Augmented Unit Quaternion Optimization,\"  In this paper, we introduce and explore augmented quaternions and augmented\r\nunit quaternions, and present an augmented unit quaternion optimization model.\r\nAn augmented quaternion consist of a quaternion and a translation vector. The\r\nmultiplication rule of augmented quaternion is defined. An augmented unit\r\nquaternion consists of a unit quaternion and a translation vector. The\r\naugmented unit quaternions form a Lie group. By means of augmented unit\r\nquaternions, we study the error model and kinematics. Then we formulate two\r\nclassical problems in robot research, i.e., the hand-eye calibration problem\r\nand the simultaneous localization and mapping (SLAM) problem as augmented unit\r\nquaternion optimization problems, which are actually real smooth spherical\r\nequality constrained optimization problems. Comparing with the corresponding\r\nunit dual quaternion optimization model, the augmented unit quaternion\r\noptimization model has less variables and removes the orthogonality\r\nconstraints.\r\n\",\"['Liqun Qi', 'Xiangke Wang', 'Chunfeng Cui']\"\r\nhttp://arxiv.org/abs/2312.11309v2,Augmented reality,2023-12-18T16:02:43Z,2024-10-23T05:30:24Z,\"The Ultimate Combo: Boosting Adversarial Example Transferability by\r\n  Composing Data Augmentations\",\"  To help adversarial examples generalize from surrogate machine-learning (ML)\r\nmodels to targets, certain transferability-based black-box evasion attacks\r\nincorporate data augmentations (e.g., random resizing). Yet, prior work has\r\nexplored limited augmentations and their composition. To fill the gap, we\r\nsystematically studied how data augmentation affects transferability.\r\nSpecifically, we explored 46 augmentation techniques originally proposed to\r\nhelp ML models generalize to unseen benign samples, and assessed how they\r\nimpact transferability, when applied individually or composed. Performing\r\nexhaustive search on a small subset of augmentation techniques and genetic\r\nsearch on all techniques, we identified augmentation combinations that help\r\npromote transferability. Extensive experiments with the ImageNet and CIFAR-10\r\ndatasets and 18 models showed that simple color-space augmentations (e.g.,\r\ncolor to greyscale) attain high transferability when combined with standard\r\naugmentations. Furthermore, we discovered that composing augmentations impacts\r\ntransferability mostly monotonically (i.e., more augmentations $\\rightarrow$\r\n$\\ge$transferability). We also found that the best composition significantly\r\noutperformed the state of the art (e.g., 91.8% vs. $\\le$82.5% average\r\ntransferability to adversarially trained targets on ImageNet). Lastly, our\r\ntheoretical analysis, backed by empirical evidence, intuitively explains why\r\ncertain augmentations promote transferability.\r\n\",\"['Zebin Yun', 'Achi-Or Weingarten', 'Eyal Ronen', 'Mahmood Sharif']\"\r\nhttp://arxiv.org/abs/2403.00875v1,Augmented reality,2024-03-01T07:58:29Z,2024-03-01T07:58:29Z,\"Enhancing Protein Predictive Models via Proteins Data Augmentation: A\r\n  Benchmark and New Directions\",\"  Augmentation is an effective alternative to utilize the small amount of\r\nlabeled protein data. However, most of the existing work focuses on design-ing\r\nnew architectures or pre-training tasks, and relatively little work has studied\r\ndata augmentation for proteins. This paper extends data augmentation techniques\r\npreviously used for images and texts to proteins and then benchmarks these\r\ntechniques on a variety of protein-related tasks, providing the first\r\ncomprehensive evaluation of protein augmentation. Furthermore, we propose two\r\nnovel semantic-level protein augmentation methods, namely Integrated Gradients\r\nSubstitution and Back Translation Substitution, which enable protein\r\nsemantic-aware augmentation through saliency detection and biological\r\nknowledge. Finally, we integrate extended and proposed augmentations into an\r\naugmentation pool and propose a simple but effective framework, namely\r\nAutomated Protein Augmentation (APA), which can adaptively select the most\r\nsuitable augmentation combinations for different tasks. Extensive experiments\r\nhave shown that APA enhances the performance of five protein related tasks by\r\nan average of 10.55% across three architectures compared to vanilla\r\nimplementations without augmentation, highlighting its potential to make a\r\ngreat impact on the field.\r\n\",\"['Rui Sun', 'Lirong Wu', 'Haitao Lin', 'Yufei Huang', 'Stan Z. Li']\"\r\nhttp://arxiv.org/abs/2410.01088v2,Augmented reality,2024-10-01T21:33:10Z,2025-02-04T17:27:51Z,Exploring Empty Spaces: Human-in-the-Loop Data Augmentation,\"  Data augmentation is crucial to make machine learning models more robust and\r\nsafe. However, augmenting data can be challenging as it requires generating\r\ndiverse data points to rigorously evaluate model behavior on edge cases and\r\nmitigate potential harms. Creating high-quality augmentations that cover these\r\n\"\"unknown unknowns\"\" is a time- and creativity-intensive task. In this work, we\r\nintroduce Amplio, an interactive tool to help practitioners navigate \"\"unknown\r\nunknowns\"\" in unstructured text datasets and improve data diversity by\r\nsystematically identifying empty data spaces to explore. Amplio includes three\r\nhuman-in-the-loop data augmentation techniques: Augment With Concepts, Augment\r\nby Interpolation, and Augment with Large Language Model. In a user study with\r\n18 professional red teamers, we demonstrate the utility of our augmentation\r\nmethods in helping generate high-quality, diverse, and relevant model safety\r\nprompts. We find that Amplio enabled red teamers to augment data quickly and\r\ncreatively, highlighting the transformative potential of interactive\r\naugmentation workflows.\r\n\",\"['Catherine Yeh', 'Donghao Ren', 'Yannick Assogba', 'Dominik Moritz', 'Fred Hohman']\"\r\nhttp://arxiv.org/abs/2107.11990v2,Augmented reality,2021-07-26T06:54:53Z,2023-03-16T05:23:18Z,Augmentation Pathways Network for Visual Recognition,\"  Data augmentation is practically helpful for visual recognition, especially\r\nat the time of data scarcity. However, such success is only limited to quite a\r\nfew light augmentations (e.g., random crop, flip). Heavy augmentations are\r\neither unstable or show adverse effects during training, owing to the big gap\r\nbetween the original and augmented images. This paper introduces a novel\r\nnetwork design, noted as Augmentation Pathways (AP), to systematically\r\nstabilize training on a much wider range of augmentation policies. Notably, AP\r\ntames various heavy data augmentations and stably boosts performance without a\r\ncareful selection among augmentation policies. Unlike traditional single\r\npathway, augmented images are processed in different neural paths. The main\r\npathway handles the light augmentations, while other pathways focus on the\r\nheavier augmentations. By interacting with multiple paths in a dependent\r\nmanner, the backbone network robustly learns from shared visual patterns among\r\naugmentations, and suppresses the side effect of heavy augmentations at the\r\nsame time. Furthermore, we extend AP to high-order versions for high-order\r\nscenarios, demonstrating its robustness and flexibility in practical usage.\r\nExperimental results on ImageNet demonstrate the compatibility and\r\neffectiveness on a much wider range of augmentations, while consuming fewer\r\nparameters and lower computational costs at inference time.\r\n\",\"['Yalong Bai', 'Mohan Zhou', 'Wei Zhang', 'Bowen Zhou', 'Tao Mei']\"\r\nhttp://arxiv.org/abs/1310.7526v3,Augmented reality,2013-10-28T18:30:24Z,2017-06-05T20:13:39Z,\"KCH representations, augmentations, and $A$-polynomials\",\"  We describe a correspondence between augmentations and certain\r\nrepresentations of the knot group. The correspondence makes the 2-variable\r\naugmentation polynomial into a generalization of the classical $A$-polynomial.\r\nIt also associates to an augmentation a rank, which is bounded by the bridge\r\nnumber and shares its behavior under connect sums. We also study augmentations\r\nwith rank equal to the braid index.\r\n\",['Christopher Cornwell']\r\nhttp://arxiv.org/abs/1909.09148v2,Augmented reality,2019-09-19T08:36:45Z,2019-11-21T15:56:49Z,\"Data Augmentation Revisited: Rethinking the Distribution Gap between\r\n  Clean and Augmented Data\",\"  Data augmentation has been widely applied as an effective methodology to\r\nimprove generalization in particular when training deep neural networks.\r\nRecently, researchers proposed a few intensive data augmentation techniques,\r\nwhich indeed improved accuracy, yet we notice that these methods augment data\r\nhave also caused a considerable gap between clean and augmented data. In this\r\npaper, we revisit this problem from an analytical perspective, for which we\r\nestimate the upper-bound of expected risk using two terms, namely, empirical\r\nrisk and generalization error, respectively. We develop an understanding of\r\ndata augmentation as regularization, which highlights the major features. As a\r\nresult, data augmentation significantly reduces the generalization error, but\r\nmeanwhile leads to a slightly higher empirical risk. On the assumption that\r\ndata augmentation helps models converge to a better region, the model can\r\nbenefit from a lower empirical risk achieved by a simple method, i.e., using\r\nless-augmented data to refine the model trained on fully-augmented data. Our\r\napproach achieves consistent accuracy gain on a few standard image\r\nclassification benchmarks, and the gain transfers to object detection.\r\n\",\"['Zhuoxun He', 'Lingxi Xie', 'Xin Chen', 'Ya Zhang', 'Yanfeng Wang', 'Qi Tian']\"\r\nhttp://arxiv.org/abs/2003.06606v1,Augmented reality,2020-03-14T11:18:22Z,2020-03-14T11:18:22Z,\"Learn to Augment: Joint Data Augmentation and Network Optimization for\r\n  Text Recognition\",\"  Handwritten text and scene text suffer from various shapes and distorted\r\npatterns. Thus training a robust recognition model requires a large amount of\r\ndata to cover diversity as much as possible. In contrast to data collection and\r\nannotation, data augmentation is a low cost way. In this paper, we propose a\r\nnew method for text image augmentation. Different from traditional augmentation\r\nmethods such as rotation, scaling and perspective transformation, our proposed\r\naugmentation method is designed to learn proper and efficient data augmentation\r\nwhich is more effective and specific for training a robust recognizer. By using\r\na set of custom fiducial points, the proposed augmentation method is flexible\r\nand controllable. Furthermore, we bridge the gap between the isolated processes\r\nof data augmentation and network optimization by joint learning. An agent\r\nnetwork learns from the output of the recognition network and controls the\r\nfiducial points to generate more proper training samples for the recognition\r\nnetwork. Extensive experiments on various benchmarks, including regular scene\r\ntext, irregular scene text and handwritten text, show that the proposed\r\naugmentation and the joint learning methods significantly boost the performance\r\nof the recognition networks. A general toolkit for geometric augmentation is\r\navailable.\r\n\",\"['Canjie Luo', 'Yuanzhi Zhu', 'Lianwen Jin', 'Yongpan Wang']\"\r\nhttp://arxiv.org/abs/2010.11171v2,Augmented reality,2020-10-21T17:46:32Z,2021-10-27T00:16:34Z,How Data Augmentation affects Optimization for Linear Regression,\"  Though data augmentation has rapidly emerged as a key tool for optimization\r\nin modern machine learning, a clear picture of how augmentation schedules\r\naffect optimization and interact with optimization hyperparameters such as\r\nlearning rate is nascent. In the spirit of classical convex optimization and\r\nrecent work on implicit bias, the present work analyzes the effect of\r\naugmentation on optimization in the simple convex setting of linear regression\r\nwith MSE loss.\r\n  We find joint schedules for learning rate and data augmentation scheme under\r\nwhich augmented gradient descent provably converges and characterize the\r\nresulting minimum. Our results apply to arbitrary augmentation schemes,\r\nrevealing complex interactions between learning rates and augmentations even in\r\nthe convex setting. Our approach interprets augmented (S)GD as a stochastic\r\noptimization method for a time-varying sequence of proxy losses. This gives a\r\nunified way to analyze learning rate, batch size, and augmentations ranging\r\nfrom additive noise to random projections. From this perspective, our results,\r\nwhich also give rates of convergence, can be viewed as Monro-Robbins type\r\nconditions for augmented (S)GD.\r\n\",\"['Boris Hanin', 'Yi Sun']\"\r\nhttp://arxiv.org/abs/2105.13608v2,Augmented reality,2021-05-28T06:32:32Z,2021-06-02T15:18:33Z,\"Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data\r\n  Augmentation via MiniMax\",\"  In Natural Language Processing (NLP), finding data augmentation techniques\r\nthat can produce high-quality human-interpretable examples has always been\r\nchallenging. Recently, leveraging kNN such that augmented examples are\r\nretrieved from large repositories of unlabelled sentences has made a step\r\ntoward interpretable augmentation. Inspired by this paradigm, we introduce\r\nMinimax-kNN, a sample efficient data augmentation strategy tailored for\r\nKnowledge Distillation (KD). We exploit a semi-supervised approach based on KD\r\nto train a model on augmented data. In contrast to existing kNN augmentation\r\ntechniques that blindly incorporate all samples, our method dynamically selects\r\na subset of augmented samples that maximizes KL-divergence between the teacher\r\nand student models. This step aims to extract the most efficient samples to\r\nensure our augmented data covers regions in the input space with maximum loss\r\nvalue. We evaluated our technique on several text classification tasks and\r\ndemonstrated that Minimax-kNN consistently outperforms strong baselines. Our\r\nresults show that Minimax-kNN requires fewer augmented examples and less\r\ncomputation to achieve superior performance over the state-of-the-art kNN-based\r\naugmentation techniques.\r\n\",\"['Ehsan Kamalloo', 'Mehdi Rezagholizadeh', 'Peyman Passban', 'Ali Ghodsi']\"\r\nhttp://arxiv.org/abs/2110.13555v2,Augmented reality,2021-10-26T10:33:25Z,2021-11-29T03:38:19Z,Directional Self-supervised Learning for Heavy Image Augmentations,\"  Despite the large augmentation family, only a few cherry-picked robust\r\naugmentation policies are beneficial to self-supervised image representation\r\nlearning. In this paper, we propose a directional self-supervised learning\r\nparadigm (DSSL), which is compatible with significantly more augmentations.\r\nSpecifically, we adapt heavy augmentation policies after the views lightly\r\naugmented by standard augmentations, to generate harder view (HV). HV usually\r\nhas a higher deviation from the original image than the lightly augmented\r\nstandard view (SV). Unlike previous methods equally pairing all augmented views\r\nto symmetrically maximize their similarities, DSSL treats augmented views of\r\nthe same instance as a partially ordered set (with directions as\r\nSV$\\leftrightarrow $SV, SV$\\leftarrow$HV), and then equips a directional\r\nobjective function respecting to the derived relationships among views. DSSL\r\ncan be easily implemented with a few lines of codes and is highly flexible to\r\npopular self-supervised learning frameworks, including SimCLR, SimSiam, BYOL.\r\nExtensive experimental results on CIFAR and ImageNet demonstrated that DSSL can\r\nstably improve various baselines with compatibility to a wider range of\r\naugmentations.\r\n\",\"['Yalong Bai', 'Yifan Yang', 'Wei Zhang', 'Tao Mei']\"\r\nhttp://arxiv.org/abs/2206.04726v2,Augmented reality,2022-06-09T18:46:38Z,2022-06-13T07:11:26Z,\"COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive\r\n  Learning\",\"  Graph contrastive learning (GCL) improves graph representation learning,\r\nleading to SOTA on various downstream tasks. The graph augmentation step is a\r\nvital but scarcely studied step of GCL. In this paper, we show that the node\r\nembedding obtained via the graph augmentations is highly biased, somewhat\r\nlimiting contrastive models from learning discriminative features for\r\ndownstream tasks. Thus, instead of investigating graph augmentation in the\r\ninput space, we alternatively propose to perform augmentations on the hidden\r\nfeatures (feature augmentation). Inspired by so-called matrix sketching, we\r\npropose COSTA, a novel COvariance-preServing feaTure space Augmentation\r\nframework for GCL, which generates augmented features by maintaining a \"\"good\r\nsketch\"\" of original features. To highlight the superiority of feature\r\naugmentation with COSTA, we investigate a single-view setting (in addition to\r\nmulti-view one) which conserves memory and computations. We show that the\r\nfeature augmentation with COSTA achieves comparable/better results than graph\r\naugmentation based models.\r\n\",\"['Yifei Zhang', 'Hao Zhu', 'Zixing Song', 'Piotr Koniusz', 'Irwin King']\"\r\nhttp://arxiv.org/abs/2211.01184v1,Augmented reality,2022-11-02T14:58:03Z,2022-11-02T14:58:03Z,\"Joint Data and Feature Augmentation for Self-Supervised Representation\r\n  Learning on Point Clouds\",\"  To deal with the exhausting annotations, self-supervised representation\r\nlearning from unlabeled point clouds has drawn much attention, especially\r\ncentered on augmentation-based contrastive methods. However, specific\r\naugmentations hardly produce sufficient transferability to high-level tasks on\r\ndifferent datasets. Besides, augmentations on point clouds may also change\r\nunderlying semantics. To address the issues, we propose a simple but efficient\r\naugmentation fusion contrastive learning framework to combine data\r\naugmentations in Euclidean space and feature augmentations in feature space. In\r\nparticular, we propose a data augmentation method based on sampling and graph\r\ngeneration. Meanwhile, we design a data augmentation network to enable a\r\ncorrespondence of representations by maximizing consistency between augmented\r\ngraph pairs. We further design a feature augmentation network that encourages\r\nthe model to learn representations invariant to the perturbations using an\r\nencoder perturbation. We comprehensively conduct extensive object\r\nclassification experiments and object part segmentation experiments to validate\r\nthe transferability of the proposed framework. Experimental results demonstrate\r\nthat the proposed framework is effective to learn the point cloud\r\nrepresentation in a self-supervised manner, and yields state-of-the-art results\r\nin the community. The source code is publicly available at:\r\nhttps://zhiyongsu.github.io/Project/AFSRL.html.\r\n\",\"['Zhuheng Lu', 'Yuewei Dai', 'Weiqing Li', 'Zhiyong Su']\"\r\nhttp://arxiv.org/abs/2305.13520v1,Augmented reality,2023-05-22T22:23:40Z,2023-05-22T22:23:40Z,\"Tied-Augment: Controlling Representation Similarity Improves Data\r\n  Augmentation\",\"  Data augmentation methods have played an important role in the recent advance\r\nof deep learning models, and have become an indispensable component of\r\nstate-of-the-art models in semi-supervised, self-supervised, and supervised\r\ntraining for vision. Despite incurring no additional latency at test time, data\r\naugmentation often requires more epochs of training to be effective. For\r\nexample, even the simple flips-and-crops augmentation requires training for\r\nmore than 5 epochs to improve performance, whereas RandAugment requires more\r\nthan 90 epochs. We propose a general framework called Tied-Augment, which\r\nimproves the efficacy of data augmentation in a wide range of applications by\r\nadding a simple term to the loss that can control the similarity of\r\nrepresentations under distortions. Tied-Augment can improve state-of-the-art\r\nmethods from data augmentation (e.g. RandAugment, mixup), optimization (e.g.\r\nSAM), and semi-supervised learning (e.g. FixMatch). For example,\r\nTied-RandAugment can outperform RandAugment by 2.0% on ImageNet. Notably, using\r\nTied-Augment, data augmentation can be made to improve generalization even when\r\ntraining for a few epochs and when fine-tuning. We open source our code at\r\nhttps://github.com/ekurtulus/tied-augment/tree/main.\r\n\",\"['Emirhan Kurtulus', 'Zichao Li', 'Yann Dauphin', 'Ekin Dogus Cubuk']\"\r\nhttp://arxiv.org/abs/2304.09965v1,Blockchain,2023-04-19T20:55:59Z,2023-04-19T20:55:59Z,Vulnerability of Finitely-long Blockchains in Securing Data,\"  Recently, blockchain has been applied in various fields to secure data\r\nexchanges and storage in decentralized systems. In a blockchain application\r\nwhere the task of the application which makes use of the data stored in a\r\nblockchain has to be accomplished by a time instant, the employed blockchain is\r\nessentially finitely-long. In this paper, we consider a general finitely-long\r\nblockchain model which is generalized from most existing works on finitely-long\r\nblockchain applications, and take the first step towards characterizing the\r\nvulnerability of finitely-long blockchains in securing data against\r\ndouble-spending attacks. For the first time, we develop a general closed-form\r\nexpression for the probability of success in launching a double-spending attack\r\non a finitely-long blockchain. This probability essentially characterizes the\r\nvulnerability of finitely-long blockchains. Then, we prove that the probability\r\nof success in launching a double-spending attack on a finitely-long blockchain\r\nis no greater than that on an infinitely-long blockchain, which implies that\r\nfinitely-long blockchains are less vulnerable to double-spending attacks than\r\ninfinitely-long blockchains. Moreover, we show that unlike infinitely-long\r\nblockchains which can be surely paralyzed by a 51% attack, finitely-long\r\nblockchains are more resistant to 51% attacks.\r\n\",\"['Yiming Jiang', 'Jiangfan Zhang']\"\r\nhttp://arxiv.org/abs/1905.07014v1,Blockchain,2019-05-15T13:42:46Z,2019-05-15T13:42:46Z,A Framework for Blockchain Interoperability and Runtime Selection,\"  The suitability of a particular blockchain for a given use case depends\r\nmainly on the blockchain's functional and non-functional properties. Such\r\nproperties may vary over time, and thus, a selected blockchain may become\r\nunsuitable for a given use case. This uncertainty may hinder the widespread\r\nadoption of blockchain technologies in general. To mitigate the impact of\r\nvolatile blockchain properties, we propose a framework that monitors several\r\nblockchains, allows the user to define functional and non-functional\r\nrequirements, determines the most appropriate blockchain, and enables the\r\nswitchover to that chain at runtime. Our evaluation using a reference\r\nimplementation shows that switching to another blockchain can save cost and\r\nenable users to benefit from better performance and a higher level of trust.\r\n\",\"['Philipp Frauenthaler', 'Michael Borkowski', 'Stefan Schulte']\"\r\nhttp://arxiv.org/abs/1909.02914v1,Blockchain,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,\"Blockchain Technologies for Smart Energy Systems: Fundamentals,\r\n  Challenges and Solutions\",\"  In this paper, we discuss the integration of blockchain in smart energy\r\nsystems. We present various blockchain technology solutions, review important\r\nblockchain platforms, and several blockchain based smart energy projects in\r\ndifferent smart energy domains. The majority of blockchain platforms with\r\nembedded combination of blockchain technology solutions are computing- and\r\nresource- intensive, and hence not entirely suitable for smart energy\r\napplications. We consider the requirements of smart energy systems and\r\naccordingly identify appropriate blockchain technology solutions for smart\r\nenergy applications. Our analysis can help in the development of flexible\r\nblockchain platforms for smart energy systems.\r\n\",\"['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']\"\r\nhttp://arxiv.org/abs/2002.12837v1,Blockchain,2020-02-26T13:49:47Z,2020-02-26T13:49:47Z,Testimonium: A Cost-Efficient Blockchain Relay,\"  Current blockchain technologies provide very limited means of\r\ninteroperability. In particular, solutions enabling blockchains to verify the\r\nexistence of data on other blockchains are either very costly or are not fully\r\ndecentralized. To overcome these limitations, we introduce Testimonium, a novel\r\nblockchain relay scheme that applies a validation-on-demand pattern and the\r\non-chain execution of Simplified Payment Verifications to enable the\r\nverification of data across blockchains while remaining fully decentralized.\r\nEvaluating the scheme for Ethereum-based blockchains shows that Testimonium\r\nachieves a cost reduction of up to 92% over existing solutions. As such, the\r\nscheme lays a strong foundation for generic blockchain interoperability. For\r\ninstance, it enables the development of an atomic-commit protocol for\r\ndistributed transactions across blockchains.\r\n\",\"['Philipp Frauenthaler', 'Marten Sigwart', 'Christof Spanring', 'Stefan Schulte']\"\r\nhttp://arxiv.org/abs/1910.14614v1,Blockchain,2019-10-31T17:02:07Z,2019-10-31T17:02:07Z,\"Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability\r\n  Prediction\",\"  Blockchain and blockchain-based decentralized applications are attracting\r\nincreasing attentions recently. In public blockchain systems, users usually\r\nconnect to third-party peers or run a peer to join the P2P blockchain network.\r\nHowever, connecting to unreliable blockchain peers will make users waste\r\nresources and even lose millions of dollars of cryptocurrencies. In order to\r\nselect the reliable blockchain peers, it is urgently needed to evaluate and\r\npredict the reliability of them. Faced with this problem, we propose H-BRP,\r\nHybrid Blockchain Reliability Prediction model to extract the blockchain\r\nreliability factors then make personalized prediction for each user.\r\nLarge-scale real-world experiments are conducted on 100 blockchain requesters\r\nand 200 blockchain peers. The implement and dataset of 2,000,000 test cases are\r\nreleased. The experimental results show that the proposed model obtains better\r\naccuracy than other approaches.\r\n\",\"['Peilin Zheng', 'Zibin Zheng', 'Liang Chen']\"\r\nhttp://arxiv.org/abs/2105.02118v1,Blockchain,2021-04-16T14:49:38Z,2021-04-16T14:49:38Z,\"Managing Blockchain Systems and Applications: A Process Model for\r\n  Blockchain Configurations\",\"  Blockchain is a radical innovation with a unique value proposition that\r\nshifts trust from institutions to algorithms. Still, the potential of\r\nblockchains remains elusive due to knowledge gaps between computer science\r\nresearch and socio-economic research. Building on information technology\r\ngovernance literature and the theory of coevolution, this study develops a\r\nprocess model for blockchain configurations that captures blockchain capability\r\ndimensions and application areas. We demonstrate the applicability of the\r\nproposed blockchain configuration process model on four blockchain projects.\r\nThe proposed blockchain configuration process model assists with the selection\r\nand configuration of blockchain systems based on a set of known requirements\r\nfor a blockchain project. Our findings contribute to research by bridging\r\nknowledge gaps between computer science and socio-economic research on\r\nblockchain. Specifically, we explore existing blockchain concepts and integrate\r\nthem in a process model for blockchain configurations.\r\n\",\"['Olga Labazova', 'Erol Kazan', 'Tobias Dehling', 'Tuure Tuunanen', 'Ali Sunyaev']\"\r\nhttp://arxiv.org/abs/1707.01766v1,Blockchain,2017-07-06T13:03:04Z,2017-07-06T13:03:04Z,A Logic of Blockchain Updates,\"  Blockchains are distributed data structures that are used to achieve\r\nconsensus in systems for cryptocurrencies (like Bitcoin) or smart contracts\r\n(like Ethereum). Although blockchains gained a lot of popularity recently,\r\nthere is no logic-based model for blockchains available. We introduce BCL, a\r\ndynamic logic to reason about blockchain updates, and show that BCL is sound\r\nand complete with respect to a simple blockchain model.\r\n\",\"['Kai Brnnler', 'Dandolo Flumini', 'Thomas Studer']\"\r\nhttp://arxiv.org/abs/1803.00892v1,Blockchain,2018-03-02T15:27:39Z,2018-03-02T15:27:39Z,A Framework for Blockchain-Based Applications,\"  Blockchains have recently generated explosive interest from both academia and\r\nindustry, with many proposed applications. But descriptions of many these\r\nproposals are more visionary projections than realizable proposals, and even\r\nbasic definitions are often missing. We define \"\"blockchain\"\" and \"\"blockchain\r\nnetwork\"\", and then discuss two very different, well known classes of blockchain\r\nnetworks: cryptocurrencies and Git repositories. We identify common primitive\r\nelements of both and use them to construct a framework for explicitly\r\narticulating what characterizes blockchain networks. The framework consists of\r\na set of questions that every blockchain initiative should address at the very\r\noutset. It is intended to help one decide whether or not blockchain is an\r\nappropriate approach to a particular application, and if it is, to assist in\r\nits initial design stage.\r\n\",['Ephraim Feig']\r\nhttp://arxiv.org/abs/2112.11072v2,Blockchain,2021-12-21T10:10:51Z,2022-12-27T21:31:10Z,\"Scalable Multi-Chain Coordination via the Hierarchical Longest Chain\r\n  Rule\",\"  This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain\r\nsystem which achieves high transaction throughput through a hierarchy of merged\r\nmined blockchains, each operating in parallel on a partition the overall\r\napplication state. Most notably, the full PoW available within the network is\r\napplied to all blockchains in BlockReduce, and cross-blockchain state\r\ntransitions are enabled seamlessly within the core protocol. This paper shows\r\nthat, given a hierarchy of blockchains and its associated security model, the\r\nprotocol scales superlinearly in transaction throughput with the number of\r\nblockchains operated by the protocol.\r\n\",\"['Yanni Georghiades', 'Karl Kreder', 'Jonathan Downing', 'Alan Orwick', 'Sriram Vishwanath']\"\r\nhttp://arxiv.org/abs/2210.14888v1,Blockchain,2022-10-24T11:50:18Z,2022-10-24T11:50:18Z,A Decision Framework for Blockchain Adoption,\"  Blockchain and distributed ledger technologies are gaining the interest of\r\nthe academy, companies, and institutions. Nonetheless, the path toward\r\nblockchain adoption is not straightforward, as blockchain is a complex\r\ntechnology that requires revisiting the standard way of addressing problems and\r\ntackling them from a decentralized perspective. Thus, decision-makers adopt\r\nblockchain technology for the wrong reasons or prefer it to more suitable ones.\r\nThis work presents a decision framework for blockchain adoption to help\r\ndecision-makers decide whether blockchain is applicable, valuable, and\r\npreferable to other technologies. In particular, The decision framework is\r\ncomposed of a small set of questions that can be answered from a managerial\r\nstandpoint and that do not require a deep technical knowledge of\r\nblockchain-related topics.\r\n\",\"['Vittorio Capocasale', 'Guido Perboli']\"\r\nhttp://arxiv.org/abs/1910.00742v1,Blockchain,2019-10-02T01:37:20Z,2019-10-02T01:37:20Z,\"ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for\r\n  Supporting Hierarchical Storage\",\"  The fast developing Industrial Internet of Things (IIoT) technologies provide\r\na promising opportunity to build large-scale systems to connect numerous\r\nheterogeneous devices into the Internet. Most existing IIoT infrastructures are\r\nbased on a centralized architecture, which is easier for management but cannot\r\neffectively support immutable and verifiable services among multiple parties.\r\nBlockchain technology provides many desired features for large-scale IIoT\r\ninfrastructures, such as decentralization, trustworthiness, trackability, and\r\nimmutability. This paper presents a blockchain-based IIoT architecture to\r\nsupport immutable and verifiable services. However, when applying blockchain\r\ntechnology to the IIoT infrastructure, the required storage space posts a grant\r\nchallenge to resource-constrained IIoT infrastructures. To address the storage\r\nissue, this paper proposes a hierarchical blockchain storage structure,\r\n\\textit{ChainSplitter}. Specially, the proposed architecture features a\r\nhierarchical storage structure where the majority of the blockchain is stored\r\nin the clouds, while the most recent blocks are stored in the overlay network\r\nof the individual IIoT networks. The proposed architecture seamlessly binds\r\nlocal IIoT networks, the blockchain overlay network, and the cloud\r\ninfrastructure together through two connectors, the \\textit{blockchain\r\nconnector} and the \\textit{cloud connector}, to construct the hierarchical\r\nblockchain storage. The blockchain connector in the overlay network builds\r\nblocks in blockchain from data generated in IIoT networks, and the cloud\r\nconnector resolves the blockchain synchronization issues between the overlay\r\nnetwork and the clouds. We also provide a case study to show the efficiency of\r\nthe proposed hierarchical blockchain storage in a practical Industrial IoT\r\ncase.\r\n\",\"['Gang Wang', 'Zhijie Jerry Shi', 'Mark Nixon', 'Song Han']\"\r\nhttp://arxiv.org/abs/2111.13683v1,Blockchain,2021-11-25T07:13:15Z,2021-11-25T07:13:15Z,A Survey of Blockchain Data Management Systems,\"  Blockchain has been widely deployed in various sectors, such as finance,\r\neducation, and public services. Since blockchain runs as an immutable\r\ndistributed ledger, it has decentralized mechanisms with persistency,\r\nanonymity, and auditability, where transactions are jointly performed through\r\ncryptocurrency-based consensus algorithms by worldwide distributed nodes. There\r\nhave been many survey papers reviewing the blockchain technologies from\r\ndifferent perspectives, e.g., digital currencies, consensus algorithms, and\r\nsmart contracts. However, none of them have focused on the blockchain data\r\nmanagement systems. To fill in this gap, we have conducted a comprehensive\r\nsurvey on the data management systems, based on three typical types of\r\nblockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed\r\nAcyclic Graph)-based blockchain. We categorize their data management mechanisms\r\ninto three layers: blockchain architecture, blockchain data structure, and\r\nblockchain storage engine, where block architecture indicates how to record\r\ntransactions on a distributed ledger, blockchain data structure refers to the\r\ninternal structure of each block, and blockchain storage engine specifies the\r\nstorage form of data on the blockchain system. For each layer, the works\r\nadvancing the state-of-the-art are discussed together with technical\r\nchallenges. Furthermore, we lay out the future research directions for the\r\nblockchain data management systems.\r\n\",\"['Qian Wei', 'Bingzhe Li', 'Wanli Chang', 'Zhiping Jia', 'Zhaoyan Shen', 'Zili Shao']\"\r\nhttp://arxiv.org/abs/2207.07453v1,Blockchain,2022-07-15T13:01:00Z,2022-07-15T13:01:00Z,\"A Consensus Algorithm Based on Risk Assessment Model for Permissioned\r\n  Blockchain\",\"  Blockchain technology enables stakeholders to conduct trusted data sharing\r\nand exchange without a trusted centralized institution. These features make\r\nblockchain applications attractive to enhance trustworthiness in very different\r\ncontexts. Due to unique design concepts and outstanding performance, blockchain\r\nhas become a popular research topic in industry and academia in recent years.\r\nEvery participant is anonymous in a permissionless blockchain represented by\r\ncryptocurrency applications such as Bitcoin. In this situation, some special\r\nincentive mechanisms are applied to permissionless blockchain, such as mined\r\nnative cryptocurrency to solve the trust issues of permissionless blockchain.\r\nIn many use cases, permissionless blockchain has bottlenecks in transaction\r\nthroughput performance, which restricts further application in the real world.\r\nA permissioned blockchain can reach a consensus among a group of entities that\r\ndo not establish an entire trust relationship. Unlike permissionless\r\nblockchains, the participants must be identified in permissioned blockchains.\r\nBy relying on the traditional crash fault-tolerant consensus protocols,\r\npermissioned blockchains can achieve high transaction throughput and low\r\nlatency without sacrificing security. However, how to balance the security and\r\nconsensus efficiency is still the issue that needs to be solved urgently in\r\npermissioned blockchains. As the core module of blockchain technology, the\r\nconsensus algorithm plays a vital role in the performance of the blockchain\r\nsystem. Thus, this paper proposes a new consensus algorithm for permissioned\r\nblockchain, the Risk Assessment-based Consensus protocol (RAC), combined with\r\nthe decentralized design concept and the risk-node assessment mechanism to\r\naddress the unbalance issues of performance in speed, scalability, and\r\nsecurity.\r\n\",\"['Xiaohui Zhang', 'Mingying Xue', 'Xianghua Miao']\"\r\nhttp://arxiv.org/abs/2407.17761v1,Blockchain,2024-07-25T04:28:52Z,2024-07-25T04:28:52Z,Towards the Blockchain Massive Adoption with Permissionless Storage,\"  Blockchain technology emerged with the advent of Bitcoin and rapidly\r\ndeveloped over the past few decades, becoming widely accepted and known by the\r\npublic. However, in the past decades, the massive adoption of blockchain\r\ntechnology has yet to come. Rather than the scalability issue, the blockchain\r\napplication is challenged by its expensive usage cost. However, the high cost\r\nof blockchain usage is deeply connected with the blockchain consensus and\r\nsecurity mechanism. The permissionless blockchain must maintain its high cost\r\nfor security against the 51% Attack. Chain users indirectly cover the cost as\r\ncoins are appointed for blockchain usage fees. This conflict prevents the\r\nmassive adoption of blockchain. Thus, blockchain must be improved to solve\r\nthose problems: 1. The cost of blockchain usage should be low enough. 2. The\r\nblockchain should remain decentralized. 3. The scalability of blockchain must\r\nmeet the demand.\r\n  In my thesis, new approaches are applied to solve the issues above. The key\r\ncontribution is the discovery of the useful PoW. It extends the Nakamoto PoW\r\nwith another usage of file data encoding during the same Nakamoto Consensus\r\ncomputation to prove honest data preservation. Based on this theory, a\r\npermissionless storage network is proposed as the new security engine for the\r\nblockchain. It bridges the high blockchain security cost to the storage users\r\nwith real demands who are willing to pay for the storage resource. On the other\r\nhand, the chain users can benefit from the low transaction fee. Meanwhile, we\r\nalso provide a scalability solution to shard the blockchain. It enables high\r\nTPS and keeps decentralization. The solutions in this thesis provide the\r\nanswers to all the dependencies of the massive adoption.\r\n\",['Jia Kan']\r\nhttp://arxiv.org/abs/1907.07099v1,Blockchain,2019-07-16T16:23:25Z,2019-07-16T16:23:25Z,Blockchain Mutability: Challenges and Proposed Solutions,\"  Blockchain's evolution during the past decade is astonishing: from bitcoin to\r\nover 2.000 altcoins, and from decentralised electronic payments to transactions\r\nprogrammable by smart contracts and complex tokens governed by decentralised\r\norganisations. While the new generation of blockchain applications is still\r\nevolving, blockchain's technical characteristics are also advancing. Yet,\r\nimmutability, a hitherto indisputable property according to which blockchain\r\ndata cannot be edited nor deleted, remains the cornerstone of blockchain's\r\nsecurity. Nevertheless, blockchain's immutability is being called into question\r\nlately in the light of the new erasing requirements imposed by the GDPR's\r\n``\\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges\r\nblockchain data to be editable in order restricted content redactions,\r\nmodifications or deletions to be applied when requested, blockchains compliance\r\nwith the regulation is indeed challenging, if not impracticable. Towards\r\nresolving this contradiction, various methods and techniques for mutable\r\nblockchains have been proposed in an effort to satisfy regulatory erasing\r\nrequirements while preserving blockchains' security. To this end, this work\r\naims to provide a comprehensive review on the state-of-the-art research\r\napproaches, technical workarounds and advanced cryptographic techniques that\r\nhave been put forward to resolve this conflict and to discuss their potentials,\r\nconstraints and limitations when applied in the wild to either permissioned or\r\npermissionless blockchains.\r\n\",\"['Eugenia Politou', 'Fran Casino', 'Efthimios Alepis', 'Constantinos Patsakis']\"\r\nhttp://arxiv.org/abs/1912.05241v1,Blockchain,2019-12-11T11:33:36Z,2019-12-11T11:33:36Z,Performance Analysis of the Libra Blockchain: An Experimental Study,\"  Since Bitcoin was first introduced in 2008, many types of cryptocurrencies\r\nhave been proposed based on blockchain. However, the performance of\r\npermissionless blockchains restricts the widespread of cryptocurrency.\r\nRecently, Libra was proposed by Facebook based on a permissioned blockchain,\r\ni.e. the Libra blockchain. The vision of Libra is to become a global currency\r\nsupporting financial applications, but it is doubted whether the performance of\r\nthe Libra blockchain is able to support frequent micropayment scenarios. In\r\nthis paper, we propose a methodology to evaluate the performance of blockchain\r\nplatforms and conducted an experimental study on the Libra blockchain. The\r\nresults show that the Libra blockchain can only process about one thousand\r\ntransactions per second at most, and the performance drops significantly as the\r\nnumber of validators increases. Although it outperforms permissionless\r\nblockchain platforms, the performance of the Libra blockchain is still\r\nunsatisfactory compared to other permissioned blockchains like Hyperledger\r\nFabric and needs to make effective improvements in order to support global\r\nmicropayment in the future.\r\n\",\"['Jiashuo Zhang', 'Jianbo Gao', 'Zhenhao Wu', 'Wentian Yan', 'Qize Wu', 'Qingshan Li', 'Zhong Chen']\"\r\nhttp://arxiv.org/abs/2001.01174v1,Blockchain,2020-01-05T05:58:41Z,2020-01-05T05:58:41Z,\"Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain\r\n  Transactions\",\"  The interoperability across multiple blockchains would play a critical role\r\nin future blockchain-based data management paradigm. Existing techniques either\r\nwork only for two blockchains or requires a centralized component to govern the\r\ncross-blockchain transaction execution, neither of which would meet the\r\nscalability requirement. This paper proposes a new distributed commit protocol,\r\nnamely \\textit{cross-blockchain transaction} (CBT), for conducting transactions\r\nacross an arbitrary number of blockchains without any centralized component.\r\nThe key idea of CBT is to extend the two-phase commit protocol with a heartbeat\r\nmechanism to ensure the liveness of CBT without introducing additional nodes or\r\nblockchains. We have implemented CBT and compared it to the state-of-the-art\r\nprotocols, demonstrating CBT's low overhead (3.6\\% between two blockchains,\r\nless than $1\\%$ among 32 or more blockchains) and high scalability (linear\r\nscalability on up to 64-blockchain transactions). In addition, we developed a\r\ngraphic user interface for users to virtually monitor the status of the\r\ncross-blockchain transactions.\r\n\",\"['Xinying Wang', 'Olamide Timothy Tawose', 'Feng Yan', 'Dongfang Zhao']\"\r\nhttp://arxiv.org/abs/2010.16034v1,Blockchain,2020-10-30T02:55:19Z,2020-10-30T02:55:19Z,State sharding model on the blockchain,\"  Blockchain is an incrementally updated ledger maintained by distributed nodes\r\nrather than centralized organizations. The current blockchain technology faces\r\nscalability issues, which include two aspects: low transaction throughput and\r\nhigh storage capacity costs. This paper studies the blockchain structure based\r\non state sharding technology, and mainly solves the problem of non-scalability\r\nof block chain storage. This paper designs and implements the blockchain state\r\nsharding scheme, proposes a specific state sharding data structure and\r\nalgorithm implementation, and realizes a complete blockchain structure so that\r\nthe blockchain has the advantages of high throughput, processing a large number\r\nof transactions and saving storage costs. Experimental results show that a\r\nblockchain network with more than 100,000 nodes can be divided into 1024\r\nshards. A blockchain network with this structure can process 500,000\r\ntransactions in about 5 seconds. If the consensus time of the blockchain is\r\nabout 10 seconds, and the block generation time of the blockchain system of the\r\nsharding mechanism is 15 seconds, the transaction throughput can reach 33,000\r\ntx/sec. Experimental results show that the throughput of the proposed protocol\r\nincreases with the increase of the network node size. This confirms the\r\nscalability of the blockchain structure based on sharding technology.\r\n\",\"['Xiangyu Wang', 'Ting Yang', 'Yu Wang']\"\r\nhttp://arxiv.org/abs/2212.14671v1,Blockchain,2022-12-12T02:05:59Z,2022-12-12T02:05:59Z,Novel Architecture to Create and Maintain Personal Blockchains,\"  Blockchain has been touted as a revolutionary technology. However, despite\r\nthe excitement, blockchain has not been adopted in many fields. Many are\r\nhesitant to adopt blockchain technology due to privacy concerns, barriers to\r\nuse, or lack of practical use cases. In this work, we outline a potential\r\nblockchain use case for tracking financial transactions across multiple\r\nfinancial institutions. We show the downsides of traditional centralized\r\napproaches and that blockchain approaches fail to give all the privacy and\r\naccessibility required for this use case. Thus we propose a novel blockchain\r\narchitecture to support our use case. This novel architecture combines the ease\r\nof use of public blockchains with the privacy of private blockchains by\r\nallowing users to create personal blockchains. We believe this novel personal\r\nblockchain architecture will lead to more blockchain adoption, particularly in\r\nuse cases handling private data.\r\n\",\"['Collin Connors', 'Dilip Sarkar']\"\r\nhttp://arxiv.org/abs/2305.03895v1,Blockchain,2023-05-06T02:15:00Z,2023-05-06T02:15:00Z,Rateless Coded Blockchain for Dynamic IoT Networks,\"  A key constraint that limits the implementation of blockchain in Internet of\r\nThings (IoT) is its large storage requirement resulting from the fact that each\r\nblockchain node has to store the entire blockchain. This increases the burden\r\non blockchain nodes, and increases the communication overhead for new nodes\r\njoining the network since they have to copy the entire blockchain. In order to\r\nreduce storage requirements without compromising on system security and\r\nintegrity, coded blockchains, based on error correcting codes with fixed rates\r\nand lengths, have been recently proposed. This approach, however, does not fit\r\nwell with dynamic IoT networks in which nodes actively leave and join. In such\r\ndynamic blockchains, the existing coded blockchain approaches lead to high\r\ncommunication overheads for new joining nodes and may have high decoding\r\nfailure probability. This paper proposes a rateless coded blockchain with\r\ncoding parameters adjusted to network conditions. Our goals are to minimize\r\nboth the storage requirement at each blockchain node and the communication\r\noverhead for each new joining node, subject to a target decoding failure\r\nprobability. We evaluate the proposed scheme in the context of real-world\r\nBitcoin blockchain and show that both storage and communication overhead are\r\nreduced by 99.6\\% with a maximum $10^{-12}$ decoding failure probability.\r\n\",\"['Changlin Yang', 'Alexei Ashikhmin', 'Xiaodong Wang', 'Zibin Zheng']\"\r\nhttp://arxiv.org/abs/2203.00502v1,Cancer vaccine,2022-02-04T11:50:19Z,2022-02-04T11:50:19Z,\"Sensor technologies in cancer research for new directions in diagnosis\r\n  and treatment: and exploratory analysis\",\"  The goal of this study is an exploratory analysis concerning main sensor\r\ntechnologies applied in cancer research to detect new directions in diagnosis\r\nand treatments. The study focused on types of cancer having a high incidence\r\nand mortality worldwide: breast, lung, colorectal and prostate. Data of the Web\r\nof Science (WOS) core collection database are used to retrieve articles related\r\nto sensor technologies and cancer research over 1991-2021 period. We utilized\r\nGephi software version 0.9.2 to visualize the co-word networks of the\r\ninteraction between sensor technologies and cancers under study. Results show\r\nmain clusters of interaction per typology of cancer. Biosensor is the only type\r\nof sensor that plays an essential role in all types of cancer: breast cancer,\r\nlung cancer, prostate cancer, and colorectal cancer. Electrochemical sensor is\r\napplied in all types of cancer under study except lung cancer. Electrochemical\r\nbiosensor is used in breast cancer, lung cancer, and prostate cancer research\r\nbut not colorectal cancer. Optical sensor can also be considered one of the\r\nsensor technologies that significantly is used in breast cancer, prostate\r\ncancer, and colorectal cancer. This study shows that this type of sensor is\r\napplied in more diversified approaches. Moreover, the oxygen sensor is mostly\r\nstudied in lung cancer and breast cancer due to the usage in breath analysis\r\nfor the treatment process. Finally, Cmos sensor is a technology used mainly in\r\nlung cancer and colorectal cancer. Results here suggest new directions for the\r\nevolution of science and technology of sensors in cancer research to support\r\ninnovation and research policy directed to new technological trajectories\r\nhaving a potential of accelerated growth and positive social impact for\r\ndiagnosis and treatments of cancer.\r\n\",\"['Mario Coccia', 'Saeed Roshani', 'Melika Mosleh']\"\r\nhttp://arxiv.org/abs/1612.09478v1,Cancer vaccine,2016-12-30T12:56:52Z,2016-12-30T12:56:52Z,Discovery of cancer common and specific driver gene sets,\"  Cancer is known as a disease mainly caused by gene alterations. Discovery of\r\nmutated driver pathways or gene sets is becoming an important step to\r\nunderstand molecular mechanisms of carcinogenesis. However, systematically\r\ninvestigating commonalities and specificities of driver gene sets among\r\nmultiple cancer types is still a great challenge, but this investigation will\r\nundoubtedly benefit deciphering cancers and will be helpful for personalized\r\ntherapy and precision medicine in cancer treatment. In this study, we propose\r\ntwo optimization models to \\emph{de novo} discover common driver gene sets\r\namong multiple cancer types (ComMDP) and specific driver gene sets of one\r\ncertain or multiple cancer types to other cancers (SpeMDP), respectively. We\r\nfirst apply ComMDP and SpeMDP to simulated data to validate their efficiency.\r\nThen, we further apply these methods to 12 cancer types from The Cancer Genome\r\nAtlas (TCGA) and obtain several biologically meaningful driver pathways. As\r\nexamples, we construct a common cancer pathway model for BRCA and OV, infer a\r\ncomplex driver pathway model for BRCA carcinogenesis based on common driver\r\ngene sets of BRCA with eight cancer types, and investigate specific driver\r\npathways of the liquid cancer lymphoblastic acute myeloid leukemia (LAML)\r\nversus other solid cancer types. In these processes more candidate cancer genes\r\nare also found.\r\n\",\"['Junhua Zhang', 'Shihua Zhang']\"\r\nhttp://arxiv.org/abs/1110.5865v1,Cancer vaccine,2011-10-26T18:07:37Z,2011-10-26T18:07:37Z,\"Cancer Networks: A general theoretical and computational framework for\r\n  understanding cancer\",\"  We present a general computational theory of cancer and its developmental\r\ndynamics. The theory is based on a theory of the architecture and function of\r\ndevelopmental control networks which guide the formation of multicellular\r\norganisms. Cancer networks are special cases of developmental control networks.\r\nCancer results from transformations of normal developmental networks. Our\r\ntheory generates a natural classification of all possible cancers based on\r\ntheir network architecture. Each cancer network has a unique topology and\r\nsemantics and developmental dynamics that result in distinct clinical tumor\r\nphenotypes. We apply this new theory with a series of proof of concept cases\r\nfor all the basic cancer types. These cases have been computationally modeled,\r\ntheir behavior simulated and mathematically described using a multicellular\r\nsystems biology approach. There are fascinating correspondences between the\r\ndynamic developmental phenotype of computationally modeled {\\em in silico}\r\ncancers and natural {\\em in vivo} cancers. The theory lays the foundation for a\r\nnew research paradigm for understanding and investigating cancer. The theory of\r\ncancer networks implies that new diagnostic methods and new treatments to cure\r\ncancer will become possible.\r\n\",['Eric Werner']\r\nhttp://arxiv.org/abs/2303.09141v1,Cancer vaccine,2023-03-16T08:09:49Z,2023-03-16T08:09:49Z,On a fundamental problem in the analysis of cancer registry data,\"  In epidemiology research with cancer registry data, it is often of primary\r\ninterest to make inference on cancer death, not overall survival. Since cause\r\nof death is not easy to collect or is not necessarily reliable in cancer\r\nregistries, some special methodologies have been introduced and widely used by\r\nusing the concepts of the relative survival ratio and the net survival. In\r\nmaking inference of those measures, external life tables of the general\r\npopulation are utilized to adjust the impact of non-cancer death on overall\r\nsurvival. The validity of this adjustment relies on the assumption that\r\nmortality in the external life table approximates non-cancer mortality of\r\ncancer patients. However, the population used to calculate a life table may\r\ninclude cancer death and cancer patients. Sensitivity analysis proposed by\r\nTalb\\\"\"{a}ck and Dickman to address it requires additional information which is\r\noften not easily available. We propose a method to make inference on the net\r\nsurvival accounting for potential presence of cancer patients and cancer death\r\nin the life table for the general population. The idea of adjustment is to\r\nconsider correspondence of cancer mortality in the life table and that in the\r\ncancer registry. We realize a novel method to adjust cancer mortality in the\r\ncancer registry without any additional information to the standard analyses of\r\ncancer registries. Our simulation study revealed that the proposed method\r\nsuccessfully removed the bias. We illustrate the proposed method with the\r\ncancer registry data in England.\r\n\",\"['Sho Komukai', 'Satoshi Hattori', 'Bernard Rachet']\"\r\nhttp://arxiv.org/abs/1409.3263v1,Cancer vaccine,2014-09-10T21:45:52Z,2014-09-10T21:45:52Z,\"Understanding genomic alterations in cancer genomes using an integrative\r\n  network approach\",\"  In recent years, cancer genome sequencing and other high-throughput studies\r\nof cancer genomes have generated many notable discoveries. In this review,\r\nNovel genomic alteration mechanisms, such as chromothripsis (chromosomal\r\ncrisis) and kataegis (mutation storms), and their implications for cancer are\r\ndiscussed. Genomic alterations spur cancer genome evolution. Thus, the\r\nrelationship between cancer clonal evolution and cancer stems cells is\r\ncommented. The key question in cancer biology concerns how these genomic\r\nalterations support cancer development and metastasis in the context of\r\nbiological functioning. Thus far, efforts such as pathway analysis have\r\nimproved the understanding of the functional contributions of genetic mutations\r\nand DNA copy number variations to cancer development, progression and\r\nmetastasis. However, the known pathways correspond to a small fraction,\r\nplausibly 5-10%, of somatic mutations and genes with an altered copy number. To\r\ndevelop a comprehensive understanding of the function of these genomic\r\nalterations in cancer, an integrative network framework is proposed and\r\ndiscussed. Finally, the challenges and the directions of studying cancer omic\r\ndata using an integrative network approach are commented.\r\n\",['Edwin Wang']\r\nhttp://arxiv.org/abs/1408.2009v1,Cancer vaccine,2014-08-09T02:46:28Z,2014-08-09T02:46:28Z,\"Predictive genomics: A cancer hallmark network framework for predicting\r\n  tumor clinical phenotypes using genome sequencing data\",\"  We discuss a cancer hallmark network framework for modelling\r\ngenome-sequencing data to predict cancer clonal evolution and associated\r\nclinical phenotypes. Strategies of using this framework in conjunction with\r\ngenome sequencing data in an attempt to predict personalized drug targets, drug\r\nresistance, and metastasis for a cancer patient, as well as cancer risks for a\r\nhealthy individual are discussed. Accurate prediction of cancer clonal\r\nevolution and clinical phenotypes will have substantial impact on timely\r\ndiagnosis, personalized management and prevention of cancer.\r\n\",\"['Edwin Wang', 'Naif Zaman', 'Shauna Mcgee', 'Jean-Sbastien Milanese', 'Ali Masoudi-Nejad', \"\"Maureen O'Connor\"\"]\"\r\nhttp://arxiv.org/abs/1910.03934v1,Cancer vaccine,2019-10-08T05:42:36Z,2019-10-08T05:42:36Z,Noncoding RNAs serve as the deadliest regulators for cancer,\"  Cancer is one of the leading causes of human death. Many efforts have made to\r\nunderstand its mechanism and have further identified many proteins and DNA\r\nsequence variations as suspected targets for therapy. However, drugs targeting\r\nthese targets have low success rates, suggesting the basic mechanism still\r\nremains unclear. Here, we develop a computational software combining Cox\r\nproportional-hazards model and stability-selection to unearth an overlooked,\r\nyet the most important cancer drivers hidden in massive data from The Cancer\r\nGenome Atlas (TCGA), including 11,574 RNAseq samples and clinic data.\r\nGenerally, noncoding RNAs primarily regulate cancer deaths and work as the\r\ndeadliest cancer inducers and repressors, in contrast to proteins as\r\nconventionally thought. Especially, processed-pseudogenes serve as the primary\r\ncancer inducers, while lincRNA and antisense RNAs dominate the repressors.\r\nStrikingly, noncoding RNAs serves as the universal strongest regulators for all\r\ncancer types although personal clinic variables such as alcohol and smoking\r\nsignificantly alter cancer genome. Furthermore, noncoding RNAs also work as\r\ncentral hubs in cancer regulatory network and as biomarkers to discriminate\r\ncancer types. Therefore, noncoding RNAs overall serve as the deadliest cancer\r\nregulators, which refreshes the basic concept of cancer mechanism and builds a\r\nnovel basis for cancer research and therapy. Biological functions of\r\npseudogenes have rarely been recognized. Here we reveal them as the most\r\nimportant cancer drivers for all cancer types from big data, breaking a wall to\r\nexplore their biological potentials.\r\n\",\"['Anyou Wang', 'Hai Rong']\"\r\nhttp://arxiv.org/abs/2007.00887v1,Cancer vaccine,2020-07-02T05:18:08Z,2020-07-02T05:18:08Z,Computational methods for cancer driver discovery: A survey,\"  Motivation: Uncovering the genomic causes of cancer, known as cancer driver\r\ngenes, is a fundamental task in biomedical research. Cancer driver genes drive\r\nthe development and progression of cancer, thus identifying cancer driver genes\r\nand their regulatory mechanism is crucial to the design of cancer treatment and\r\nintervention. Many computational methods, which take the advantages of computer\r\nscience and data science, have been developed to utilise multiple types of\r\ngenomic data to reveal cancer drivers and their regulatory mechanism behind\r\ncancer development and progression. Due to the complexity of the mechanistic\r\ninsight of cancer genes in driving cancer and the fast development of the\r\nfield, it is necessary to have a comprehensive review about the current\r\ncomputational methods for discovering different types of cancer drivers.\r\nResults: We survey computational methods for identifying cancer drivers from\r\ngenomic data. We categorise the methods into three groups, methods for single\r\ndriver identification, methods for driver module identification, and methods\r\nfor identifying personalised cancer drivers. We also conduct a case study to\r\ncompare the performance of the current methods. We further analyse the\r\nadvantages and limitations of the current methods, and discuss the challenges\r\nand future directions of the topic. In addition, we investigate the resources\r\nfor discovering and validating cancer drivers in order to provide a one-stop\r\nreference of the tools to facilitate cancer driver discovery. The ultimate goal\r\nof the paper is to help those interested in the topic to establish a solid\r\nbackground to carry out further research in the field.\r\n\",\"['Vu Viet Hoang Pham', 'Lin Liu', 'Cameron Bracken', 'Gregory Goodall', 'Jiuyong Li', 'Thuc Duy Le']\"\r\nhttp://arxiv.org/abs/1205.1923v1,Cancer vaccine,2012-05-09T09:37:52Z,2012-05-09T09:37:52Z,\"Using data mining techniques for diagnosis and prognosis of cancer\r\n  disease\",\"  Breast cancer is one of the leading cancers for women in developed countries\r\nincluding India. It is the second most common cause of cancer death in women.\r\nThe high incidence of breast cancer in women has increased significantly in the\r\nlast years. In this paper we have discussed various data mining approaches that\r\nhave been utilized for breast cancer diagnosis and prognosis. Breast Cancer\r\nDiagnosis is distinguishing of benign from malignant breast lumps and Breast\r\nCancer Prognosis predicts when Breast Cancer is to recur in patients that have\r\nhad their cancers excised. This study paper summarizes various review and\r\ntechnical articles on breast cancer diagnosis and prognosis also we focus on\r\ncurrent research being carried out using the data mining techniques to enhance\r\nthe breast cancer diagnosis and prognosis.\r\n\",['Shweta Kharya']\r\nhttp://arxiv.org/abs/2310.16650v1,Cancer vaccine,2023-10-25T13:59:36Z,2023-10-25T13:59:36Z,\"Data-integration with pseudoweights and survey-calibration: application\r\n  to developing US-representative lung cancer risk models for use in screening\",\"  Accurate cancer risk estimation is crucial to clinical decision-making, such\r\nas identifying high-risk people for screening. However, most existing cancer\r\nrisk models incorporate data from epidemiologic studies, which usually cannot\r\nrepresent the target population. While population-based health surveys are\r\nideal for making inference to the target population, they typically do not\r\ncollect time-to-cancer incidence data. Instead, time-to-cancer specific\r\nmortality is often readily available on surveys via linkage to vital\r\nstatistics. We develop calibrated pseudoweighting methods that integrate\r\nindividual-level data from a cohort and a survey, and summary statistics of\r\ncancer incidence from national cancer registries. By leveraging\r\nindividual-level cancer mortality data in the survey, the proposed methods\r\nimpute time-to-cancer incidence for survey sample individuals and use survey\r\ncalibration with auxiliary variables of influence functions generated from Cox\r\nregression to improve robustness and efficiency of the inverse-propensity\r\npseudoweighting method in estimating pure risks. We develop a lung cancer\r\nincidence pure risk model from the Prostate, Lung, Colorectal, and Ovarian\r\n(PLCO) Cancer Screening Trial using our proposed methods by integrating data\r\nfrom the National Health Interview Survey (NHIS) and cancer registries.\r\n\",\"['Lingxiao Wang', 'Yan Li', 'Barry Graubard', 'Hormuzd Katki']\"\r\nhttp://arxiv.org/abs/2405.05643v1,Cancer vaccine,2024-05-09T09:30:54Z,2024-05-09T09:30:54Z,\"Cancer mortality projection: disparities, COVID-19, and late diagnosis\r\n  impact\",\"  This paper investigates projection of two major causes of cancer mortality,\r\nbreast cancer and lung cancer, by using a Bayesian modelling framework. We\r\ninvestigate patterns in 2001-2018 (as baseline) in cause-specific cancer\r\nmortality and project these by year of death and various risk factors: age,\r\ngender, regions of England, income deprivation quintile, average\r\nage-at-diagnosis, and non-smoker prevalence rates. We then assess excess cancer\r\nmortality during the COVID-19 pandemic years, and we examine the impact of\r\ndiagnosis delays on lung cancer mortality across various scenarios. Our\r\nfindings indicate that socio-economic disparities in lung cancer mortality will\r\npersist in the future. Additionally, we observe slight variations in breast\r\ncancer mortality across different regions up to 2036. Furthermore, marginal\r\nincreases in excess deaths from lung and breast cancer are estimated in\r\nspecific regions of England throughout the pandemic year (2020-2022),\r\ncontrasting with the national trend. However, the excess lung cancer deaths\r\nmarkedly differ by age, region and deprivation as a result of delays in cancer\r\ndiagnosis. Specifically, we find a notably higher number of excess deaths in\r\nthe northern regions of England compared to the southern regions, as well as\r\namong individuals living in the most deprived areas compared to those in the\r\nleast deprived areas.\r\n\",\"['A. Arik', 'A. J. G. Cairns', 'G. Streftaris']\"\r\nhttp://arxiv.org/abs/2407.19330v1,Cancer vaccine,2024-07-27T19:53:41Z,2024-07-27T19:53:41Z,\"Mapping Cancer Stem Cell Markers Distribution:A Hypergraph Analysis\r\n  Across Organs\",\"  This study presents an interdisciplinary approach to analyse the distribution\r\nof cancer stem cell markers (CSCMs) across various cancer-affected organs using\r\nhypergraphs. Cancer stem cells (CSCs) play a crucial role in cancer initiation,\r\nprogression, and metastasis. By employing hypergraphs, we model the\r\nrelationships between CSCM locations and cancerous organs, providing a\r\ncomprehensive representation of these interactions. Initially, we utilised an\r\nunweighted incidence matrix and its Markov transition matrices to gain a\r\ndynamic perspective on CSCM distributions. This method allows us to observe how\r\nthese markers spread and influence cancer progression in a dynamical context.\r\nBy calculating mutual information for each node and hyperedge, our analysis\r\nuncovers complex interaction patterns between CSCMs and organs, highlighting\r\nthe critical roles of certain markers in cancer progression and metastasis. Our\r\napproach offers a detailed representation of cancer stem cell networks,\r\nenhancing our understanding of the mechanisms driving cancer heterogeneity and\r\nmetastasis. By integrating hypergraph theory with cancer biology, this study\r\nprovides valuable insights for developing targeted cancer therapies.\r\n\",\"['David H. Margarit', 'Gustavo Paccosi', 'Marcela V. Reale', 'Lilia M. Romanelli']\"\r\nhttp://arxiv.org/abs/1112.1510v2,Cancer vaccine,2011-12-07T10:11:20Z,2012-10-11T06:46:23Z,\"An approach for the identification of targets specific to bone\r\n  metastasis using cancer genes interactome and gene ontology analysis\",\"  Metastasis is one of the most enigmatic aspects of cancer pathogenesis and is\r\na major cause of cancer-associated mortality. Secondary bone cancer (SBC) is a\r\ncomplex disease caused by metastasis of tumor cells from their primary site and\r\nis characterized by intricate interplay of molecular interactions.\r\nIdentification of targets for multifactorial diseases such as SBC, the most\r\nfrequent complication of breast and prostate cancers, is a challenge. Towards\r\nachieving our aim of identification of targets specific to SBC, we constructed\r\na 'Cancer Genes Network', a representative protein interactome of cancer genes.\r\nUsing graph theoretical methods, we obtained a set of key genes that are\r\nrelevant for generic mechanisms of cancers and have a role in biological\r\nessentiality. We also compiled a curated dataset of 391 SBC genes from\r\npublished literature which serves as a basis of ontological correlates of\r\nsecondary bone cancer. Building on these results, we implement a strategy based\r\non generic cancer genes, SBC genes and gene ontology enrichment method, to\r\nobtain a set of targets that are specific to bone metastasis. Through this\r\nstudy, we present an approach for probing one of the major complications in\r\ncancers, namely, metastasis. The results on genes that play generic roles in\r\ncancer phenotype, obtained by network analysis of 'Cancer Genes Network', have\r\nbroader implications in understanding the role of molecular regulators in\r\nmechanisms of cancers. Specifically, our study provides a set of potential\r\ntargets that are of ontological and regulatory relevance to secondary bone\r\ncancer.\r\n\",\"['Shikha Vashisht', 'Ganesh Bagler']\"\r\nhttp://arxiv.org/abs/2110.12057v1,Cancer vaccine,2021-10-21T05:32:36Z,2021-10-21T05:32:36Z,\"Predictive factors associated with survival rate of cervical cancer\r\n  patients in Brunei Darussalam\",\"  Introduction: Cervical cancer is the third most prevalent cancer among women\r\nin Brunei Darussalam. This study aims to report the overall survival rates and\r\nassociated factors of patients diagnosed with malignant cervical cancer in\r\nBrunei Darussalam. Methods: A retrospective study of patients diagnosed with\r\ncervical cancer from 2007 to 2017 in Brunei Darussalam. The data were obtained\r\nfrom the population-based cancer registry in Brunei Darussalam. Kaplan- Meier\r\nsurvival analysis was used to estimate the overall survival rates at 1-, 3- and\r\n5-year intervals while the log-rank test was used to assess differences in\r\nsurvival between groups. Cox Proportional Hazard (PH) regression analysis was\r\nused to examine the association of demographic and clinical factors on the\r\nsurvival of cervical cancer patients. Results: A total of 329 registered\r\nmalignant cervical cancer cases were analyzed. The mean age at diagnosis of\r\npatients with cervical cancer was 46.7 years. There were 28.6% deaths and the\r\noverall survival rates at 1, 3 and 5 years were 85.4%, 72.6% and 68.6%\r\nrespectively. Age at diagnosis, cancer stage and histology types were\r\nsignificant predictive factors for overall survival of the patients diagnosed\r\nwith cervical cancers when analysed on both log rank tests and Cox PH model.\r\nConclusion: Age at diagnosis, cancer stage and histology types were\r\nsignificantly associated with the overall survival rates of cervical cancer\r\npatients in Brunei Darussalam. Early detection and management of cervical\r\ncancer at early stages should be prioritized to improve the survival rate and\r\nquality of cancer care.\r\n\",\"['Fadhliah Madli', 'Elvynna Leong', 'Sok King Ong', 'Edwin Lim', 'Khairul Amilin Tengah']\"\r\nhttp://arxiv.org/abs/2308.02528v1,Cancer vaccine,2023-07-31T16:28:42Z,2023-07-31T16:28:42Z,A comprehensive review of deep learning in lung cancer,\"  To provide the reader with a historical perspective on cancer classification\r\napproaches, we first discuss the fundamentals of the area of cancer diagnosis\r\nin this article, including the processes of cancer diagnosis and the standard\r\nclassification methods employed by clinicians. Current methods for cancer\r\ndiagnosis are deemed ineffective, calling for new and more intelligent\r\napproaches.\r\n\",['Farzane Tajidini']\r\nhttp://arxiv.org/abs/1711.09015v1,Cancer vaccine,2017-11-24T15:18:52Z,2017-11-24T15:18:52Z,\"The cellular ROS-scavenging function, a key factor determining the\r\n  specific vulnerability of cancer cells to cold atmospheric plasma in vitro\",\"  Cold atmospheric plasma (CAP) has shown its promising application in cancer\r\ntreatment both in vitro and in vivo. However, the anti-cancer mechanism is\r\nstill largely unknown. CAP may kill cancer cells via triggering the rise of\r\nintracellular ROS, DNA damage, mitochondrial damage, or cellular membrane\r\ndamage. While, the specific vulnerability of cancer cells to CAP has been\r\nobserved, the underlying mechanism of such cell-based specific vulnerability to\r\nCAP is completely unknown. Here, through the comparison of CAP treatment and\r\nH2O2 treatment on 10 different cancer cell lines in vitro, we observed that the\r\nH2O2 consumption speed by cancer cells was strongly correlated to the\r\ncytotoxicity of CAP treatment on cancer cells. Cancer cells that clear\r\nextracellular H2O2 more quickly are more resistant to the cytotoxicity of CAP\r\ntreatment. This finding strongly indicates that the anti-oxidant system in\r\ncancer cells play a key role in the specific vulnerability of cancer cells to\r\nCAP treatment in vitro.\r\n\",\"['Dayun Yan', 'Jonathan H. Sherman', 'Jerome Canady', 'Barry Trink', 'Michael Keidar']\"\r\nhttp://arxiv.org/abs/1812.09203v1,Cancer vaccine,2018-12-21T15:42:00Z,2018-12-21T15:42:00Z,Pan-Cancer Epigenetic Biomarker Selection from Blood Samples Using SAS,\"  A key focus in current cancer research is the discovery of cancer biomarkers\r\nthat allow earlier detection with high accuracy and lower costs for both\r\npatients and hospitals. Blood samples have long been used as a health status\r\nindicator, but DNA methylation signatures in blood have not been fully\r\nappreciated in cancer research. Historically, analysis of cancer has been\r\nconducted directly with the patient's tumor or related tissues. Such analyses\r\nallow physicians to diagnose a patient's health and cancer status; however,\r\nphysicians must observe certain symptoms that prompt them to use biopsies or\r\nimaging to verify the diagnosis. This is a post-hoc approach. Our study will\r\nfocus on epigenetic information for cancer detection, specifically information\r\nabout DNA methylation in human peripheral blood samples in cancer discordant\r\nmonozygotic twin-pairs. This information might be able to help us detect cancer\r\nmuch earlier, before the first symptom appears. Several other types of\r\nepigenetic data can also be used, but here we demonstrate the potential of\r\nblood DNA methylation data as a biomarker for pan-cancer using SAS 9.3 and SAS\r\nEM. We report that 55 methylation CpG sites measurable in blood samples can be\r\nused as biomarkers for early cancer detection and classification.\r\n\",\"['Xi Chen', 'Jin Xie', 'Qingcong Yuan']\"\r\nhttp://arxiv.org/abs/1912.12379v1,Cancer vaccine,2019-12-28T01:07:36Z,2019-12-28T01:07:36Z,\"A Common Gene Expression Signature Analysis Method for Multiple Types of\r\n  Cancer\",\"  Mining gene expression profiles has proven valuable for identifying\r\nsignatures serving as surrogates of cancer phenotypes. However, the\r\nsimilarities of such signatures across different cancer types have not been\r\nstrong enough to conclude that they represent a universal biological mechanism\r\nshared among multiple cancer types. Here we describe a network-based approach\r\nthat explores gene-to-gene connections in multiple cancer datasets while\r\nmaximizing the overall association of the subnetwork with clinical outcomes.\r\nWith the dataset of The Cancer Genome Atlas (TCGA), we studied the\r\ncharacteristics of common gene expression of three types of cancers: Rectum\r\nadenocarcinoma (READ), Breast invasive carcinoma (BRCA) and Colon\r\nadenocarcinoma (COAD). By analyzing several pairs of highly correlated genes\r\nafter filtering and clustering work, we found that the co-expressed genes\r\nacross multiple types of cancers point to particular biological mechanisms\r\nrelated to cancer cell progression , suggesting that they represent important\r\nattributes of cancer in need of being elucidated for potential applications in\r\ndiagnostic, prognostic and therapeutic products applicable to multiple cancer\r\ntypes.\r\n\",\"['Yingcheng Sun', 'Xiangru Liang', 'Kenneth Loparo']\"\r\nhttp://arxiv.org/abs/2301.11126v1,Cancer vaccine,2023-01-24T18:06:52Z,2023-01-24T18:06:52Z,Three facets of mathematical cancer biology research,\"  Cancer, as the uncontrollable cell growth, is related to many branches of\r\nbiology. In this review, we will discuss three mathematical approaches for\r\nstudying cancer biology: population dynamics, gene regulation, and\r\ndevelopmental biology. If we understand all biochemical mechanisms of cancer\r\ncells, we can directly calculate how the cancer cell population behaves.\r\nInversely, just from the cell count data, we can use population dynamics to\r\ninfer the mechanisms. Cancer cells emerge from certain genetic mutations, which\r\naffect the expression of other genes through gene regulation. Therefore,\r\nknowledge of gene regulation can help with cancer prevention and treatment.\r\nDevelopmental biology studies acquisition and maintenance of normal cellular\r\nfunction, which is inspiring to cancer biology in the opposite direction.\r\nBesides, cancer cells implanted into an embryo can differentiate into normal\r\ntissues, which provides a possible approach of curing cancer. This review\r\nillustrates the role of mathematics in these three fields: what mathematical\r\nmodels are used, what data analysis tools are applied, and what mathematical\r\ntheorems need to be proved. We hope that applied mathematicians and even pure\r\nmathematicians can find meaningful mathematical problems related to cancer\r\nbiology.\r\n\",['Yue Wang']\r\nhttp://arxiv.org/abs/2302.02456v2,Cancer vaccine,2023-02-05T18:50:12Z,2023-02-15T23:36:32Z,Deep Learning Approach for Early Stage Lung Cancer Detection,\"  Lung cancer is the leading cause of death among different types of cancers.\r\nEvery year, the lives lost due to lung cancer exceed those lost to pancreatic,\r\nbreast, and prostate cancer combined. The survival rate for lung cancer\r\npatients is very low compared to other cancer patients due to late diagnostics.\r\nThus, early lung cancer diagnostics is crucial for patients to receive early\r\ntreatments, increasing the survival rate or even becoming cancer-free. This\r\npaper proposed a deep-learning model for early lung cancer prediction and\r\ndiagnosis from Computed Tomography (CT) scans. The proposed mode achieves high\r\naccuracy. In addition, it can be a beneficial tool to support radiologists'\r\ndecisions in predicting and detecting lung cancer and its stage.\r\n\",\"['Saleh Abunajm', 'Nelly Elsayed', 'Zag ElSayed', 'Murat Ozer']\"\r\nhttp://arxiv.org/abs/2412.11167v2,Cultured meat,2024-12-15T12:30:52Z,2025-02-16T12:21:29Z,Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette,\"  Large language models (LLMs) face challenges in aligning with diverse\r\ncultural values despite their remarkable performance in generation, which stems\r\nfrom inherent monocultural biases and difficulties in capturing nuanced\r\ncultural semantics. Existing methods struggle to adapt to unkown culture after\r\nfine-tuning. Inspired by cultural geography across five continents, we propose\r\nCultural Palette, a multi-agent framework that redefines cultural alignment as\r\nan adaptive \"\"color-blending\"\" process for country-specific adaptation. Our\r\napproach harnesses cultural geography across five continents (Africa, America,\r\nAsia, Europe, Oceania) through three key steps: First, we synthesize the\r\nPentachromatic Cultural Palette Dataset using GPT-4o, refining\r\ncontinental-level dialogues with Hofstede cultural dimensions to establish\r\nfoundational cultural representations. Second, five continent-level alignment\r\nagents form specialized cultural communities that generate region-specific\r\ndraft responses. Third, a Meta Agent employs Cultural MoErges to dynamically\r\nblend these cultural \"\"colors\"\" through attention-gated parameter merging, akin\r\nto mixing pigments on a palette, resolving conflicts while preserving cultural\r\nnuances to produce the final culturally-aligned response. Extensive experiments\r\nacross various countries demonstrate that Cultural Palette surpasses existing\r\nbaselines in cultural alignment.\r\n\",\"['Jiahao Yuan', 'Zixiang Di', 'Shangzixin Zhao', 'Usman Naseem']\"\r\nhttp://arxiv.org/abs/2410.12971v1,Cultured meat,2024-10-16T19:06:08Z,2024-10-16T19:06:08Z,Self-Pluralising Culture Alignment for Large Language Models,\"  As large language models (LLMs) become increasingly accessible in many\r\ncountries, it is essential to align them to serve pluralistic human values\r\nacross cultures. However, pluralistic culture alignment in LLMs remain an open\r\nproblem. In this paper, we propose CultureSPA, a Self-Pluralising Culture\r\nAlignment framework that allows LLMs to simultaneously align to pluralistic\r\ncultures. The framework first generates questions on various culture topics,\r\nthen yields LLM outputs in response to these generated questions under both\r\nculture-aware and culture-unaware settings. By comparing culture-aware/unaware\r\noutputs, we are able to detect and collect culture-related instances. These\r\ninstances are employed to fine-tune LLMs to serve pluralistic cultures in\r\neither a culture-joint or culture-specific way. Extensive experiments\r\ndemonstrate that CultureSPA significantly improves the alignment of LLMs to\r\ndiverse cultures without compromising general abilities. And further\r\nimprovements can be achieved if CultureSPA is combined with advanced prompt\r\nengineering techniques. Comparisons between culture-joint and culture-specific\r\ntuning strategies, along with variations in data quality and quantity,\r\nillustrate the robustness of our method. We also explore the mechanisms\r\nunderlying CultureSPA and the relations between different cultures it reflects.\r\n\",\"['Shaoyang Xu', 'Yongqi Leng', 'Linhao Yu', 'Deyi Xiong']\"\r\nhttp://arxiv.org/abs/2310.06458v2,Cultured meat,2023-10-10T09:29:38Z,2024-09-02T02:26:18Z,\"Cultural Compass: Predicting Transfer Learning Success in Offensive\r\n  Language Detection with Cultural Features\",\"  The increasing ubiquity of language technology necessitates a shift towards\r\nconsidering cultural diversity in the machine learning realm, particularly for\r\nsubjective tasks that rely heavily on cultural nuances, such as Offensive\r\nLanguage Detection (OLD). Current understanding underscores that these tasks\r\nare substantially influenced by cultural values, however, a notable gap exists\r\nin determining if cultural features can accurately predict the success of\r\ncross-cultural transfer learning for such subjective tasks. Addressing this,\r\nour study delves into the intersection of cultural features and transfer\r\nlearning effectiveness. The findings reveal that cultural value surveys indeed\r\npossess a predictive power for cross-cultural transfer learning success in OLD\r\ntasks and that it can be further improved using offensive word distance. Based\r\non these results, we advocate for the integration of cultural information into\r\ndatasets. Additionally, we recommend leveraging data sources rich in cultural\r\ninformation, such as surveys, to enhance cultural adaptability. Our research\r\nsignifies a step forward in the quest for more inclusive, culturally sensitive\r\nlanguage technologies.\r\n\",\"['Li Zhou', 'Antonia Karamolegkou', 'Wenyu Chen', 'Daniel Hershcovich']\"\r\nhttp://arxiv.org/abs/2504.08820v1,Cultured meat,2025-04-09T13:40:13Z,2025-04-09T13:40:13Z,\"CAReDiO: Cultural Alignment of LLM via Representativeness and\r\n  Distinctiveness Guided Data Optimization\",\"  As Large Language Models (LLMs) more deeply integrate into human life across\r\nvarious regions, aligning them with pluralistic cultures is crucial for\r\nimproving user experience and mitigating cultural conflicts. Existing\r\napproaches develop culturally aligned LLMs primarily through fine-tuning with\r\nmassive carefully curated culture-specific corpora. Nevertheless, inspired by\r\nculture theories, we identify two key challenges faced by these datasets: (1)\r\nRepresentativeness: These corpora fail to fully capture the target culture's\r\ncore characteristics with redundancy, causing computation waste; (2)\r\nDistinctiveness: They struggle to distinguish the unique nuances of a given\r\nculture from shared patterns across other relevant ones, hindering precise\r\ncultural modeling. To handle these challenges, we introduce CAReDiO, a novel\r\ncultural data construction framework. Specifically, CAReDiO utilizes powerful\r\nLLMs to automatically generate cultural conversation data, where both the\r\nqueries and responses are further optimized by maximizing representativeness\r\nand distinctiveness. Using CAReDiO, we construct a small yet effective dataset,\r\ncovering five cultures, and compare it with several recent cultural corpora.\r\nExtensive experiments demonstrate that our method generates more effective data\r\nand enables cultural alignment with as few as 100 training samples, enhancing\r\nboth performance and efficiency.\r\n\",\"['Jing Yao', 'Xiaoyuan Yi', 'Jindong Wang', 'Zhicheng Dou', 'Xing Xie']\"\r\nhttp://arxiv.org/abs/2007.02359v5,Cultured meat,2020-07-05T15:06:50Z,2022-09-15T20:00:11Z,\"Cultures as networks of cultural traits: A unifying framework for\r\n  measuring culture and cultural distances\",\"  Making use of the information from the World Value Survey (WVS), and\r\noperationalizing a definition of national culture that encompasses both the\r\nrelevance of specific cultural traits and the interdependence among them, this\r\npaper proposes a methodology to reveal the latent structure of national culture\r\nand to measure cultural distance between countries that takes into account both\r\nthe difference in cultural traits and the difference in the network structure\r\nof national cultures. Exploiting the possibilities offered by copula graphical\r\nmodels for discrete data, this paper infers the cultural networks of all the\r\ncountries included in the WVS (Wave 6) and proposes a novel unifying framework\r\nto measure national culture and international cultural distances. The Jeffreys'\r\ndivergence between copula graphical models, taken as the measure of cultural\r\ndistance between countries, captures the orthogonality of the two components of\r\ncultural distance: the one based on cultural traits and the one based on the\r\nnetwork structure among them. Moreover, the two components are shown to\r\ncorrelate with different national and structural characteristics of cultural\r\nnetworks, thus encompassing the different informational sets related to\r\nnational cultures.\r\n\",\"['Luca De Benedictis', 'Roberto Rondinelli', 'Veronica Vinciotti']\"\r\nhttp://arxiv.org/abs/2309.12342v2,Cultured meat,2023-08-25T14:50:13Z,2024-05-08T14:48:39Z,\"Cultural Alignment in Large Language Models: An Explanatory Analysis\r\n  Based on Hofstede's Cultural Dimensions\",\"  The deployment of large language models (LLMs) raises concerns regarding\r\ntheir cultural misalignment and potential ramifications on individuals and\r\nsocieties with diverse cultural backgrounds. While the discourse has focused\r\nmainly on political and social biases, our research proposes a Cultural\r\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\r\ncultural dimension framework, which offers an explanatory cross-cultural\r\ncomparison through the latent variable analysis. We apply our approach to\r\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\r\ncultural dimensions of regions like the United States, China, and Arab\r\ncountries, using different prompting styles and exploring the effects of\r\nlanguage-specific fine-tuning on the models' behavioural tendencies and\r\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\r\nthe difference between LLMs in explanatory cultural dimensions. Our study\r\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\r\na unique capability to adapt to cultural nuances, particularly in Chinese\r\nsettings. However, it faces challenges with American and Arab cultures. The\r\nresearch also highlights that fine-tuning LLama 2 models with different\r\nlanguages changes their responses to cultural questions, emphasizing the need\r\nfor culturally diverse development in AI for worldwide acceptance and ethical\r\nuse. For more details or to contribute to this research, visit our GitHub page\r\nhttps://github.com/reemim/Hofstedes_CAT/\r\n\",\"['Reem I. Masoud', 'Ziquan Liu', 'Martin Ferianc', 'Philip Treleaven', 'Miguel Rodrigues']\"\r\nhttp://arxiv.org/abs/1805.09635v1,Cultured meat,2018-05-24T12:40:47Z,2018-05-24T12:40:47Z,When Cultures Meet: Modelling Cross-Cultural Knowledge Spaces,\"  Cross cultural research projects are becoming a norm in our global world.\r\nMore and more projects are being executed using teams from eastern and western\r\ncultures. Cultural competence might help project managers to achieve project\r\ngoals and avoid potential risks in cross cultural project environments and\r\nwould also support them to promote creativity and motivation through flexible\r\nleadership. In our paper we introduce an idea for constructing an information\r\nsystem, a cross cultural knowledge space, which could support cross cultural\r\ncommunication, collaborative learning experiences and time based project\r\nmanagement functions. The case cultures in our project are Finnish and\r\nJapanese. The system can be used both in virtual and in physical spaces for\r\nexample to clarify cultural business etiquette. The core of our system design\r\nwill be based on cross cultural ontology, and the system implementation on XML\r\ntechnologies. Our approach is a practical, step by step example of constructive\r\nresearch. In our paper we shortly describe Hofstede's dimensions for assessing\r\ncultures as one example of a larger framework for our study. We also discuss\r\nthe concept of time in cultural context.\r\n\",['Anneli Heimbrger']\r\nhttp://arxiv.org/abs/2211.07460v1,Cultured meat,2022-11-14T15:42:27Z,2022-11-14T15:42:27Z,\"An Analytics of Culture: Modeling Subjectivity, Scalability,\r\n  Contextuality, and Temporality\",\"  There is a bidirectional relationship between culture and AI; AI models are\r\nincreasingly used to analyse culture, thereby shaping our understanding of\r\nculture. On the other hand, the models are trained on collections of cultural\r\nartifacts thereby implicitly, and not always correctly, encoding expressions of\r\nculture. This creates a tension that both limits the use of AI for analysing\r\nculture and leads to problems in AI with respect to cultural complex issues\r\nsuch as bias.\r\n  One approach to overcome this tension is to more extensively take into\r\naccount the intricacies and complexities of culture. We structure our\r\ndiscussion using four concepts that guide humanistic inquiry into culture:\r\nsubjectivity, scalability, contextuality, and temporality. We focus on these\r\nconcepts because they have not yet been sufficiently represented in AI\r\nresearch. We believe that possible implementations of these aspects into AI\r\nresearch leads to AI that better captures the complexities of culture. In what\r\nfollows, we briefly describe these four concepts and their absence in AI\r\nresearch. For each concept, we define possible research challenges.\r\n\",\"['Nanne van Noord', 'Melvin Wevers', 'Tobias Blanke', 'Julia Noordegraaf', 'Marcel Worring']\"\r\nhttp://arxiv.org/abs/2310.01929v3,Cultured meat,2023-10-03T10:13:36Z,2024-08-13T08:11:49Z,\"Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of\r\n  Text-To-Image Models\",\"  Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have\r\ndemonstrated remarkable prompt-based image generation capabilities.\r\nMultilingual encoders may have a substantial impact on the cultural agency of\r\nthese models, as language is a conduit of culture. In this study, we explore\r\nthe cultural perception embedded in TTI models by characterizing culture across\r\nthree hierarchical tiers: cultural dimensions, cultural domains, and cultural\r\nconcepts. Based on this ontology, we derive prompt templates to unlock the\r\ncultural knowledge in TTI models, and propose a comprehensive suite of\r\nevaluation techniques, including intrinsic evaluations using the CLIP space,\r\nextrinsic evaluations with a Visual-Question-Answer (VQA) model and human\r\nassessments, to evaluate the cultural content of TTI-generated images. To\r\nbolster our research, we introduce the CulText2I dataset, derived from six\r\ndiverse TTI models and spanning ten languages. Our experiments provide insights\r\nregarding Do, What, Which and How research questions about the nature of\r\ncultural encoding in TTI models, paving the way for cross-cultural applications\r\nof these models.\r\n\",\"['Mor Ventura', 'Eyal Ben-David', 'Anna Korhonen', 'Roi Reichart']\"\r\nhttp://arxiv.org/abs/2406.14504v2,Cultured meat,2024-06-20T17:06:58Z,2024-10-14T15:39:36Z,Translating Across Cultures: LLMs for Intralingual Cultural Adaptation,\"  LLMs are increasingly being deployed for multilingual applications and have\r\ndemonstrated impressive translation capabilities between several low and\r\nhigh-resource languages. An aspect of translation that often gets overlooked is\r\nthat of cultural adaptation, or modifying source culture references to suit the\r\ntarget culture. While specialized translation models still outperform LLMs on\r\nthe machine translation task when viewed from the lens of correctness, they are\r\nnot sensitive to cultural differences often requiring manual correction. LLMs\r\non the other hand have a rich reservoir of cultural knowledge embedded within\r\nits parameters that can be potentially exploited for such applications. In this\r\npaper, we define the task of cultural adaptation and create an evaluation\r\nframework to evaluate the performance of modern LLMs for cultural adaptation\r\nand analyze their cross-cultural knowledge while connecting related concepts\r\nacross different cultures. We also analyze possible issues with automatic\r\nadaptation. We hope that this task will offer more insight into the cultural\r\nunderstanding of LLMs and their creativity in cross-cultural scenarios.\r\n\",\"['Pushpdeep Singh', 'Mayur Patidar', 'Lovekesh Vig']\"\r\nhttp://arxiv.org/abs/1002.1196v1,Cultured meat,2010-02-05T11:02:02Z,2010-02-05T11:02:02Z,Cultural commons and cultural evolution,\"  Culture evolves following a process that is akin to biological evolution,\r\nalthough with some significant differences. At the same time culture has often\r\na collective good value for human groups. This paper studies culture in an\r\nevolutionary perspective, with a focus on the implications of group definition\r\nfor the coexistence of different cultures. A model of cultural evolution is\r\npresented where agents interacts in an artificial environment. The belonging to\r\na specific memetic group is a major factor allowing agents to exploit different\r\nenvironmental niches with, as a result, the coexistence of different cultures\r\nin the same environment.\r\n\",['Giangiacomo Bravo']\r\nhttp://arxiv.org/abs/1810.03605v1,Cultured meat,2018-10-08T08:47:46Z,2018-10-08T08:47:46Z,\"Critical review of models, containing cultural levels beyond the\r\n  organizational one\",\"  The current article traces back the scientific interest to cultural levels\r\nacross the organization at the University of National and World Economy, and\r\nespecially in the series of Economic Alternatives - an official scientific\r\nmagazine, issued by this Institution. Further, a wider and critical review of\r\ninternational achievements in this field is performed, revealing diverse\r\nanalysis perspectives with respect to cultural levels. Also, a useful model of\r\nexploring and teaching the cultural levels beyond the organization is proposed.\r\n  Keywords: globalization, national culture, organization culture, cultural\r\nlevels, cultural economics. JEL: M14, Z10.\r\n\",['Kiril Dimitrov']\r\nhttp://arxiv.org/abs/2211.15271v2,Cultured meat,2022-11-28T12:54:34Z,2022-11-29T11:22:38Z,The Myth of Culturally Agnostic AI Models,\"  The paper discusses the potential of large vision-language models as objects\r\nof interest for empirical cultural studies. Focusing on the comparative\r\nanalysis of outputs from two popular text-to-image synthesis models, DALL-E 2\r\nand Stable Diffusion, the paper tries to tackle the pros and cons of striving\r\ntowards culturally agnostic vs. culturally specific AI models. The paper\r\ndiscusses several examples of memorization and bias in generated outputs which\r\nshowcase the trade-off between risk mitigation and cultural specificity, as\r\nwell as the overall impossibility of developing culturally agnostic models.\r\n\",['Eva Cetinic']\r\nhttp://arxiv.org/abs/2311.14367v2,Cultured meat,2023-11-24T09:23:13Z,2023-12-21T11:38:28Z,\"Joint modelling of national cultures accounting for within and\r\n  between-country heterogeneity\",\"  Cultural values vary significantly around the world. Despite a large\r\nheterogeneity, similarities across national cultures are present. This paper\r\nstudies cross-country culture heterogeneity via the joint inference of\r\ncountry-specific copula graphical models from world-wide survey data. To this\r\nend, a random graph generative model of the cultural networks is introduced,\r\nwith a latent space and proximity measures that embed cultural relatedness\r\nacross countries. Within-country heterogeneity is also accounted for, via\r\nparametric modelling of the marginal distributions of each cultural trait. All\r\ntogether, the different components of the model are able to identify several\r\ndimensions of culture.\r\n\",\"['Veronica Vinciotti', 'Luca De Benedictis', 'Ernst C. Wit']\"\r\nhttp://arxiv.org/abs/2409.07475v1,Cultured meat,2024-08-28T10:24:54Z,2024-08-28T10:24:54Z,\"Cross-Cultural Communication in the Digital Age: An Analysis of Cultural\r\n  Representation and Inclusivity in Emojis\",\"  Emojis have become a universal language in the digital world, enabling users\r\nto express emotions, ideas, and identities across diverse cultural contexts. As\r\nemojis incorporate more cultural symbols and diverse representations, they play\r\na crucial role in cross-cultural communication. This research project aims to\r\nanalyze the representation of different cultures in emojis, investigate how\r\nemojis facilitate cross-cultural communication and promote inclusivity, and\r\nexplore the impact of emojis on understanding and interpretation in different\r\ncultural contexts.\r\n\",\"['Lingfeng Li', 'Xiangwen Zheng']\"\r\nhttp://arxiv.org/abs/2503.16520v1,Cultured meat,2025-03-17T01:23:57Z,2025-03-17T01:23:57Z,\"Not All Personas Are Worth It: Culture-Reflective Persona Data\r\n  Augmentation\",\"  Incorporating personas into conversational AI models is crucial for achieving\r\nauthentic and engaging interactions. However, the cultural diversity and\r\nadaptability of existing persona datasets is often overlooked, reducing their\r\nefficacy in building culturally aware AI systems. To address this issue, we\r\npropose a two-step pipeline for generating culture-specific personas and\r\nintroduce KoPersona, a dataset comprising 200,000 personas designed to capture\r\nKorean cultural values, behaviors, and social nuances. A comprehensive\r\nevaluation through various metrics validates the quality of KoPersona and its\r\nrelevance to Korean culture. This work not only contributes to persona-based\r\nresearch, but also establishes a scalable approach for creating culturally\r\nrelevant personas adaptable to various languages and cultural contexts.\r\n\",\"['Ji-Eun Han', 'Yoonseok Heo']\"\r\nhttp://arxiv.org/abs/2405.15145v3,Cultured meat,2024-05-24T01:49:02Z,2024-11-21T10:52:29Z,\"CulturePark: Boosting Cross-cultural Understanding in Large Language\r\n  Models\",\"  Cultural bias is pervasive in many large language models (LLMs), largely due\r\nto the deficiency of data representative of different cultures. Typically,\r\ncultural datasets and benchmarks are constructed either by extracting subsets\r\nof existing datasets or by aggregating from platforms such as Wikipedia and\r\nsocial media. However, these approaches are highly dependent on real-world data\r\nand human annotations, making them costly and difficult to scale. Inspired by\r\ncognitive theories on social communication, this paper introduces CulturePark,\r\nan LLM-powered multi-agent communication framework for cultural data\r\ncollection. CulturePark simulates cross-cultural human communication with\r\nLLM-based agents playing roles in different cultures. It generates high-quality\r\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\r\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\r\nculture-specific LLMs. We evaluated these models across three downstream tasks:\r\ncontent moderation, cultural alignment, and cultural education. Results show\r\nthat for content moderation, our GPT-3.5-based models either match or\r\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\r\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\r\nhuman participants, our models demonstrate superior outcomes in both learning\r\nefficacy and user experience compared to GPT-4. CulturePark proves an important\r\nstep in addressing cultural bias and advancing the democratization of AI,\r\nhighlighting the critical role of culturally inclusive data in model training.\r\nCode is released at https://github.com/Scarelette/CulturePark.\r\n\",\"['Cheng Li', 'Damien Teney', 'Linyi Yang', 'Qingsong Wen', 'Xing Xie', 'Jindong Wang']\"\r\nhttp://arxiv.org/abs/2408.05102v1,Cultured meat,2024-08-09T14:45:22Z,2024-08-09T14:45:22Z,How Well Do LLMs Identify Cultural Unity in Diversity?,\"  Much work on the cultural awareness of large language models (LLMs) focuses\r\non the models' sensitivity to geo-cultural diversity. However, in addition to\r\ncross-cultural differences, there also exists common ground across cultures.\r\nFor instance, a bridal veil in the United States plays a similar\r\ncultural-relevant role as a honggaitou in China. In this study, we introduce a\r\nbenchmark dataset CUNIT for evaluating decoder-only LLMs in understanding the\r\ncultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluation\r\nexamples building upon 285 traditional cultural-specific concepts across 10\r\ncountries. Based on a systematic manual annotation of cultural-relevant\r\nfeatures per concept, we calculate the cultural association between any pair of\r\ncross-cultural concepts. Built upon this dataset, we design a contrastive\r\nmatching task to evaluate the LLMs' capability to identify highly associated\r\ncross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popular\r\nprompting strategies, under the settings of either giving all extracted concept\r\nfeatures or no features at all on CUNIT Interestingly, we find that cultural\r\nassociations across countries regarding clothing concepts largely differ from\r\nfood. Our analysis shows that LLMs are still limited to capturing\r\ncross-cultural associations between concepts compared to humans. Moreover,\r\ngeo-cultural proximity shows a weak influence on model performance in capturing\r\ncross-cultural associations.\r\n\",\"['Jialin Li', 'Junli Wang', 'Junjie Hu', 'Ming Jiang']\"\r\nhttp://arxiv.org/abs/2410.12880v3,Cultured meat,2024-10-15T18:13:10Z,2025-01-24T18:56:07Z,\"Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to\r\n  Sensitivity in Large Language Models\",\"  As LLMs are increasingly deployed in global applications, the importance of\r\ncultural sensitivity becomes paramount, ensuring that users from diverse\r\nbackgrounds feel respected and understood. Cultural harm can arise when these\r\nmodels fail to align with specific cultural norms, resulting in\r\nmisrepresentations or violations of cultural values. This work addresses the\r\nchallenges of ensuring cultural sensitivity in LLMs, especially in\r\nsmall-parameter models that often lack the extensive training data needed to\r\ncapture global cultural nuances. We present two key contributions: (1) A\r\ncultural harm test dataset, created to assess model outputs across different\r\ncultural contexts through scenarios that expose potential cultural\r\ninsensitivities, and (2) A culturally aligned preference dataset, aimed at\r\nrestoring cultural sensitivity through fine-tuning based on feedback from\r\ndiverse annotators. These datasets facilitate the evaluation and enhancement of\r\nLLMs, ensuring their ethical and safe deployment across different cultural\r\nlandscapes. Our results show that integrating culturally aligned feedback leads\r\nto a marked improvement in model behavior, significantly reducing the\r\nlikelihood of generating culturally insensitive or harmful content. Ultimately,\r\nthis work paves the way for more inclusive and respectful AI systems, fostering\r\na future where LLMs can safely and ethically navigate the complexities of\r\ndiverse cultural landscapes.\r\n\",\"['Somnath Banerjee', 'Sayan Layek', 'Hari Shrawgi', 'Rajarshi Mandal', 'Avik Halder', 'Shanu Kumar', 'Sagnik Basu', 'Parag Agrawal', 'Rima Hazra', 'Animesh Mukherjee']\"\r\nhttp://arxiv.org/abs/2311.14096v2,Cultured meat,2023-11-23T16:45:56Z,2024-06-26T15:26:44Z,Cultural Bias and Cultural Alignment of Large Language Models,\"  Culture fundamentally shapes people's reasoning, behavior, and communication.\r\nAs people increasingly use generative artificial intelligence (AI) to expedite\r\nand automate personal and professional tasks, cultural values embedded in AI\r\nmodels may bias people's authentic expression and contribute to the dominance\r\nof certain cultures. We conduct a disaggregated evaluation of cultural bias for\r\nfive widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)\r\nby comparing the models' responses to nationally representative survey data.\r\nAll models exhibit cultural values resembling English-speaking and Protestant\r\nEuropean countries. We test cultural prompting as a control strategy to\r\nincrease cultural alignment for each country/territory. For recent models\r\n(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'\r\noutput for 71-81% of countries and territories. We suggest using cultural\r\nprompting and ongoing evaluation to reduce cultural bias in the output of\r\ngenerative AI.\r\n\",\"['Yan Tao', 'Olga Viberg', 'Ryan S. Baker', 'Rene F. Kizilcec']\"\r\nhttp://arxiv.org/abs/2104.08256v1,Gene therapy,2021-04-16T17:38:12Z,2021-04-16T17:38:12Z,\"Predicting synchronized gene coexpression patterns from fibration\r\n  symmetries in gene regulatory networks in bacteria\",\"  Background: Gene regulatory networks coordinate the expression of genes\r\nacross physiological states and ensure a synchronized expression of genes in\r\ncellular subsystems, critical for the coherent functioning of cells. Here we\r\naddress the questions whether it is possible to predict gene synchronization\r\nfrom network structure alone. We have recently shown that synchronized gene\r\nexpression may be predicted from symmetries in the gene regulatory networks\r\n(GRN) and described by the concept of symmetry fibrations. We showed that\r\nsymmetry fibrations partition the genes into groups called fibers based on the\r\nsymmetries of their 'input trees', the set of paths in the network through\r\nwhich signals can reach a gene. In idealized dynamic gene expression models,\r\nall genes in a fiber are perfectly synchronized, while less idealized models -\r\nwith gene input functions differencing between genes - predict symmetry\r\nbreaking and desynchronization.\r\n  Results: To study the functional role of gene fibers and to test whether some\r\nof the fiber-induced coexpression remains in reality, we analyze gene\r\nfibrations for the gene regulatory networks of E. coli and B. subtilis and\r\nconfront them with expression data. We find approximate gene coexpression\r\npatterns consistent with symmetry fibrations with idealized gene expression\r\ndynamics. This shows that network structure alone provides useful information\r\nabout gene synchronization, and suggest that gene input functions within fibers\r\nmay be further streamlined by evolutionary pressures to realize a coexpression\r\nof genes.\r\n  Conclusions: Thus, gene fibrations provides a sound conceptual tool to\r\ndescribe tunable coexpression induced by network topology and shaped by\r\nmechanistic details of gene expression.\r\n\",\"['Ian Leifer', 'Mishael Snchez-Prez', 'Cecilia Ishida', 'Hernn A. Makse']\"\r\nhttp://arxiv.org/abs/0805.3598v2,Gene therapy,2008-05-23T09:19:26Z,2008-07-23T04:04:23Z,\"Gene profiling for determining pluripotent genes in a time course\r\n  microarray experiment\",\"  In microarray experiments, it is often of interest to identify genes which\r\nhave a pre-specified gene expression profile with respect to time. Methods\r\navailable in the literature are, however, typically not stringent enough in\r\nidentifying such genes, particularly when the profile requires equivalence of\r\ngene expression levels at certain time points. In this paper, the authors\r\nintroduce a new methodology, called gene profiling, that uses simultaneous\r\ndifferential and equivalent gene expression level testing to rank genes\r\naccording to a pre-specified gene expression profile. Gene profiling treats the\r\nvector of true gene expression levels as a linear combination of appropriate\r\nvectors, i.e., vectors that give the required criteria for the profile. This\r\ngene-profile model is fitted to the data and the resultant parameter estimates\r\nare summarized in a single test statistic that is then used to rank the genes.\r\nThe theoretical underpinnings of gene profiling (equivalence testing,\r\nintersection-union tests) are discussed in this paper, and the gene profiling\r\nmethodology is applied to our motivating stem cell experiment.\r\n\",\"['J. Tuke', 'G. F. V. Glonek', 'P. J. Solomon']\"\r\nhttp://arxiv.org/abs/1003.1204v1,Gene therapy,2010-03-05T08:28:06Z,2010-03-05T08:28:06Z,\"From gene trees to species trees II: Species tree inference in the deep\r\n  coalescence model\",\"  When gene copies are sampled from various species, the resulting gene tree\r\nmight disagree with the containing species tree. The primary causes of gene\r\ntree and species tree discord include lineage sorting, horizontal gene\r\ntransfer, and gene duplication and loss. Each of these events yields a\r\ndifferent parsimony criterion for inferring the (containing) species tree from\r\ngene trees. With lineage sorting, species tree inference is to find the tree\r\nminimizing extra gene lineages that had to coexist along species lineages; with\r\ngene duplication, it becomes to find the tree minimizing gene duplications\r\nand/or losses. In this paper, we show the following results: (i) The deep\r\ncoalescence cost is equal to the number of gene losses minus two times the gene\r\nduplication cost in the reconciliation of a uniquely leaf labeled gene tree and\r\na species tree. The deep coalescence cost can be computed in linear time for\r\nany arbitrary gene tree and species tree. (ii) The deep coalescence cost is\r\nalways no less than the gene duplication cost in the reconciliation of an\r\narbitrary gene tree and a species tree. (iii) Species tree inference by\r\nminimizing deep coalescences is NP-hard.\r\n\",['Louxin Zhang']\r\nhttp://arxiv.org/abs/1101.3474v1,Gene therapy,2011-01-18T15:13:30Z,2011-01-18T15:13:30Z,\"Integration of Differential Gene-combination Search and Gene Set\r\n  Enrichment Analysis: A General Approach\",\"  Gene Set Enrichment Analysis (GSEA) and its variations aim to discover\r\ncollections of genes that show moderate but coordinated differences in\r\nexpression. However, such techniques may be ineffective if many individual\r\ngenes in a phenotype-related gene set have weak discriminative power. A\r\npotential solution is to search for combinations of genes that are highly\r\ndifferentiating even when individual genes are not. Although such techniques\r\nhave been developed, these approaches have not been used with GSEA to any\r\nsignificant degree because of the large number of potential gene combinations\r\nand the heterogeneity of measures that assess the differentiation provided by\r\ngene groups of different sizes.\r\n  To integrate the search for differentiating gene combinations and GSEA, we\r\npropose a general framework with two key components: (A) a procedure that\r\nreduces the number of scores to be handled by GSEA to the number of genes by\r\nsummarizing the scores of the gene combinations involving a particular gene in\r\na single score, and (B) a procedure to integrate the heterogeneous scores from\r\ncombinations of different sizes and from different gene combination measures by\r\nmapping the scores to p-values. Experiments on four gene expression data sets\r\ndemonstrate that the integration of GSEA and gene combination search can\r\nenhance the power of traditional GSEA by discovering gene sets that include\r\ngenes with weak individual differentiation but strong joint discriminative\r\npower. Also, gene sets discovered by the integrative framework share several\r\ncommon biological processes and improve the consistency of the results among\r\nthree lung cancer data sets.\r\n\",\"['Gang Fang', 'Michael Steinbach', 'Chad L. Myers', 'Vipin Kumar']\"\r\nhttp://arxiv.org/abs/1301.6547v2,Gene therapy,2013-01-28T14:05:20Z,2014-11-20T23:14:20Z,The infinitely many genes model with horizontal gene transfer,\"  The genome of bacterial species is much more flexible than that of\r\neukaryotes. Moreover, the distributed genome hypothesis for bacteria states\r\nthat the total number of genes present in a bacterial population is greater\r\nthan the genome of every single individual. The pangenome, i.e. the set of all\r\ngenes of a bacterial species (or a sample), comprises the core genes which are\r\npresent in all living individuals, and accessory genes, which are carried only\r\nby some individuals. In order to use accessory genes for adaptation to\r\nenvironmental forces, genes can be transferred horizontally between\r\nindividuals. Here, we extend the infinitely many genes model from Baumdicker,\r\nHess and Pfaffelhuber (2010) for horizontal gene transfer. We take a\r\ngenealogical view and give a construction -- called the Ancestral Gene Transfer\r\nGraph -- of the joint genealogy of all genes in the pangenome. As application,\r\nwe compute moments of several statistics (e.g. the number of differences\r\nbetween two individuals and the gene frequency spectrum) under the infinitely\r\nmany genes model with horizontal gene transfer.\r\n\",\"['Franz Baumdicker', 'Peter Pfaffelhuber']\"\r\nhttp://arxiv.org/abs/2211.08096v1,Gene therapy,2022-11-15T12:27:26Z,2022-11-15T12:27:26Z,\"Unveiling interpretable development-specific gene signatures in the\r\n  developing human prefrontal cortex with ICGS\",\"  In this paper, to unveil interpretable development-specific gene signatures\r\nin human PFC, we propose a novel gene selection method, named Interpretable\r\nCausality Gene Selection (ICGS), which adopts a Bayesian Network (BN) to\r\nrepresent causality between multiple gene variables and a development variable.\r\nThe proposed ICGS method combines the positive instances-based contrastive\r\nlearning with a Variational AutoEncoder (VAE) to obtain this optimal BN\r\nstructure and use a Markov Blanket (MB) to identify gene signatures causally\r\nrelated to the development variable. Moreover, the differential expression\r\ngenes (DEGs) are used to filter redundant genes before gene selection. In order\r\nto identify gene signatures, we apply the proposed ICGS to the human PFC\r\nsingle-cell transcriptomics data. The experimental results demonstrate that the\r\nproposed method can effectively identify interpretable development-specific\r\ngene signatures in human PFC. Gene ontology enrichment analysis and ASD-related\r\ngene analysis show that these identified gene signatures reveal the key\r\nbiological processes and pathways in human PFC and have more potential for\r\nneurodevelopment disorder cure. These gene signatures are expected to bring\r\nimportant implications for understanding PFC development heterogeneity and\r\nfunction in humans.\r\n\",\"['Meng Huang', 'Xiucai Ye', 'Tetsuya Sakurai']\"\r\nhttp://arxiv.org/abs/2310.03611v2,Gene therapy,2023-10-05T15:45:53Z,2023-10-06T11:53:50Z,\"GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene\r\n  Interactions From Gene Expression Data\",\"  Detecting and discovering new gene interactions based on known gene\r\nexpressions and gene interaction data presents a significant challenge. Various\r\nstatistical and deep learning methods have attempted to tackle this challenge\r\nby leveraging the topological structure of gene interactions and gene\r\nexpression patterns to predict novel gene interactions. In contrast, some\r\napproaches have focused exclusively on utilizing gene expression profiles. In\r\nthis context, we introduce GENER, a parallel-layer deep learning network\r\ndesigned exclusively for the identification of gene-gene relationships using\r\ngene expression data. We conducted two training experiments and compared the\r\nperformance of our network with that of existing statistical and deep learning\r\napproaches. Notably, our model achieved an average AUROC score of 0.834 on the\r\ncombined BioGRID&DREAM5 dataset, outperforming competing methods in predicting\r\ngene-gene interactions.\r\n\",\"['Ahmed Fakhry', 'Raneem Khafagy', 'Adriaan-Alexander Ludl']\"\r\nhttp://arxiv.org/abs/q-bio/0609018v1,Gene therapy,2006-09-13T07:25:11Z,2006-09-13T07:25:11Z,\"Gene cluster analysis method reliably identifies horizontally\r\n  transferred genes and reveals their involvement in operon formation\",\"  The formation mechanism of operons remains controversial despite the proposal\r\nof many models. Although acquisition of genes from other species, horizontal\r\ngene transfer, is considered to occur, definitive concrete cases have been\r\nunavailable. It is desirable to select horizontally transferred genes reliably\r\nand examine their relationship to operons. We here developed a method to\r\nidentify candidates of horizontally transferred genes based on minimization of\r\ngene cluster insertions/deletions. To select a benchmark set of positively\r\nhorizontally transferred genes against which the candidate set can be\r\nappraised, we devised another procedure using intergenetic alignments.\r\nComparison with the benchmark set of horizontally transferred genes\r\ndemonstrated the absence of a significant number of false positives in the\r\ncandidates, showing that the method identifies horizontally transferred genes\r\nwith a high degree of confidence. Horizontally transferred genes constitute at\r\nleast 5.5% of the genes in Escherichia, Shigella, and Salmonella and ~46% of\r\nwhich originate from other gamma-proteobacteria. Not only informational genes,\r\nbut also operational genes (those involved in housekeeping) are horizontally\r\ntransferred less frequently than expected. A gene-cluster analysis of\r\nEscherichia coli K-12 operons revealed that horizontal transfer produced four\r\nentire operons and expanded two operons, but deletion of intervening genes\r\naccounts for the formation of no operons. We propose that operons generally\r\nform by horizontal gene transfer. We further suggest that genes with related\r\nessential functions tend to reside in conserved operons, while genes in\r\nnonconserved operons generally confer slight advantage to the organisms and\r\nfrequently undergo horizontal transfer and decay.\r\n\",\"['Keiichi Homma', 'Satoshi Fukuchi', 'Yoji Nakamura', 'Takashi Gojobori', 'Ken Nishikawa']\"\r\nhttp://arxiv.org/abs/1201.3995v2,Gene therapy,2012-01-19T09:32:51Z,2012-05-03T02:23:39Z,Reconciliation of Gene and Species Trees With Polytomies,\"  Motivation: Millions of genes in the modern species belong to only thousands\r\nof `gene families'. A gene family includes instances of the same gene in\r\ndifferent species (orthologs) and duplicate genes in the same species\r\n(paralogs). Genes are gained and lost during evolution. With advances in\r\nsequencing technology, researchers are able to investigate the important roles\r\nof gene duplications and losses in adaptive evolution. Because of gene complex\r\nevolution, ortholog identification is a basic but difficult task in comparative\r\ngenomics. A key method for the task is to use an explicit model of the\r\nevolutionary history of the genes being studied, called the gene (family) tree.\r\nIt compares the gene tree with the evolutionary history of the species in which\r\nthe genes reside, called the species tree, using the procedure known as tree\r\nreconciliation. Reconciling binary gene and specific trees is simple. However,\r\nboth gene and species trees may be non-binary in practice and thus tree\r\nreconciliation presents challenging problems. Here, non-binary gene and species\r\ntree reconciliation is studied in a binary refinement model.\r\n  Results: The problem of reconciling arbitrary gene and species trees is\r\nproved NP-hard even for the duplication cost. We then present the first\r\nefficient method for reconciling a non-binary gene tree and a non-binary\r\nspecies tree. It attempts to find binary refinements of the given gene and\r\nspecies trees that minimize reconciliation cost. Our algorithms have been\r\nimplemented into a software to support quick automated analysis of large data\r\nsets.\r\n  Availability: The program, together with the source code, is available at its\r\nonline server http://phylotoo.appspot.com.\r\n\",\"['Yu Zheng', 'Taoyang Wu', 'Louxin Zhang']\"\r\nhttp://arxiv.org/abs/1901.04847v1,Gene therapy,2019-01-11T23:53:33Z,2019-01-11T23:53:33Z,\"Determining Multifunctional Genes and Diseases in Human Using Gene\r\n  Ontology\",\"  The study of human genes and diseases is very rewarding and can lead to\r\nimprovements in healthcare, disease diagnostics and drug discovery. In this\r\npaper, we further our previous study on gene disease relationship specifically\r\nwith the multifunctional genes. We investigate the multifunctional gene disease\r\nrelationship based on the published molecular function annotations of genes\r\nfrom the Gene Ontology which is the most comprehensive source on gene\r\nfunctions.\r\n\",\"['Hisham Al-Mubaid', 'Sasikanth Potu', 'M. Shenify']\"\r\nhttp://arxiv.org/abs/1301.3933v2,Gene therapy,2013-01-16T22:02:22Z,2013-01-18T01:38:48Z,Gene set bagging for estimating replicability of gene set analyses,\"  Background: Significance analysis plays a major role in identifying and\r\nranking genes, transcription factor binding sites, DNA methylation regions, and\r\nother high-throughput features for association with disease. We propose a new\r\napproach, called gene set bagging, for measuring the stability of ranking\r\nprocedures using predefined gene sets. Gene set bagging involves resampling the\r\noriginal high-throughput data, performing gene-set analysis on the resampled\r\ndata, and confirming that biological categories replicate. This procedure can\r\nbe thought of as bootstrapping gene-set analysis and can be used to determine\r\nwhich are the most reproducible gene sets. Results: Here we apply this approach\r\nto two common genomics applications: gene expression and DNA methylation. Even\r\nwith state-of-the-art statistical ranking procedures, significant categories in\r\na gene set enrichment analysis may be unstable when subjected to resampling.\r\nConclusions: We demonstrate that gene lists are not necessarily stable, and\r\ntherefore additional steps like gene set bagging can improve biological\r\ninference of gene set analysis.\r\n\",\"['Andrew E. Jaffe', 'John D. Storey', 'Hongkai Ji', 'Jeffrey T. Leek']\"\r\nhttp://arxiv.org/abs/1512.08798v1,Gene therapy,2015-12-29T21:15:22Z,2015-12-29T21:15:22Z,\"Evolutionary and topological properties of gene modules and driver\r\n  mutations in a leukemia gene regulatory network\",\"  The diverse, specialized genes in today's lifeforms evolved from a common\r\ncore of ancient, elementary genes. However, these genes did not evolve\r\nindividually: gene expression is controlled by a complex network of\r\ninteractions, and alterations in one gene may drive reciprocal changes in its\r\nproteins' binding partners. We show that the topology of a leukemia gene\r\nregulatory network is strongly coupled with evolutionary properties.\r\nSlowly-evolving (\"\"cold\"\"), old genes tend to interact with each other, as do\r\nrapidly-evolving (\"\"hot\"\"), young genes, causing genes to evolve in clusters. We\r\nargue that gene duplication placed old, cold genes at the center of the\r\nnetwork, and young, hot genes on the periphery, and demonstrate this with\r\nsingle-node centrality measures and two new measures of efficiency. Integrating\r\ncentrality measures with evolutionary information, we define a\r\nmedically-relevant \"\"cancer network core,\"\" strongly enriched for common cancer\r\nmutations ($p=2\\times 10^{-14}$). This could aid in identifying driver\r\nmutations and therapeutic targets.\r\n\",\"['Anthony Szedlak', 'Nicholas Smith', 'Li Liu', 'Giovanni Paternostro', 'Carlo Piermarocchi']\"\r\nhttp://arxiv.org/abs/2106.13642v1,Gene therapy,2021-06-25T13:51:46Z,2021-06-25T13:51:46Z,VEGN: Variant Effect Prediction with Graph Neural Networks,\"  Genetic mutations can cause disease by disrupting normal gene function.\r\nIdentifying the disease-causing mutations from millions of genetic variants\r\nwithin an individual patient is a challenging problem. Computational methods\r\nwhich can prioritize disease-causing mutations have, therefore, enormous\r\napplications. It is well-known that genes function through a complex regulatory\r\nnetwork. However, existing variant effect prediction models only consider a\r\nvariant in isolation. In contrast, we propose VEGN, which models variant effect\r\nprediction using a graph neural network (GNN) that operates on a heterogeneous\r\ngraph with genes and variants. The graph is created by assigning variants to\r\ngenes and connecting genes with an gene-gene interaction network. In this\r\ncontext, we explore an approach where a gene-gene graph is given and another\r\nwhere VEGN learns the gene-gene graph and therefore operates both on given and\r\nlearnt edges. The graph neural network is trained to aggregate information\r\nbetween genes, and between genes and variants. Variants can exchange\r\ninformation via the genes they connect to. This approach improves the\r\nperformance of existing state-of-the-art models.\r\n\",\"['Jun Cheng', 'Carolin Lawrence', 'Mathias Niepert']\"\r\nhttp://arxiv.org/abs/2310.02275v1,Gene therapy,2023-09-29T13:33:53Z,2023-09-29T13:33:53Z,\"MuSe-GNN: Learning Unified Gene Representation From Multimodal\r\n  Biological Graph Data\",\"  Discovering genes with similar functions across diverse biomedical contexts\r\nposes a significant challenge in gene representation learning due to data\r\nheterogeneity. In this study, we resolve this problem by introducing a novel\r\nmodel called Multimodal Similarity Learning Graph Neural Network, which\r\ncombines Multimodal Machine Learning and Deep Graph Neural Networks to learn\r\ngene representations from single-cell sequencing and spatial transcriptomic\r\ndata. Leveraging 82 training datasets from 10 tissues, three sequencing\r\ntechniques, and three species, we create informative graph structures for model\r\ntraining and gene representations generation, while incorporating\r\nregularization with weighted similarity learning and contrastive learning to\r\nlearn cross-data gene-gene relationships. This novel design ensures that we can\r\noffer gene representations containing functional similarity across different\r\ncontexts in a joint space. Comprehensive benchmarking analysis shows our\r\nmodel's capacity to effectively capture gene function similarity across\r\nmultiple modalities, outperforming state-of-the-art methods in gene\r\nrepresentation learning by up to 97.5%. Moreover, we employ bioinformatics\r\ntools in conjunction with gene representations to uncover pathway enrichment,\r\nregulation causal networks, and functions of disease-associated or\r\ndosage-sensitive genes. Therefore, our model efficiently produces unified gene\r\nrepresentations for the analysis of gene functions, tissue functions, diseases,\r\nand species evolution.\r\n\",\"['Tianyu Liu', 'Yuge Wang', 'Rex Ying', 'Hongyu Zhao']\"\r\nhttp://arxiv.org/abs/1501.00302v2,Gene therapy,2015-01-01T19:47:22Z,2015-02-24T23:25:39Z,An Event-Driven Approach for Studying Gene Block Evolution in Bacteria,\"  Motivation: Gene blocks are genes co-located on the chromosome. In many\r\ncases, genes blocks are conserved between bacterial species, sometimes as\r\noperons, when genes are co-transcribed. The conservation is rarely absolute:\r\ngene loss, gain, duplication, block splitting, and block fusion are frequently\r\nobserved. An open question in bacterial molecular evolution is that of the\r\nformation and breakup of gene blocks, for which several models have been\r\nproposed. These models, however, are not generally applicable to all types of\r\ngene blocks, and consequently cannot be used to broadly compare and study gene\r\nblock evolution. To address this problem we introduce an event-based method for\r\ntracking gene block evolution in bacteria. Results: We show here that the\r\nevolution of gene blocks in proteobacteria can be described by a small set of\r\nevents. Those include the insertion of genes into, or the splitting of genes\r\nout of a gene block, gene loss, and gene duplication. We show how the\r\nevent-based method of gene block evolution allows us to determine the\r\nevolutionary rate, and to trace the ancestral states of their formation. We\r\nconclude that the event-based method can be used to help us understand the\r\nformation of these important bacterial genomic structures. Availability: The\r\nsoftware is available under GPLv3 license on\r\nhttp://github.com/reamdc1/gene_block_evolution.git Supplementary online\r\nmaterial: http://iddo-friedberg.net/operon-evolution Contact: Iddo Friedberg\r\ni.friedberg@miamioh.edu\r\n\",\"['David C Ream', 'Asma R Bankapur', 'Iddo Friedberg']\"\r\nhttp://arxiv.org/abs/2411.18391v1,Gene therapy,2024-11-27T14:33:13Z,2024-11-27T14:33:13Z,\"GeneQuery: A General QA-based Framework for Spatial Gene Expression\r\n  Predictions from Histology Images\",\"  Gene expression profiling provides profound insights into molecular\r\nmechanisms, but its time-consuming and costly nature often presents significant\r\nchallenges. In contrast, whole-slide hematoxylin and eosin (H&E) stained\r\nhistological images are readily accessible and allow for detailed examinations\r\nof tissue structure and composition at the microscopic level. Recent\r\nadvancements have utilized these histological images to predict spatially\r\nresolved gene expression profiles. However, state-of-the-art works treat gene\r\nexpression prediction as a multi-output regression problem, where each gene is\r\nlearned independently with its own weights, failing to capture the shared\r\ndependencies and co-expression patterns between genes. Besides, existing works\r\ncan only predict gene expression values for genes seen during training,\r\nlimiting their ability to generalize to new, unseen genes.\r\n  To address the above limitations, this paper presents GeneQuery, which aims\r\nto solve this gene expression prediction task in a question-answering (QA)\r\nmanner for better generality and flexibility. Specifically, GeneQuery takes\r\ngene-related texts as queries and whole-slide images as contexts and then\r\npredicts the queried gene expression values. With such a transformation,\r\nGeneQuery can implicitly estimate the gene distribution by introducing the gene\r\nrandom variable. Besides, the proposed GeneQuery consists of two architecture\r\nimplementations, i.e., spot-aware GeneQuery for capturing patterns between\r\nimages and gene-aware GeneQuery for capturing patterns between genes.\r\nComprehensive experiments on spatial transcriptomics datasets show that the\r\nproposed GeneQuery outperforms existing state-of-the-art methods on known and\r\nunseen genes. More results also demonstrate that GeneQuery can potentially\r\nanalyze the tissue structure.\r\n\",\"['Ying Xiong', 'Linjing Liu', 'Yufei Cui', 'Shangyu Wu', 'Xue Liu', 'Antoni B. Chan', 'Chun Jason Xue']\"\r\nhttp://arxiv.org/abs/1105.1217v1,Gene therapy,2011-05-06T02:56:45Z,2011-05-06T02:56:45Z,\"Marker Genes for Anatomical Regions in the Brain: Insights from the\r\n  Allen Gene Expression Atlas\",\"  Quantitative criteria are proposed to identify genes (and sets of genes)\r\nwhose expression marks a specific brain region (or a set of brain regions).\r\nGene-expression energies, obtained for thousands of mouse genes by numerization\r\nof in-situ hybridization images in the Allen Gene Expression Atlas, are used to\r\ntest these methods in the mouse brain. Individual genes are ranked using\r\nintegrals of their expression energies across brain regions. The ranking is\r\ngeneralized to sets of genes and the problem of optimal markers of a classical\r\nregion receives a linear-algebraic solution. Moreover, the goodness of the\r\nfitting of the expression profile of a gene to the profile of a brain region is\r\nclosely related to the co-expression of genes. The geometric interpretation of\r\nthis fact leads to a quantitative criterion to detect markers of pairs of brain\r\nregions. Local properties of the gene-expression profiles are also used to\r\ndetect genes that separate a given grain region from its environment.\r\n\",\"['Pascal Grange', 'Partha P. Mitra']\"\r\nhttp://arxiv.org/abs/2204.10473v1,Gene therapy,2022-04-22T02:54:01Z,2022-04-22T02:54:01Z,\"Gene Function Prediction with Gene Interaction Networks: A Context Graph\r\n  Kernel Approach\",\"  Predicting gene functions is a challenge for biologists in the post genomic\r\nera. Interactions among genes and their products compose networks that can be\r\nused to infer gene functions. Most previous studies adopt a linkage assumption,\r\ni.e., they assume that gene interactions indicate functional similarities\r\nbetween connected genes. In this study, we propose to use a gene's context\r\ngraph, i.e., the gene interaction network associated with the focal gene, to\r\ninfer its functions. In a kernel-based machine-learning framework, we design a\r\ncontext graph kernel to capture the information in context graphs. Our\r\nexperimental study on a testbed of p53-related genes demonstrates the advantage\r\nof using indirect gene interactions and shows the empirical superiority of the\r\nproposed approach over linkage-assumption-based methods, such as the algorithm\r\nto minimize inconsistent connected genes and diffusion kernels.\r\n\",\"['Xin Li', 'Hsinchun Chen', 'Jiexun Li', 'Zhu Zhang']\"\r\nhttp://arxiv.org/abs/2412.12688v1,Gene therapy,2024-12-17T09:08:52Z,2024-12-17T09:08:52Z,\"UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation\r\n  Benchmarks with Unified Entrez Gene Identifiers\",\"  Gene studies are crucial for fields such as protein structure prediction,\r\ndrug discovery, and cancer genomics, yet they face challenges in fully\r\nutilizing the vast and diverse information available. Gene studies require\r\nclean, factual datasets to ensure reliable results. Ontology graphs, neatly\r\norganized domain terminology graphs, provide ideal sources for domain facts.\r\nHowever, available gene ontology annotations are currently distributed across\r\nvarious databases without unified identifiers for genes and gene products. To\r\naddress these challenges, we introduce Unified Entrez Gene Identifier Dataset\r\nand Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale\r\npublic Gene Ontology Annotations (GOA) from various databases using unique gene\r\nidentifiers. UniEntrezDB includes a pre-training dataset and four downstream\r\ntasks designed to comprehensively evaluate gene embedding performance from\r\ngene, protein, and cell levels, ultimately enhancing the reliability and\r\napplicability of LLMs in gene research and other professional settings.\r\n\",\"['Yuwei Miao', 'Yuzhi Guo', 'Hehuan Ma', 'Jingquan Yan', 'Feng Jiang', 'Weizhi An', 'Jean Gao', 'Junzhou Huang']\"\r\nhttp://arxiv.org/abs/q-bio/0509037v1,Gene therapy,2005-09-27T14:17:53Z,2005-09-27T14:17:53Z,\"A probabilistic model for gene content evolution with duplication, loss,\r\n  and horizontal transfer\",\"  We introduce a Markov model for the evolution of a gene family along a\r\nphylogeny. The model includes parameters for the rates of horizontal gene\r\ntransfer, gene duplication, and gene loss, in addition to branch lengths in the\r\nphylogeny. The likelihood for the changes in the size of a gene family across\r\ndifferent organisms can be calculated in O(N+hM^2) time and O(N+M^2) space,\r\nwhere N is the number of organisms, $h$ is the height of the phylogeny, and M\r\nis the sum of family sizes. We apply the model to the evolution of gene content\r\nin Preoteobacteria using the gene families in the COG (Clusters of Orthologous\r\nGroups) database.\r\n\",\"['Mikls Csrs', 'Istvn Mikls']\"\r\nhttp://arxiv.org/abs/2006.16925v3,Neurotechnology,2020-06-23T07:46:22Z,2024-09-18T23:37:41Z,\"Ethical Analysis on the Application of Neurotechnology for Human\r\n  Augmentation in Physicians and Surgeons\",\"  With the shortage of physicians and surgeons and increase in demand worldwide\r\ndue to situations such as the COVID-19 pandemic, there is a growing interest in\r\nfinding solutions to help address the problem. A solution to this problem would\r\nbe to use neurotechnology to provide them augmented cognition, senses and\r\naction for optimal diagnosis and treatment. Consequently, doing so can\r\nnegatively impact them and others. We argue that applying neurotechnology for\r\nhuman enhancement in physicians and surgeons can cause injustices, and harm to\r\nthem and patients. In this paper, we will first describe the augmentations and\r\nneurotechnologies that can be used to achieve the relevant augmentations for\r\nphysicians and surgeons. We will then review selected ethical concerns\r\ndiscussed within literature, discuss the neuroengineering behind using\r\nneurotechnology for augmentation purposes, then conclude with an analysis on\r\noutcomes and ethical issues of implementing human augmentation via\r\nneurotechnology in medical and surgical practice.\r\n\",\"['Soaad Hossain', 'Syed Ishtiaque Ahmed']\"\r\nhttp://arxiv.org/abs/1607.05023v1,Neurotechnology,2016-07-18T11:28:11Z,2016-07-18T11:28:11Z,\"Intelligent Biohybrid Neurotechnologies: Are They Really What They\r\n  Claim?\",\"  In the era of intelligent biohybrid neurotechnologies for brain repair, new\r\nfanciful terms are appearing in the scientific dictionary to define what has so\r\nfar been unimaginable. As the emerging neurotechnologies are becoming\r\nincreasingly polyhedral and sophisticated, should we talk about evolution and\r\nrank the intelligence of these devices?\r\n\",\"['Gabriella Panuccio', 'Marianna Semprini', 'Lorenzo Natale', 'Michela Chiappalone']\"\r\nhttp://arxiv.org/abs/2404.00047v2,Neurotechnology,2024-03-25T09:43:20Z,2024-09-11T17:02:08Z,\"Foundational guidelines for enhancing neurotechnology research and\r\n  development through end-user involvement\",\"  Neurotechnologies are increasingly becoming integrated with our everyday\r\nlives, our bodies and our mental states. As the popularity and impact of\r\nneurotechnology grows, so does our responsibility to ensure we understand its\r\nparticular implications on its end users, as well as broader ethical and\r\nsocietal implications. Enabling end-users and stakeholders to participate in\r\nthe development of neurotechnology, from its earliest stages of conception,\r\nwill help us better navigate our design around these considerations and deliver\r\nmore impactful technologies. There are many terms and frameworks to articulate\r\nthe concept of involving end users in the technology development lifecycle, for\r\nexample: 'Public and Patient Involvement and Engagement' (PPIE), 'lived\r\nexperience' and 'co-design'. Here we utilise the PPIE framework to develop\r\nclear guidelines for implementing a robust involvement process of current and\r\nfuture end-users in neurotechnology. We present best practice guidance for\r\nresearchers and engineers who are interested in developing and conducting a PPI\r\nstrategy for their neurotechnology. We provide advice from various online\r\nsources to orient individual teams (and funders) to carve up their own approach\r\nto meaningful involvement. After an introduction that coveys the tangible and\r\nconceptual benefits of user involvement, we guide the reader to develop a\r\ngeneral strategy towards setting up their own process. We then help the reader\r\nmap out their relevant stakeholders and provide advice on how to consider user\r\ndiversity and representation. We also provide advice on how to quantify the\r\noutcomes of the engagement, as well as a check-list to ensure transparency and\r\naccountability at various stages. The aim is the establishment of gold-standard\r\nmethodologies for ensuring that patient and public insights are at the\r\nforefront of our scientific inquiry and product development.\r\n\",\"['Amparo Gemes', 'Tiago da Silva Costa', 'Tamar Makin']\"\r\nhttp://arxiv.org/abs/1903.00981v1,Neurotechnology,2019-03-03T20:20:32Z,2019-03-03T20:20:32Z,\"A Separation Principle for Discrete-Time Fractional-Order Dynamical\r\n  Systems and its Implications to Closed-loop Neurotechnology\",\"  Closed-loop neurotechnology requires the capability to predict the state\r\nevolution and its regulation under (possibly) partial measurements. There is\r\nevidence that neurophysiological dynamics can be modeled by fractional-order\r\ndynamical systems. Therefore, we propose to establish a separation principle\r\nfor discrete-time fractional-order dynamical systems, which are inherently\r\nnonlinear and are able to capture spatiotemporal relations that exhibit\r\nnon-Markovian properties. The separation principle states that the problems of\r\ncontroller and state estimator design can be done independently of each other\r\nwhile ensuring proper estimation and control in closed-loop setups. Lastly, we\r\nillustrate, as proof-of-concept, the application of the separation principle\r\nwhen designing controllers and estimators for these classes of systems in the\r\ncontext of neurophysiological data. In particular, we rely on real data to\r\nderive the models used to assess and regulate the evolution of closed-loop\r\nneurotechnologies based on electroencephalographic data.\r\n\",\"['Sarthak Chatterjee', 'Orlando Romero', 'Srgio Pequito']\"\r\nhttp://arxiv.org/abs/2110.11475v1,Neurotechnology,2021-10-21T20:54:24Z,2021-10-21T20:54:24Z,Future of Smart Classroom in the Era of Wearable Neurotechnology,\"  Interdisciplinary research among engineering, computer science, and\r\nneuroscience to understand and utilize the human brain signals resulted in\r\nadvances and widespread applicability of wearable neurotechnology in adaptive\r\nhuman-in-the-loop smart systems. Considering these advances, we envision that\r\nfuture education will exploit the advances in wearable neurotechnology and move\r\ntoward more personalized smart classrooms where instructions and interactions\r\nare tailored towards. students' individual strengths and needs. In this paper,\r\nwe discuss the future of smart classrooms and how advances in neuroscience,\r\nmachine learning, and embedded systems as key enablers will provide the\r\ninfrastructure for envisioned smart classrooms and personalized education along\r\nwith open challenges that are required to be addressed.\r\n\",\"['Mojtaba Taherisadr', 'Berken Utku Demirel', 'Mohammad Abdullah Al Faruque', 'Salma Elmalaki']\"\r\nhttp://arxiv.org/abs/2403.07945v4,Neurotechnology,2024-03-11T03:44:18Z,2025-01-26T20:27:15Z,\"A Mathematical Framework for the Problem of Security for Cognition in\r\n  Neurotechnology\",\"  The rapid advancement in neurotechnology in recent years has created an\r\nemerging critical intersection between neurotechnology and security.\r\nImplantable devices, non-invasive monitoring, and non-invasive therapies all\r\ncarry with them the prospect of violating the privacy and autonomy of\r\nindividuals' cognition. A growing number of scientists and physicians have made\r\ncalls to address this issue, but applied efforts have been relatively limited.\r\nA major barrier hampering scientific and engineering efforts to address these\r\nsecurity issues is the lack of a clear means of describing and analyzing\r\nrelevant problems. In this paper we develop Cognitive Neurosecurity, a\r\nmathematical framework which enables such description and analysis by drawing\r\non methods and results from multiple fields. We demonstrate certain statistical\r\nproperties which have significant implications for Cognitive Neurosecurity, and\r\nthen present descriptions of the algorithmic problems faced by attackers\r\nattempting to violate privacy and autonomy, and defenders attempting to\r\nobstruct such attempts.\r\n\",\"['Bryce Allen Bagley', 'Claudia K Petritsch']\"\r\nhttp://arxiv.org/abs/2207.13190v1,Neurotechnology,2022-07-26T21:38:01Z,2022-07-26T21:38:01Z,How does artificial intelligence contribute to iEEG research?,\"  Artificial intelligence (AI) is a fast-growing field focused on modeling and\r\nmachine implementation of various cognitive functions with an increasing number\r\nof applications in computer vision, text processing, robotics, neurotechnology,\r\nbio-inspired computing and others. In this chapter, we describe how AI methods\r\ncan be applied in the context of intracranial electroencephalography (iEEG)\r\nresearch. IEEG data is unique as it provides extremely high-quality signals\r\nrecorded directly from brain tissue. Applying advanced AI models to these data\r\ncarries the potential to further our understanding of many fundamental\r\nquestions in neuroscience. At the same time, as an invasive technique, iEEG\r\nlends itself well to long-term, mobile brain-computer interface applications,\r\nparticularly for communication in severely paralyzed individuals. We provide a\r\ndetailed overview of these two research directions in the application of AI\r\ntechniques to iEEG. That is, (1) the development of computational models that\r\ntarget fundamental questions about the neurobiological nature of cognition\r\n(AI-iEEG for neuroscience) and (2) applied research on monitoring and\r\nidentification of event-driven brain states for the development of clinical\r\nbrain-computer interface systems (AI-iEEG for neurotechnology). We explain key\r\nmachine learning concepts, specifics of processing and modeling iEEG data and\r\ndetails of state-of-the-art iEEG-based neurotechnology and brain-computer\r\ninterfaces.\r\n\",\"['Julia Berezutskaya', 'Anne-Lise Saive', 'Karim Jerbi', 'Marcel van Gerven']\"\r\nhttp://arxiv.org/abs/1703.02365v1,Neurotechnology,2017-03-07T13:12:31Z,2017-03-07T13:12:31Z,\"Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about\r\n  Neurotechnologies\",\"  Teegi is an anthropomorphic and tangible avatar exposing a users' brain\r\nactivity in real time. It is connected to a device sensing the brain by means\r\nof electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its\r\neyes along with the person being monitored. It also displays on its scalp the\r\nassociated EEG signals, thanks to a semi-spherical display made of LEDs.\r\nAttendees can interact directly with Teegi -- e.g. move its limbs -- to\r\ndiscover by themselves the underlying brain processes. Teegi can be used for\r\nscientific outreach to introduce neurotechnologies in general and\r\nbrain-computer interfaces (BCI) in particular.\r\n\",\"['Jrmy Frey', 'Renaud Gervais', 'Thibault Lain', 'Maxime Duluc', 'Hugo Germain', 'Stphanie Fleck', 'Fabien Lotte', 'Martin Hachet']\"\r\nhttp://arxiv.org/abs/2405.10780v2,Neurotechnology,2024-05-13T21:37:50Z,2024-05-31T15:00:36Z,\"Intelligent and Miniaturized Neural Interfaces: An Emerging Era in\r\n  Neurotechnology\",\"  Integrating smart algorithms on neural devices presents significant\r\nopportunities for various brain disorders. In this paper, we review the latest\r\nadvancements in the development of three categories of intelligent neural\r\nprostheses featuring embedded signal processing on the implantable or wearable\r\ndevice. These include: 1) Neural interfaces for closed-loop symptom tracking\r\nand responsive stimulation; 2) Neural interfaces for emerging network-related\r\nconditions, such as psychiatric disorders; and 3) Intelligent BMI SoCs for\r\nmovement recovery following paralysis.\r\n\",\"['Mahsa Shoaran', 'Uisub Shin', 'MohammadAli Shaeri']\"\r\nhttp://arxiv.org/abs/1804.10454v2,Neurotechnology,2018-04-27T11:56:04Z,2019-01-21T11:39:50Z,\"Mining within-trial oscillatory brain dynamics to address the\r\n  variability of optimized spatial filters\",\"  Data-driven spatial filtering algorithms optimize scores such as the contrast\r\nbetween two conditions to extract oscillatory brain signal components. Most\r\nmachine learning approaches for filter estimation, however, disregard\r\nwithin-trial temporal dynamics and are extremely sensitive to changes in\r\ntraining data and involved hyperparameters. This leads to highly variable\r\nsolutions and impedes the selection of a suitable candidate for,\r\ne.g.,~neurotechnological applications. Fostering component introspection, we\r\npropose to embrace this variability by condensing the functional signatures of\r\na large set of oscillatory components into homogeneous clusters, each\r\nrepresenting specific within-trial envelope dynamics.\r\n  The proposed method is exemplified by and evaluated on a complex hand force\r\ntask with a rich within-trial structure. Based on electroencephalography data\r\nof 18 healthy subjects, we found that the components' distinct temporal\r\nenvelope dynamics are highly subject-specific. On average, we obtained seven\r\nclusters per subject, which were strictly confined regarding their underlying\r\nfrequency bands. As the analysis method is not limited to a specific spatial\r\nfiltering algorithm, it could be utilized for a wide range of\r\nneurotechnological applications, e.g., to select and monitor functionally\r\nrelevant features for brain-computer interface protocols in stroke\r\nrehabilitation.\r\n\",\"['Andreas Meinel', 'Henrich Kolkhorst', 'Michael Tangermann']\"\r\nhttp://arxiv.org/abs/1410.7550v1,Neurotechnology,2014-10-28T08:37:01Z,2014-10-28T08:37:01Z,Learning deep dynamical models from image pixels,\"  Modeling dynamical systems is important in many disciplines, e.g., control,\r\nrobotics, or neurotechnology. Commonly the state of these systems is not\r\ndirectly observed, but only available through noisy and potentially\r\nhigh-dimensional observations. In these cases, system identification, i.e.,\r\nfinding the measurement mapping and the transition mapping (system dynamics) in\r\nlatent space can be challenging. For linear system dynamics and measurement\r\nmappings efficient solutions for system identification are available. However,\r\nin practical applications, the linearity assumptions does not hold, requiring\r\nnon-linear system identification techniques. If additionally the observations\r\nare high-dimensional (e.g., images), non-linear system identification is\r\ninherently hard. To address the problem of non-linear system identification\r\nfrom high-dimensional observations, we combine recent advances in deep learning\r\nand system identification. In particular, we jointly learn a low-dimensional\r\nembedding of the observation by means of deep auto-encoders and a predictive\r\ntransition model in this low-dimensional space. We demonstrate that our model\r\nenables learning good predictive models of dynamical systems from pixel\r\ninformation only.\r\n\",\"['Niklas Wahlstrm', 'Thomas B. Schn', 'Marc Peter Deisenroth']\"\r\nhttp://arxiv.org/abs/1505.03964v1,Neurotechnology,2015-05-15T05:53:45Z,2015-05-15T05:53:45Z,\"Algebraic identification of the effective connectivity of constrained\r\n  geometric network models of neural signaling\",\"  Cellular neural circuit and networks consisting of interconnected neurons and\r\nglia are ulti- mately responsible for the information processing associated\r\nwith information processing in the brain. While there are major efforts aimed\r\nat mapping the structural and (electro)physiological connectivity of brain\r\nnetworks, such as the White House BRAIN Initiative aimed at the devel- opment\r\nof neurotechnologies capable of high density neural recordings, theoretical and\r\ncompu- tational methods for analyzing and making sense of all this data seem to\r\nbe further behind. Here, we propose and provide a summary of an approach for\r\ncalculating effective connectivity from experimental observations of neuronal\r\nnetwork activity. The proposed method operates on network-level data, makes use\r\nof all relevant prior knowledge, such as dynamical models of individual cells\r\nin the network and the physical structural connectivity of the network, and is\r\nbroadly applicable to large classes of biological and non-biological networks.\r\n\",\"['Marius Buibas', 'Gabriel A. Silva']\"\r\nhttp://arxiv.org/abs/2007.11674v1,Neurotechnology,2020-07-18T18:05:14Z,2020-07-18T18:05:14Z,\"Using EEG-based brain connectivity for the study of brain dynamics in\r\n  brain-computer interfaces\",\"  The analysis of brain connectivity aims to understand the emergence of\r\nfunctional networks into the brain. This information can be used in the process\r\nof electroencephalographic (EEG) signal analysis and classification for a\r\nbraincomputer interface (BCI). These systems provide an alternative channel of\r\ncommunication and control to people with motor impairments. In this article,\r\nfour strategies for using the brain connectivity in a BCI environment as a tool\r\nto obtain a deeper understanding of the cerebral mechanisms are proposed, with\r\nthe principal aim of developing a scheme oriented to neuro-rehabilitation of\r\ngait in combination with different neurotechnologies and exoskeletons. This\r\nscheme would allow improving current schemes and/or to design new control\r\nstrategies, as well as rehabilitation approaches.\r\n\",['J. A. Gaxiola-Tirado']\r\nhttp://arxiv.org/abs/2101.05084v1,Neurotechnology,2020-12-10T15:32:17Z,2020-12-10T15:32:17Z,\"This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in\r\n  Generative Models\",\"  Generative adversarial networks (GANs) are able to generate high resolution\r\nphoto-realistic images of objects that \"\"do not exist.\"\" These synthetic images\r\nare rather difficult to detect as fake. However, the manner in which these\r\ngenerative models are trained hints at a potential for information leakage from\r\nthe supplied training data, especially in the context of synthetic faces. This\r\npaper presents experiments suggesting that identity information in face images\r\ncan flow from the training corpus into synthetic samples without any\r\nadversarial actions when building or using the existing model. This raises\r\nprivacy-related questions, but also stimulates discussions of (a) the face\r\nmanifold's characteristics in the feature space and (b) how to create\r\ngenerative models that do not inadvertently reveal identity information of real\r\nsubjects whose images were used for training. We used five different face\r\nmatchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology\r\nMegaMatcher) and the StyleGAN2 synthesis model, and show that this identity\r\nleakage does exist for some, but not all methods. So, can we say that these\r\nsynthetically generated faces truly do not exist? Databases of real and\r\nsynthetically generated faces are made available with this paper to allow full\r\nreplicability of the results discussed in this work.\r\n\",\"['Patrick Tinsley', 'Adam Czajka', 'Patrick Flynn']\"\r\nhttp://arxiv.org/abs/2106.12295v1,Neurotechnology,2021-06-23T10:24:15Z,2021-06-23T10:24:15Z,Quantum Brain Networks: a Perspective,\"  We propose Quantum Brain Networks (QBraiNs) as a new interdisciplinary field\r\nintegrating knowledge and methods from neurotechnology, artificial\r\nintelligence, and quantum computing. The objective is to develop an enhanced\r\nconnectivity between the human brain and quantum computers for a variety of\r\ndisruptive applications. We foresee the emergence of hybrid classical-quantum\r\nnetworks of wetware and hardware nodes, mediated by machine learning techniques\r\nand brain-machine interfaces. QBraiNs will harness and transform in\r\nunprecedented ways arts, science, technologies, and entrepreneurship, in\r\nparticular activities related to medicine, Internet of humans, intelligent\r\ndevices, sensorial experience, gaming, Internet of things, crypto trading, and\r\nbusiness.\r\n\",\"['E. R. Miranda', 'S. Venkatesh', 'C. Hernani-Morales', 'L. Lamata', 'J. D. Martn-Guerrero', 'E. Solano']\"\r\nhttp://arxiv.org/abs/2204.02362v2,Neurotechnology,2022-04-04T12:47:07Z,2022-04-13T12:02:18Z,\"Challenges and Opportunities of Edge AI for Next-Generation Implantable\r\n  BMIs\",\"  Neuroscience and neurotechnology are currently being revolutionized by\r\nartificial intelligence (AI) and machine learning. AI is widely used to study\r\nand interpret neural signals (analytical applications), assist people with\r\ndisabilities (prosthetic applications), and treat underlying neurological\r\nsymptoms (therapeutic applications). In this brief, we will review the emerging\r\nopportunities of on-chip AI for the next-generation implantable brain-machine\r\ninterfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major\r\ntechnological challenges for the effectiveness of AI models will be discussed.\r\nFinally, we will present algorithmic and IC design solutions to enable a new\r\ngeneration of AI-enhanced and high-channel-count BMIs.\r\n\",\"['MohammadAli Shaeri', 'Arshia Afzal', 'Mahsa Shoaran']\"\r\nhttp://arxiv.org/abs/2302.03752v1,Neurotechnology,2023-02-07T20:57:15Z,2023-02-07T20:57:15Z,\"Dynamic Visualization of Gyral and Sulcal Stereoelectroencephalographic\r\n  contacts in Humans\",\"  Stereoelectroencephalography (SEEG) is a neurosurgical method to survey\r\nelectrophysiological activity within the brain to treat disorders such as\r\nEpilepsy. In this stereotactic approach, leads are implanted through straight\r\ntrajectories to survey both cortical and sub-cortical activity. Visualizing the\r\nrecorded locations covering sulcal and gyral activity while staying true to the\r\ncortical architecture is challenging due to the folded, three-dimensional\r\nnature of the human cortex. To overcome this challenge, we developed a novel\r\nvisualization concept, allowing investigators to dynamically morph between the\r\nsubjects' cortical reconstruction and an inflated cortex representation. This\r\ninflated view, in which gyri and sulci are viewed on a smooth surface, allows\r\nbetter visualization of electrodes buried within the sulcus while staying true\r\nto the underlying cortical architecture.\r\n\",\"['Markus Adamek', 'Alexander P Rockhill', 'Peter Brunner', 'Dora Hermes']\"\r\nhttp://arxiv.org/abs/2409.11751v1,Neurotechnology,2024-09-18T07:09:59Z,2024-09-18T07:09:59Z,\"Accelerated Algorithms for Source Orientation Detection (AORI) and\r\n  Spatiotemporal LCMV (ALCMV) Beamforming in EEG Source Localization\",\"  This paper illustrates the development of two efficient source localization\r\nalgorithms for electroencephalography (EEG) data, aimed at enhancing real-time\r\nbrain signal reconstruction while addressing the computational challenges of\r\ntraditional methods. Accurate EEG source localization is crucial for\r\napplications in cognitive neuroscience, neurorehabilitation, and brain-computer\r\ninterfaces (BCIs). To make significant progress toward precise source\r\norientation detection and improved signal reconstruction, we introduce the\r\nAccelerated Linear Constrained Minimum Variance (ALCMV) beamforming toolbox and\r\nthe Accelerated Brain Source Orientation Detection (AORI) toolbox. The ALCMV\r\nalgorithm speeds up EEG source reconstruction by utilizing recursive covariance\r\nmatrix calculations, while AORI simplifies source orientation detection from\r\nthree dimensions to one, reducing computational load by 66% compared to\r\nconventional methods. Using both simulated and real EEG data, we demonstrate\r\nthat these algorithms maintain high accuracy, with orientation errors below\r\n0.2% and signal reconstruction accuracy within 2%. These findings suggest that\r\nthe proposed toolboxes represent a substantial advancement in the efficiency\r\nand speed of EEG source localization, making them well-suited for real-time\r\nneurotechnological applications.\r\n\",\"['Ava Yektaeian Vaziri', 'Bahador Makkiabadi']\"\r\nhttp://arxiv.org/abs/1211.0886v1,Neurotechnology,2012-11-05T15:13:45Z,2012-11-05T15:13:45Z,Brain Computer Interface Technologies in the Coming Decades,\"  As the proliferation of technology dramatically infiltrates all aspects of\r\nmodern life, in many ways the world is becoming so dynamic and complex that\r\ntechnological capabilities are overwhelming human capabilities to optimally\r\ninteract with and leverage those technologies. Fortunately, these technological\r\nadvancements have also driven an explosion of neuroscience research over the\r\npast several decades, presenting engineers with a remarkable opportunity to\r\ndesign and develop flexible and adaptive brain-based neurotechnologies that\r\nintegrate with and capitalize on human capabilities and limitations to improve\r\nhuman-system interactions. Major forerunners of this conception are\r\nbrain-computer interfaces (BCIs), which to this point have been largely focused\r\non improving the quality of life for particular clinical populations and\r\ninclude, for example, applications for advanced communications with paralyzed\r\nor locked in patients as well as the direct control of prostheses and\r\nwheelchairs. Near-term applications are envisioned that are primarily task\r\noriented and are targeted to avoid the most difficult obstacles to development.\r\nIn the farther term, a holistic approach to BCIs will enable a broad range of\r\ntask-oriented and opportunistic applications by leveraging pervasive\r\ntechnologies and advanced analytical approaches to sense and merge critical\r\nbrain, behavioral, task, and environmental information. Communications and\r\nother applications that are envisioned to be broadly impacted by BCIs are\r\nhighlighted; however, these represent just a small sample of the potential of\r\nthese technologies.\r\n\",\"['Brent J. Lance', 'Scott E. Kerick', 'Anthony J. Ries', 'Kelvin S. Oie', 'Kaleb McDowell']\"\r\nhttp://arxiv.org/abs/1705.02042v2,Neurotechnology,2017-05-04T22:54:54Z,2017-07-13T14:37:34Z,Exponential scaling of neural algorithms - a future beyond Moore's Law?,\"  Although the brain has long been considered a potential inspiration for\r\nfuture computing, Moore's Law - the scaling property that has seen revolutions\r\nin technologies ranging from supercomputers to smart phones - has largely been\r\ndriven by advances in materials science. As the ability to miniaturize\r\ntransistors is coming to an end, there is increasing attention on new\r\napproaches to computation, including renewed enthusiasm around the potential of\r\nneural computation. This paper describes how recent advances in\r\nneurotechnologies, many of which have been aided by computing's rapid\r\nprogression over recent decades, are now reigniting this opportunity to bring\r\nneural computation insights into broader computing applications. As we\r\nunderstand more about the brain, our ability to motivate new computing\r\nparadigms with continue to progress. These new approaches to computing, which\r\nwe are already seeing in techniques such as deep learning and neuromorphic\r\nhardware, will themselves improve our ability to learn about the brain and\r\naccordingly can be projected to give rise to even further insights. This paper\r\nwill describe how this positive feedback has the potential to change the\r\ncomplexion of how computing sciences and neurosciences interact, and suggests\r\nthat the next form of exponential scaling in computing may emerge from our\r\nprogressive understanding of the brain.\r\n\",['James B. Aimone']\r\nhttp://arxiv.org/abs/2206.06276v1,Reusable launch vehicle,2022-06-13T16:03:35Z,2022-06-13T16:03:35Z,On the reusability of samples in active learning,\"  An interesting but not extensively studied question in active learning is\r\nthat of sample reusability: to what extent can samples selected for one learner\r\nbe reused by another? This paper explains why sample reusability is of\r\npractical interest, why reusability can be a problem, how reusability could be\r\nimproved by importance-weighted active learning, and which obstacles to\r\nuniversal reusability remain. With theoretical arguments and practical\r\ndemonstrations, this paper argues that universal reusability is impossible.\r\nBecause every active learning strategy must undersample some areas of the\r\nsample space, learners that depend on the samples in those areas will learn\r\nmore from a random sample selection. This paper describes several experiments\r\nwith importance-weighted active learning that show the impact of the\r\nreusability problem in practice. The experiments confirmed that universal\r\nreusability does not exist, although in some cases -- on some datasets and with\r\nsome pairs of classifiers -- there is sample reusability. Finally, this paper\r\nexplores the conditions that could guarantee the reusability between two\r\nclassifiers.\r\n\",\"['Gijs van Tulder', 'Marco Loog']\"\r\nhttp://arxiv.org/abs/1409.2223v1,Reusable launch vehicle,2014-09-08T07:10:37Z,2014-09-08T07:10:37Z,\"Assessment of classification techniques on predicting success or failure\r\n  of Software reusability\",\"  Assessment of classification techniques on predicting success or failure of\r\nSoftware reusability\r\n\",\"['Nahid Hajizadeh', 'Manijeh Keshtgari', 'Marzieh Ahmadzadeh']\"\r\nhttp://arxiv.org/abs/1205.0289v2,Reusable launch vehicle,2012-05-01T23:47:56Z,2012-05-22T21:53:00Z,On the Power of Reusable Magic States,\"  In this paper we study reusable magic states. These states are a special\r\nsubset of the standard magic states. Once distilled, reusable magic states can\r\nbe used, repeatedly, to apply some unitary U. Given this property, reusable\r\nmagic states have the potential to greatly lower qubit and gate overheads in\r\nfault-tolerant quantum computation. While these states are promising, we\r\nprovide a strong argument for their limited computational power. Specifically,\r\nwe show that if reusable magic states can be used to apply non-Clifford\r\nunitaries, then we can exploit them to efficiently simulate poly-sized quantum\r\ncircuits on a classical computer.\r\n\",['Jonas T. Anderson']\r\nhttp://arxiv.org/abs/2304.03377v1,Reusable launch vehicle,2023-04-06T21:13:53Z,2023-04-06T21:13:53Z,\"Leveraging Reusability: Improved Competitive Ratio of Greedy for\r\n  Reusable Resources\",\"  We study online weighted bipartite matching of reusable resources where an\r\nadversarial sequence of requests for resources arrive over time. A resource\r\nthat is matched is 'used' for a random duration, drawn independently from a\r\nresource-dependent distribution, after which it returns and is able to be\r\nmatched again. We study the performance of the greedy policy, which matches\r\nrequests to the resource that yields the highest reward. Previously, it was\r\nknown that the greedy policy is 1/2 competitive against a clairvoyant benchmark\r\nthat knows the request sequence in advance. In this work, we improve this\r\nresult by introducing a parameter that quantifies the degree of reusability of\r\nthe resources. Specifically, if p represents the smallest probability over the\r\nusage distributions that a matched resource returns in one time step, the\r\ngreedy policy achieves a competitive ratio of $1/(2-p)$. Furthermore, when the\r\nusage distributions are geometric, we establish a stronger competitive ratio of\r\n$(1+p)/2$, which we demonstrate to be tight. Both of these results align with\r\nthe known results in the two extreme scenarios: p = 0 corresponds to\r\nnon-reusable resources, where 1/2 is known to be tight, while p = 1 corresponds\r\nto every resource returning immediately, where greedy is the optimal policy and\r\nhence the competitive ratio is 1. Finally, we show that both results are robust\r\nto approximations of the greedy policy. Our work demonstrates that the\r\nreusability of resources can enhance performance compared to the non-reusable\r\nsetting, and that a simple greedy policy suffices when the degree of\r\nreusability is high. Our insights contribute to the understanding of how\r\nresource reusability can influence the performance of online algorithms, and\r\nhighlight the potential for improved performance as the degree of reusability\r\nincreases.\r\n\",\"['Jackie Baek', 'Shixin Wang']\"\r\nhttp://arxiv.org/abs/1210.8011v1,Reusable launch vehicle,2012-10-30T13:57:19Z,2012-10-30T13:57:19Z,Reusability Framework for Cloud Computing,\"  Cloud based development is a challenging task for several software\r\nengineering projects, especially for those which needs development with\r\nreusability. Present time of cloud computing is allowing new professional\r\nmodels for using the software development. The expected upcoming trend of\r\ncomputing is assumed to be this cloud computing because of speed of application\r\ndeployment, shorter time to market, and lower cost of operation. Until Cloud Co\r\nmputing Reusability Model is considered a fundamental capability, the speed of\r\ndeveloping services is very slow. Th is paper spreads cloud computing with\r\ncomponent based development named Cloud Co mputing Reusability Model (CCR) and\r\nenable reusability in cloud computing. In this paper Cloud Co mputing\r\nReusability Model has been proposed. The model has been validated by Cloudsim\r\nan d experimental result shows that reusability based cloud computing approach\r\nis effective in minimizing cost and time to market.\r\n\",\"['Sukhpal Singh', 'Rishideep Singh']\"\r\nhttp://arxiv.org/abs/2111.07002v1,Reusable launch vehicle,2021-11-13T00:19:03Z,2021-11-13T00:19:03Z,Refactoring for Reuse: An Empirical Study,\"  Refactoring is the de-facto practice to optimize software health. While\r\nseveral studies propose refactoring strategies to optimize software design\r\nthrough applying design patterns and removing design defects, little is known\r\nabout how developers actually refactor their code to improve its reuse.\r\nTherefore, we extract, from 1,828 open-source projects, a set of refactorings\r\nthat were intended to improve the software reusability. We analyze the impact\r\nof reusability refactorings on the state-of-the-art reusability metrics, and we\r\ncompare the distribution of reusability refactoring types, with the\r\ndistribution of the remaining mainstream refactorings. Overall, we found that\r\nthe distribution of refactoring types, applied in the context of reusability,\r\nis different from the distribution of refactoring types in mainstream\r\ndevelopment. In the refactorings performed to improve reusability, source files\r\nare subject to more design-level types of refactorings. Reusability\r\nrefactorings significantly impact, high-level code elements, such as packages,\r\nclasses, and methods, while typical refactorings, impact all code elements,\r\nincluding identifiers, and parameters. These findings provide practical\r\ninsights into the current practice of refactoring in the context of code reuse\r\ninvolving the act of refactoring.\r\n\",\"['Eman Abdullah AlOmar', 'Tianjia Wang', 'Vaibhavi Raut', 'Mohamed Wiem Mkaouer', 'Christian Newman', 'Ali Ouni']\"\r\nhttp://arxiv.org/abs/2309.07291v1,Reusable launch vehicle,2023-09-13T20:17:43Z,2023-09-13T20:17:43Z,Reusability Challenges of Scientific Workflows: A Case Study for Galaxy,\"  Scientific workflow has become essential in software engineering because it\r\nprovides a structured approach to designing, executing, and analyzing\r\nscientific experiments. Software developers and researchers have developed\r\nhundreds of scientific workflow management systems so scientists in various\r\ndomains can benefit from them by automating repetitive tasks, enhancing\r\ncollaboration, and ensuring the reproducibility of their results. However, even\r\nfor expert users, workflow creation is a complex task due to the dramatic\r\ngrowth of tools and data heterogeneity. Thus, scientists attempt to reuse\r\nexisting workflows shared in workflow repositories. Unfortunately, several\r\nchallenges prevent scientists from reusing those workflows. In this study, we\r\nthus first attempted to identify those reusability challenges. We also offered\r\nan action list and evidence-based guidelines to promote the reusability of\r\nscientific workflows. Our intensive manual investigation examined the\r\nreusability of existing workflows and exposed several challenges. The\r\nchallenges preventing reusability include tool upgrading, tool support\r\nunavailability, design flaws, incomplete workflows, failure to load a workflow,\r\netc. Such challenges and our action list offered guidelines to future workflow\r\ncomposers to create better workflows with enhanced reusability. In the future,\r\nwe plan to develop a recommender system using reusable workflows that can\r\nassist scientists in creating effective and error-free workflows.\r\n\",\"['Khairul Alam', 'Banani Roy', 'Alexander Serebrenik']\"\r\nhttp://arxiv.org/abs/2405.04021v1,Reusable launch vehicle,2024-05-07T05:48:02Z,2024-05-07T05:48:02Z,\"Robust and Reusable Fuzzy Extractors for Low-entropy Rate Randomness\r\n  Sources\",\"  Fuzzy extractors (FE) are cryptographic primitives that extract reliable\r\ncryptographic key from noisy real world random sources such as biometric\r\nsources. The FE generation algorithm takes a source sample, extracts a key and\r\ngenerates some helper data that will be used by the reproduction algorithm to\r\nrecover the key. Reusability of FE guarantees that security holds when FE is\r\nused multiple times with the same source, and robustness of FE requires\r\ntampering with the helper data be detectable.\r\n  In this paper, we consider information theoretic FEs, define a strong notion\r\nof reusability, and propose strongly robust and reusable FEs (srrFE) that\r\nprovides the strongest combined notion of reusability and robustness for FEs.\r\nWe give two constructions, one for reusable FEs and one for srrFE with\r\ninformation theoretic (IT) security for structured sources. The constructions\r\nare for structured sources and use sample-then-lock approach. We discuss each\r\nconstruction and show their unique properties in relation to existing work.\r\n  Construction 2 is the first robust and reusable FE with IT-security without\r\nassuming random oracle. The robustness is achieved by using an IT-secure MAC\r\nwith security against key-shift attack, which can be of independent interest.\r\n\",\"['Somnath Panja', 'Shaoquan Jiang', 'Reihaneh Safavi-Naini']\"\r\nhttp://arxiv.org/abs/1903.04165v2,Reusable launch vehicle,2019-03-11T08:18:32Z,2019-03-16T09:16:07Z,\"Object-oriented requirements: reusable, understandable, verifiable\",\"  Insufficient requirements reusability, understandability and verifiability\r\njeopardize software projects. Empirical studies show little success in\r\nimproving these qualities separately. Applying object-oriented thinking to\r\nrequirements leads to their unified treatment. An online library of reusable\r\nrequirement templates implements recurring requirement structures, offering a\r\nstarting point for practicing the unified approach.\r\n\",['Alexandr Naumchev']\r\nhttp://arxiv.org/abs/1007.5123v1,Reusable launch vehicle,2010-07-29T06:53:20Z,2010-07-29T06:53:20Z,\"Building Reusable Software Component For Optimization Check in ABAP\r\n  Coding\",\"  Software component reuse is the software engineering practice of developing\r\nnew software products from existing components. A reuse library or component\r\nreuse repository organizes stores and manages reusable components. This paper\r\ndescribes how a reusable component is created, how it reuses the function and\r\nchecking if optimized code is being used in building programs and applications.\r\nFinally providing coding guidelines, standards and best practices used for\r\ncreating reusable components and guidelines and best practices for making\r\nconfigurable and easy to use.\r\n\",\"['P. Shireesha', 'S. S. V. N. Sharma']\"\r\nhttp://arxiv.org/abs/1207.1173v1,Reusable launch vehicle,2012-07-05T07:22:15Z,2012-07-05T07:22:15Z,\"A Comprehensive Model to achieve Service Reusability for Multi level\r\n  stakeholders using Non-Functional attributes of Service Oriented Architecture\",\"  SOA is a prominent paradigm for accomplishing reuse of services. Service\r\nreusability is one dominant factor which has a greater influence on achieving\r\nquality in SOA systems. There exists sufficient research in this area and\r\nresearchers have contributed many works towards achieving quality in SOA\r\nsystems but much emphasis was not provided on service reusability [1] [2] [3].\r\nFew authors have addressed reusability factor with limited non-functional\r\nattributes. Our study focuses on identifying the non-functional attributes\r\nwhich have major or greater influence towards obtaining reusability in SOA\r\nsystems. The objective of this study goes into the next level, to categorize\r\nthe non-functional attributes on multi stakeholder's perspective i.e. Service\r\nConsumer, Service Provider and Service Developer which paves the way to build a\r\ncomprehensive quality model for achieving Service Reusability\r\n\",\"['Shanmugasundaram G.', 'V. Prasanna Venkatesan', 'C. Punitha Devi']\"\r\nhttp://arxiv.org/abs/2303.14959v1,Reusable launch vehicle,2023-03-27T07:45:14Z,2023-03-27T07:45:14Z,Some Initial Guidelines for Building Reusable Quantum Oracles,\"  The evolution of quantum hardware is highlighting the need for advances in\r\nquantum software engineering that help developers create quantum software with\r\ngood quality attributes. Specifically, reusability has been traditionally\r\nconsidered an important quality attribute in terms of efficiency of cost and\r\neffort. Increasing the reusability of quantum software will help developers\r\ncreate more complex solutions, by reusing simpler components, with better\r\nquality attributes, as long as the reused components have also these\r\nattributes. This work focuses on the reusability of oracles, a well-known\r\npattern of quantum algorithms that can be used to perform functions used as\r\ninput by other algorithms. In particular, in this work, we present several\r\nguidelines for making reusable quantum oracles. These guidelines include three\r\ndifferent levels for oracle reuse: the ideas inspiring the oracle, the function\r\nwhich creates the oracle, and the oracle itself. To demonstrate these\r\nguidelines, two different implementations of a range of integers oracle have\r\nbeen built by reusing simpler oracles. The quality of these implementations is\r\nevaluated in terms of functionality and quantum circuit depth. Then, we provide\r\nan example of documentation following the proposed guidelines for both\r\nimplementations to foster reuse of the provided oracles. This work aims to be a\r\nfirst point of discussion towards quantum software reusability. Additional work\r\nis needed to establish more specific criteria for quantum software reusability.\r\n\",\"['Javier Sanchez-Rivero', 'Daniel Talavn', 'Jose Garcia-Alonso', 'Antonio Ruiz-Corts', 'Juan Manuel Murillo']\"\r\nhttp://arxiv.org/abs/2310.06541v1,Reusable launch vehicle,2023-10-10T11:40:20Z,2023-10-10T11:40:20Z,\"Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A\r\n  Quantum Reinforcement Learning Approach\",\"  The advent of reusable rockets has heralded a new era in space exploration,\r\nreducing the costs of launching satellites by a significant factor. Traditional\r\nrockets were disposable, but the design of reusable rockets for repeated use\r\nhas revolutionized the financial dynamics of space missions. The most critical\r\nphase of reusable rockets is the landing stage, which involves managing the\r\ntremendous speed and attitude for safe recovery. The complexity of this task\r\npresents new challenges for control systems, specifically in terms of precision\r\nand adaptability. Classical control systems like the\r\nproportional-integral-derivative (PID) controller lack the flexibility to adapt\r\nto dynamic system changes, making them costly and time-consuming to redesign of\r\ncontroller. This paper explores the integration of quantum reinforcement\r\nlearning into the control systems of reusable rockets as a promising\r\nalternative. Unlike classical reinforcement learning, quantum reinforcement\r\nlearning uses quantum bits that can exist in superposition, allowing for more\r\nefficient information encoding and reducing the number of parameters required.\r\nThis leads to increased computational efficiency, reduced memory requirements,\r\nand more stable and predictable performance. Due to the nature of reusable\r\nrockets, which must be light, heavy computers cannot fit into them. In the\r\nreusable rocket scenario, quantum reinforcement learning, which has reduced\r\nmemory requirements due to fewer parameters, is a good solution.\r\n\",\"['Gyu Seon Kim', 'JaeHyun Chung', 'Soohyun Park']\"\r\nhttp://arxiv.org/abs/1411.1102v1,Reusable launch vehicle,2014-11-04T22:48:59Z,2014-11-04T22:48:59Z,\"Enhancing software module reusability using port plug-ins: an experiment\r\n  with the iCub robot\",\"  Systematically developing high--quality reusable software components is a\r\ndifficult task and requires careful design to find a proper balance between\r\npotential reuse, functionalities and ease of implementation. Extendibility is\r\nan important property for software which helps to reduce cost of development\r\nand significantly boosts its reusability. This work introduces an approach to\r\nenhance components reusability by extending their functionalities using\r\nplug-ins at the level of the connection points (ports). Application--dependent\r\nfunctionalities such as data monitoring and arbitration can be implemented\r\nusing a conventional scripting language and plugged into the ports of\r\ncomponents. The main advantage of our approach is that it avoids to introduce\r\napplication--dependent modifications to existing components, thus reducing\r\ndevelopment time and fostering the development of simpler and therefore more\r\nreusable components. Another advantage of our approach is that it reduces\r\ncommunication and deployment overheads as extra functionalities can be added\r\nwithout introducing additional modules.\r\n\",\"['Ali Paikan', 'Vadim Tikhanoff', 'Giorgio Metta', 'Lorenzo Natale']\"\r\nhttp://arxiv.org/abs/1608.00466v2,Reusable launch vehicle,2016-08-01T15:14:08Z,2016-10-10T03:57:26Z,\"Learning Semantically Coherent and Reusable Kernels in Convolution\r\n  Neural Nets for Sentence Classification\",\"  The state-of-the-art CNN models give good performance on sentence\r\nclassification tasks. The purpose of this work is to empirically study\r\ndesirable properties such as semantic coherence, attention mechanism and\r\nreusability of CNNs in these tasks. Semantically coherent kernels are\r\npreferable as they are a lot more interpretable for explaining the decision of\r\nthe learned CNN model. We observe that the learned kernels do not have semantic\r\ncoherence. Motivated by this observation, we propose to learn kernels with\r\nsemantic coherence using clustering scheme combined with Word2Vec\r\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\r\ntechnique to visualize attention mechanism of CNNs for decision explanation\r\npurpose. Reusable property enables kernels learned on one problem to be used in\r\nanother problem. This helps in efficient learning as only a few additional\r\ndomain specific filters may have to be learned. We demonstrate the efficacy of\r\nour core ideas of learning semantically coherent kernels and leveraging\r\nreusable kernels for efficient learning on several benchmark datasets.\r\nExperimental results show the usefulness of our approach by achieving\r\nperformance close to the state-of-the-art methods but with semantic and\r\nreusable properties.\r\n\",\"['Madhusudan Lakshmana', 'Sundararajan Sellamanickam', 'Shirish Shevade', 'Keerthi Selvaraj']\"\r\nhttp://arxiv.org/abs/2010.02756v2,Reusable launch vehicle,2020-10-06T14:21:05Z,2023-05-31T04:06:15Z,Learning Diverse Options via InfoMax Termination Critic,\"  We consider the problem of autonomously learning reusable temporally extended\r\nactions, or options, in reinforcement learning. While options can speed up\r\ntransfer learning by serving as reusable building blocks, learning reusable\r\noptions for unknown task distribution remains challenging. Motivated by the\r\nrecent success of mutual information (MI) based skill learning, we hypothesize\r\nthat more diverse options are more reusable. To this end, we propose a method\r\nfor learning termination conditions of options by maximizing MI between options\r\nand corresponding state transitions. We derive a scalable approximation of this\r\nMI maximization via gradient ascent, yielding the InfoMax Termination Critic\r\n(IMTC) algorithm. Our experiments demonstrate that IMTC significantly improves\r\nthe diversity of learned options without extrinsic rewards combined with an\r\nintrinsic option learning method. Moreover, we test the reusability of learned\r\noptions by transferring options into various tasks, confirming that IMTC helps\r\nquick adaptation, especially in complex domains where an agent needs to\r\nmanipulate objects.\r\n\",\"['Yuji Kanagawa', 'Tomoyuki Kaneko']\"\r\nhttp://arxiv.org/abs/2309.15175v1,Reusable launch vehicle,2023-09-26T18:19:38Z,2023-09-26T18:19:38Z,\"Large scale reuse of microservices using DevOps and InnerSource\r\n  practices -- A longitudinal case study\",\"  Contemporary practices such as InnerSource and DevOps promote software reuse.\r\nThis study investigates the implications of using contemporary practices on\r\nsoftware reuse. In particular, we investigate the costs, benefits, challenges,\r\nand potential improvements in contemporary reuse at Ericsson. We performed the\r\nstudy in two phases: a) the initial data collection based on a combination of\r\ndata collection methods (e.g., interviews, discussions, company portals), and\r\nb) a follow-up group discussion after a year to understand the status of the\r\nchallenges and improvements identified in the first phase. Our results indicate\r\nthat developing reusable assets resulted in upfront costs, such as additional\r\neffort in ensuring compliance. Furthermore, development with reuse also\r\nresulted in additional effort, for example, in integrating and understanding\r\nreusable assets. Ericsson perceived the additional effort as an investment\r\nresulting in long-term benefits such as improved quality, productivity,\r\ncustomer experience, and way of working. Ericsson's main challenge was\r\nincreased pressure on the producers of reusable assets, which was mitigated by\r\nscaling the InnerSource adoption. InnerSource success is evident from the\r\nincrease in the contributions to reusable assets. In addition, Ericsson\r\nimplemented measures such as automating the compliance check, which enhanced\r\nthe maturity of reusable assets and resulted in increased reuse.\r\n\",\"['Deepika Badampudi', 'Muhammad Usman', 'Xingru Chen']\"\r\nhttp://arxiv.org/abs/2403.00787v1,Reusable launch vehicle,2024-02-19T23:40:46Z,2024-02-19T23:40:46Z,\"Reusable MLOps: Reusable Deployment, Reusable Infrastructure and\r\n  Hot-Swappable Machine Learning models and services\",\"  Although Machine Learning model building has become increasingly accessible\r\ndue to a plethora of tools, libraries and algorithms being available freely,\r\neasy operationalization of these models is still a problem. It requires\r\nconsiderable expertise in data engineering, software development, cloud and\r\nDevOps. It also requires planning, agreement, and vision of how the model is\r\ngoing to be used by the business applications once it is in production, how it\r\nis going to be continuously trained on fresh incoming data, and how and when a\r\nnewer model would replace an existing model. This leads to developers and data\r\nscientists working in silos and making suboptimal decisions. It also leads to\r\nwasted time and effort. We introduce the Acumos AI platform we developed and we\r\ndemonstrate some unique novel capabilities that the Acumos model runner\r\npossesses, that can help solve the above problems. We introduce a new\r\nsustainable concept in the field of AI/ML operations - called Reusable MLOps -\r\nwhere we reuse the existing deployment and infrastructure to serve new models\r\nby hot-swapping them without tearing down the infrastructure or the\r\nmicroservice, thus achieving reusable deployment and operations for AI/ML\r\nmodels while still having continuously trained models in production.\r\n\",\"['D Panchal', 'P Verma', 'I Baran', 'D Musgrove', 'D Lu']\"\r\nhttp://arxiv.org/abs/1003.5777v1,Reusable launch vehicle,2010-03-30T09:57:26Z,2010-03-30T09:57:26Z,Specifying Reusable Components,\"  Reusable software components need expressive specifications. This paper\r\noutlines a rigorous foundation to model-based contracts, a method to equip\r\nclasses with strong contracts that support accurate design, implementation, and\r\nformal verification of reusable components. Model-based contracts\r\nconservatively extend the classic Design by Contract with a notion of model,\r\nwhich underpins the precise definitions of such concepts as abstract\r\nequivalence and specification completeness. Experiments applying model-based\r\ncontracts to libraries of data structures suggest that the method enables\r\naccurate specification of practical software.\r\n\",\"['Nadia Polikarpova', 'Carlo A. Furia', 'Bertrand Meyer']\"\r\nhttp://arxiv.org/abs/1202.5609v1,Reusable launch vehicle,2012-02-25T05:23:21Z,2012-02-25T05:23:21Z,A Framework Studio for Component Reusability,\"  The deployment of a software product requires considerable amount of time and\r\neffort. In order to increase the productivity of the software products,\r\nreusability strategies were proposed in the literature. However effective reuse\r\nis still a challenging issue. This paper presents a framework studio for\r\neffective components reusability which provides the selection of components\r\nfrom framework studio and generation of source code based on stakeholders\r\nneeds. The framework studio is implemented using swings which are integrated\r\nonto the Net Beans IDE which help in faster generation of the source code.\r\n\",\"['N Md Jubair Basha', 'Salman Abdul Moiz']\"\r\nhttp://arxiv.org/abs/1907.13114v1,Robotics,2019-07-30T17:56:17Z,2019-07-30T17:56:17Z,The Use of Agricultural Robots in Orchard Management,\"  Book chapter that summarizes recent research on agricultural robotics in\r\norchard management, including Robotic pruning, Robotic thinning, Robotic\r\nspraying, Robotic harvesting, Robotic fruit transportation, and future trends.\r\n\",\"['Qin Zhang', 'Manoj Karkee', 'Amy Tabb']\"\r\nhttp://arxiv.org/abs/2208.05095v1,Robotics,2022-08-10T01:02:57Z,2022-08-10T01:02:57Z,Robotics in Snow and Ice,\"  Definition: The terms \"\"robotics in snow and ice\"\" refers to robotic systems\r\nbeing studied, developed, and used in areas where water can be found in its\r\nsolid state. This specialized branch of field robotics investigates the impact\r\nof extreme conditions related to cold environments on autonomous vehicles.\r\n\",['Franois Pomerleau']\r\nhttp://arxiv.org/abs/2005.07474v1,Robotics,2020-05-15T11:31:54Z,2020-05-15T11:31:54Z,Robot Accident Investigation: a case study in Responsible Robotics,\"  Robot accidents are inevitable. Although rare, they have been happening since\r\nassembly-line robots were first introduced in the 1960s. But a new generation\r\nof social robots are now becoming commonplace. Often with sophisticated\r\nembedded artificial intelligence (AI) social robots might be deployed as care\r\nrobots to assist elderly or disabled people to live independently. Smart robot\r\ntoys offer a compelling interactive play experience for children and\r\nincreasingly capable autonomous vehicles (AVs) the promise of hands-free\r\npersonal transport and fully autonomous taxis. Unlike industrial robots which\r\nare deployed in safety cages, social robots are designed to operate in human\r\nenvironments and interact closely with humans; the likelihood of robot\r\naccidents is therefore much greater for social robots than industrial robots.\r\nThis paper sets out a draft framework for social robot accident investigation;\r\na framework which proposes both the technology and processes that would allow\r\nsocial robot accidents to be investigated with no less rigour than we expect of\r\nair or rail accident investigations. The paper also places accident\r\ninvestigation within the practice of responsible robotics, and makes the case\r\nthat social robotics without accident investigation would be no less\r\nirresponsible than aviation without air accident investigation.\r\n\",\"['Alan F. T. Winfield', 'Katie Winkle', 'Helena Webb', 'Ulrik Lyngs', 'Marina Jirotka', 'Carl Macrae']\"\r\nhttp://arxiv.org/abs/1403.2625v1,Robotics,2014-03-11T16:12:58Z,2014-03-11T16:12:58Z,Pattern Formation for Asynchronous Robots without Agreement in Chirality,\"  This paper presents a deterministic algorithm for forming a given asymmetric\r\npattern in finite time by a set of autonomous, homogeneous, oblivious mobile\r\nrobots under the CORDA model. The robots are represented as points on the 2D\r\nplane. There is no explicit communication between the robots. The robots\r\ncoordinate among themselves by observing the positions of the other robots on\r\nthe plane. Initially all the robots are assumed to be stationary. The robots\r\nhave local coordinate systems defined by Sense of Direction (SoD), orientation\r\nor chirality and scale. Initially the robots are in asymmetric configuration.\r\nWe show that these robots can form any given asymmetric pattern in finite time.\r\n\",\"['Sruti Gan Chaudhuri', 'Swapnil Ghike', 'Shrainik Jain', 'Krishnendu Mukhopadhyaya']\"\r\nhttp://arxiv.org/abs/1408.2072v1,Robotics,2014-08-09T07:43:54Z,2014-08-09T07:43:54Z,Formation of General Position by Asynchronous Mobile Robots,\"  The traditional distributed model of autonomous, homogeneous, mobile point\r\nrobots usually assumes that the robots do not create any visual obstruction for\r\nthe other robots, i.e., the robots are see through. In this paper, we consider\r\na slightly more realistic model, by incorporating the notion of obstructed\r\nvisibility (i.e., robots are not see through) for other robots. Under the new\r\nmodel of visibility, a robot may not have the full view of its surroundings.\r\nMany of the existing algorithms demand that each robot should have the complete\r\nknowledge of the positions of other robots. Since, vision is the only mean of\r\ntheir communication, it is required that the robots are in general position\r\n(i.e., no three robots are collinear). We consider asynchronous robots. They\r\nalso do not have common chirality (or any agreement on a global coordinate\r\nsystem). In this paper, we present a distributed algorithm for obtaining a\r\ngeneral position for the robots in finite time from any arbitrary\r\nconfiguration. The algorithm also assures collision free motion for each robot.\r\nThis algorithm may also be used as a preprocessing module for many other\r\nsubsequent tasks performed by the robots.\r\n\",\"['S. Bhagat', 'S. Gan Chaudhuri', 'K. Mukhopadhyaya']\"\r\nhttp://arxiv.org/abs/2210.05204v1,Robotics,2022-10-11T07:19:04Z,2022-10-11T07:19:04Z,A review of cuspidal serial and parallel manipulators,\"  Cuspidal robots can move from one inverse or direct kinematic solution to\r\nanother without ever passing through a singularity. These robots have remained\r\nunknown because almost all industrial robots do not have this feature. However,\r\nin fact, industrial robots are the exceptions. Some robots appeared recently in\r\nthe industrial market can be shown to be cuspidal but, surprisingly, almost\r\nnobody knows it and robot users meet difficulties in planning trajectories with\r\nthese robots. This paper proposes a review on the fundamental and application\r\naspects of cuspidal robots. It addresses the important issues raised by these\r\nrobots for the design and planning of trajectories. The identification of all\r\ncuspidal robots is still an open issue. This paper recalls in details the case\r\nof serial robots with three joints but it also addresses robots with more\r\ncomplex architectures such as 6-revolute-jointed robot and parallel robots. We\r\nhope that this paper will help disseminate more widely knowledge on cuspidal\r\nrobots.\r\n\",\"['Philippe Wenger', 'Damien Chablat']\"\r\nhttp://arxiv.org/abs/2408.05491v1,Robotics,2024-08-10T08:43:07Z,2024-08-10T08:43:07Z,Optimal Dispersion of Silent Robots in a Ring,\"  Given a set of co-located mobile robots in an unknown anonymous graph, the\r\nrobots must relocate themselves in distinct graph nodes to solve the dispersion\r\nproblem. In this paper, we consider the dispersion problem for silent robots\r\n\\cite{gorain2024collaborative}, i.e., no direct, explicit communication between\r\nany two robots placed in the nodes of an oriented $n$ node ring network. The\r\nrobots operate in synchronous rounds. The dispersion problem for silent mobile\r\nrobots has been studied in arbitrary graphs where the robots start from a\r\nsingle source. In this paper, we focus on the dispersion problem for silent\r\nmobile robots where robots can start from multiple sources. The robots have\r\nunique labels from a range $[0,\\;L]$ for some positive integer $L$. Any two\r\nco-located robots do not have the information about the label of the other\r\nrobot. The robots have weak multiplicity detection capability, which means they\r\ncan determine if it is alone on a node. The robots are assumed to be able to\r\nidentify an increase or decrease in the number of robots present on a node in a\r\nparticular round. However, the robots can not get the exact number of increase\r\nor decrease in the number of robots. We have proposed a deterministic\r\ndistributed algorithm that solves the dispersion of $k$ robots in an oriented\r\nring in $O(\\log L+k)$ synchronous rounds with $O(\\log L)$ bits of memory for\r\neach robot. A lower bound $\\Omega(\\log L+k)$ on time for the dispersion of $k$\r\nrobots on a ring network is presented to establish the optimality of the\r\nproposed algorithm.\r\n\",\"['Bibhuti Das', 'Barun Gorain', 'Kaushik Mondal', 'Krishnendu Mukhopadhyaya', 'Supantha Pandit']\"\r\nhttp://arxiv.org/abs/cs/0411018v1,Robotics,2004-11-08T20:41:44Z,2004-11-08T20:41:44Z,\"Artificial Intelligence and Systems Theory: Applied to Cooperative\r\n  Robots\",\"  This paper describes an approach to the design of a population of cooperative\r\nrobots based on concepts borrowed from Systems Theory and Artificial\r\nIntelligence. The research has been developed under the SocRob project, carried\r\nout by the Intelligent Systems Laboratory at the Institute for Systems and\r\nRobotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the\r\nproject stands both for \"\"Society of Robots\"\" and \"\"Soccer Robots\"\", the case study\r\nwhere we are testing our population of robots. Designing soccer robots is a\r\nvery challenging problem, where the robots must act not only to shoot a ball\r\ntowards the goal, but also to detect and avoid static (walls, stopped robots)\r\nand dynamic (moving robots) obstacles. Furthermore, they must cooperate to\r\ndefeat an opposing team. Our past and current research in soccer robotics\r\nincludes cooperative sensor fusion for world modeling, object recognition and\r\ntracking, robot navigation, multi-robot distributed task planning and\r\ncoordination, including cooperative reinforcement learning in cooperative and\r\nadversarial environments, and behavior-based architectures for real time task\r\nexecution of cooperating robot teams.\r\n\",\"['Pedro U. Lima', 'Luis M. M. Custodio']\"\r\nhttp://arxiv.org/abs/0808.1661v1,Robotics,2008-08-12T13:21:52Z,2008-08-12T13:21:52Z,\"Medical robotics: where we come from, where we are and where we could go\",\"  This short note presents a viewpoint about medical robotics.\r\n\",['Jocelyne Troccaz']\r\nhttp://arxiv.org/abs/1701.07790v2,Robotics,2017-01-26T17:45:47Z,2017-04-06T02:26:42Z,Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration,\"  In human-robot teams, humans often start with an inaccurate model of the\r\nrobot capabilities. As they interact with the robot, they infer the robot's\r\ncapabilities and partially adapt to the robot, i.e., they might change their\r\nactions based on the observed outcomes and the robot's actions, without\r\nreplicating the robot's policy. We present a game-theoretic model of human\r\npartial adaptation to the robot, where the human responds to the robot's\r\nactions by maximizing a reward function that changes stochastically over time,\r\ncapturing the evolution of their expectations of the robot's capabilities. The\r\nrobot can then use this model to decide optimally between taking actions that\r\nreveal its capabilities to the human and taking the best action given the\r\ninformation that the human currently has. We prove that under certain\r\nobservability assumptions, the optimal policy can be computed efficiently. We\r\ndemonstrate through a human subject experiment that the proposed model\r\nsignificantly improves human-robot team performance, compared to policies that\r\nassume complete adaptation of the human to the robot.\r\n\",\"['Stefanos Nikolaidis', 'Swaprava Nath', 'Ariel D. Procaccia', 'Siddhartha Srinivasa']\"\r\nhttp://arxiv.org/abs/1812.06784v4,Robotics,2018-12-17T14:21:37Z,2019-04-24T08:44:46Z,\"Animation Techniques in Human-Robot Interaction User Studies: a\r\n  Systematic Literature Review\",\"  There are many different ways a robot can move in Human-Robot Interaction.\r\nOne way is to use techniques from film animation to instruct the robot to move.\r\nThis article is a systematic literature review of human-robot trials, pilots,\r\nand evaluations that have applied techniques from animation to move a robot.\r\nThrough 27 articles, we find that animation techniques improves individual's\r\ninteraction with robots, improving individual's perception of qualities of a\r\nrobot, understanding what a robot intends to do, and showing the robot's state,\r\nor possible emotion. Animation techniques also help people relate to robots\r\nthat do not resemble a human or robot. The studies in the articles show further\r\nareas for research, such as applying animation principles in other types of\r\nrobots and situations, combining animation techniques with other modalities,\r\nand testing robots moving with animation techniques over the long term.\r\n\",\"['Trenton Schulz', 'Jim Torresen', 'Jo Herstad']\"\r\nhttp://arxiv.org/abs/1904.03049v2,Robotics,2019-04-05T13:17:27Z,2019-09-08T09:23:21Z,Loosely Coupled Payload Transport System with Robot Replacement,\"  In this work, we present an algorithm for robot replacement to increase the\r\noperational time of a multi-robot payload transport system. Our system\r\ncomprises a group of nonholonomic wheeled mobile robots traversing on a known\r\ntrajectory. We design a multi-robot system with loosely coupled robots that\r\nensures the system lasts much longer than the battery life of an individual\r\nrobot. A system level optimization is presented, to decide on the operational\r\nstate (charging or discharging) of each robot in the system. The charging state\r\nimplies that the robot is not in a formation and is kept on charge whereas the\r\ndischarging state implies that the robot is a part of the formation. Robot\r\nbattery recharge hubs are present along the trajectory. Robots in the formation\r\ncan be replaced at these hub locations with charged robots using a replacement\r\nmechanism. We showcase the efficacy of the proposed scheduling framework\r\nthrough simulations and experiments with real robots.\r\n\",\"['Pulkit Verma', 'Rahul Tallamraju', 'Abhay Rawat', 'Subhasis Chand', 'Kamalakar Karlapalem']\"\r\nhttp://arxiv.org/abs/1909.05777v1,Robotics,2019-09-12T16:16:21Z,2019-09-12T16:16:21Z,Robots that Take Advantage of Human Trust,\"  Humans often assume that robots are rational. We believe robots take optimal\r\nactions given their objective; hence, when we are uncertain about what the\r\nrobot's objective is, we interpret the robot's actions as optimal with respect\r\nto our estimate of its objective. This approach makes sense when robots\r\nstraightforwardly optimize their objective, and enables humans to learn what\r\nthe robot is trying to achieve. However, our insight is that---when robots are\r\naware that humans learn by trusting that the robot actions are\r\nrational---intelligent robots do not act as the human expects; instead, they\r\ntake advantage of the human's trust, and exploit this trust to more efficiently\r\noptimize their own objective. In this paper, we formally model instances of\r\nhuman-robot interaction (HRI) where the human does not know the robot's\r\nobjective using a two-player game. We formulate different ways in which the\r\nrobot can model the uncertain human, and compare solutions of this game when\r\nthe robot has conservative, optimistic, rational, and trusting human models. In\r\nan offline linear-quadratic case study and a real-time user study, we show that\r\ntrusting human models can naturally lead to communicative robot behavior, which\r\ninfluences end-users and increases their involvement.\r\n\",\"['Dylan P. Losey', 'Dorsa Sadigh']\"\r\nhttp://arxiv.org/abs/2207.01684v1,Robotics,2022-07-04T19:26:13Z,2022-07-04T19:26:13Z,\"Robot Vitals and Robot Health: Towards Systematically Quantifying\r\n  Runtime Performance Degradation in Robots Under Adverse Conditions\",\"  This paper addresses the problem of automatically detecting and quantifying\r\nperformance degradation in remote mobile robots during task execution. A robot\r\nmay encounter a variety of uncertainties and adversities during task execution,\r\nwhich can impair its ability to carry out tasks effectively and cause its\r\nperformance to degrade. Such situations can be mitigated or averted by timely\r\ndetection and intervention (e.g., by a remote human supervisor taking over\r\ncontrol in teleoperation mode). Inspired by patient triaging systems in\r\nhospitals, we introduce the framework of \"\"robot vitals\"\" for estimating overall\r\n\"\"robot health\"\". A robot's vitals are a set of indicators that estimate the\r\nextent of performance degradation faced by a robot at a given point in time.\r\nRobot health is a metric that combines robot vitals into a single scalar value\r\nestimate of performance degradation. Experiments, both in simulation and on a\r\nreal mobile robot, demonstrate that the proposed robot vitals and robot health\r\ncan be used effectively to estimate robot performance degradation during\r\nruntime.\r\n\",\"['Aniketh Ramesh', 'Rustam Stolkin', 'Manolis Chiou']\"\r\nhttp://arxiv.org/abs/2309.02979v1,Robotics,2023-09-06T13:24:45Z,2023-09-06T13:24:45Z,\"Come Closer: The Effects of Robot Personality on Human Proxemics\r\n  Behaviours\",\"  Social Robots in human environments need to be able to reason about their\r\nphysical surroundings while interacting with people. Furthermore, human\r\nproxemics behaviours around robots can indicate how people perceive the robots\r\nand can inform robot personality and interaction design. Here, we introduce\r\nCharlie, a situated robot receptionist that can interact with people using\r\nverbal and non-verbal communication in a dynamic environment, where users might\r\nenter or leave the scene at any time. The robot receptionist is stationary and\r\ncannot navigate. Therefore, people have full control over their personal space\r\nas they are the ones approaching the robot. We investigated the influence of\r\ndifferent apparent robot personalities on the proxemics behaviours of the\r\nhumans. The results indicate that different types of robot personalities,\r\nspecifically introversion and extroversion, can influence human proxemics\r\nbehaviours. Participants maintained shorter distances with the introvert robot\r\nreceptionist, compared to the extrovert robot. Interestingly, we observed that\r\nhuman-robot proxemics were not the same as typical human-human interpersonal\r\ndistances, as defined in the literature. We therefore propose new proxemics\r\nzones for human-robot interaction.\r\n\",\"['Meriam Moujahid', 'David A. Robb', 'Christian Dondrup', 'Helen Hastie']\"\r\nhttp://arxiv.org/abs/2502.01256v1,Robotics,2025-02-03T11:26:32Z,2025-02-03T11:26:32Z,Soft is Safe: Human-Robot Interaction for Soft Robots,\"  With the presence of robots increasing in the society, the need for\r\ninteracting with robots is becoming necessary. The field of Human-Robot\r\nInteraction (HRI) has emerged important since more repetitive and tiresome jobs\r\nare being done by robots. In the recent times, the field of soft robotics has\r\nseen a boom in the field of research and commercialization. The Industry 5.0\r\nfocuses on human robot collaboration which also spurs the field of soft\r\nrobotics. However the HRI for soft robotics is still in the nascent stage. In\r\nthis work we review and then discuss how HRI is done for soft robots. We first\r\ndiscuss the control, design, materials and manufacturing of soft robots. This\r\nwill provide an understanding of what is being interacted with. Then we discuss\r\nabout the various input and output modalities that are used in HRI. The\r\napplications where the HRI for soft robots are found in the literature are\r\ndiscussed in detail. Then the limitations of HRI for soft robots and various\r\nresearch opportunities that exist in this field are discussed in detail. It is\r\nconcluded that there is a huge scope for development for HRI for soft robots.\r\n\",\"['Rajashekhar V S', 'Gowdham Prabhakar']\"\r\nhttp://arxiv.org/abs/2211.05572v1,Robotics,2022-10-24T13:26:18Z,2022-10-24T13:26:18Z,Modular Robots: extending the capabilities of one robot,\"  For a robot to be perfect and enter the everyday life of humans,like\r\ncomputers did, it needs to move from special-purpose robots to general-purpose.\r\nSo, the idea of modularity is considered in this project.Thus, any type of task\r\nthat falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be\r\nachieved by adding a module to the robot.\r\n\",\"['Aymen Rachdi', 'Fedi Zrelli', 'Amine Kammmoun']\"\r\nhttp://arxiv.org/abs/1610.04080v2,Robotics,2016-10-13T13:58:59Z,2016-12-08T13:26:59Z,Cuspidal Robots,\"  This chapter is dedicated to the so-called cuspidal robots, i.e. those robots\r\nthat can move from one inverse geometric solution to another without meeting a\r\nsingular confuguration. This feature was discovered quite recently and has then\r\nbeen fascinating a lot of researchers. After a brief history of cuspidal\r\nrobots, the chapter provides the main features of cuspidal robots: explanation\r\nof the non-singular change of posture, uniqueness domains, regions of feasible\r\npaths, identification and classification of cuspidal robots. The chapter\r\nfocuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel\r\nrobots is discussed in the end of this chapter.\r\n\",['Philippe Wenger']\r\nhttp://arxiv.org/abs/1804.06383v1,Robotics,2018-04-17T17:26:30Z,2018-04-17T17:26:30Z,Effects of Interruptibility-Aware Robot Behavior,\"  As robots become increasingly prevalent in human environments, there will\r\ninevitably be times when a robot needs to interrupt a human to initiate an\r\ninteraction. Our work introduces the first interruptibility-aware mobile robot\r\nsystem, and evaluates the effects of interruptibility-awareness on human task\r\nperformance, robot task performance, and on human interpretation of the robot's\r\nsocial aptitude. Our results show that our robot is effective at predicting\r\ninterruptibility at high accuracy, allowing it to interrupt at more appropriate\r\ntimes. Results of a large-scale user study show that while participants are\r\nable to maintain task performance even in the presence of interruptions,\r\ninterruptibility-awareness improves the robot's task performance and improves\r\nparticipant social perception of the robot.\r\n\",\"['Siddhartha Banerjee', 'Andrew Silva', 'Karen Feigh', 'Sonia Chernova']\"\r\nhttp://arxiv.org/abs/1805.03737v2,Robotics,2018-05-09T21:24:50Z,2019-01-27T13:42:51Z,Graph Neural Networks for Learning Robot Team Coordination,\"  This paper shows how Graph Neural Networks can be used for learning\r\ndistributed coordination mechanisms in connected teams of robots. We capture\r\nthe relational aspect of robot coordination by modeling the robot team as a\r\ngraph, where each robot is a node, and edges represent communication links.\r\nDuring training, robots learn how to pass messages and update internal states,\r\nso that a target behavior is reached. As a proxy for more complex problems,\r\nthis short paper considers the problem where each robot must locally estimate\r\nthe algebraic connectivity of the team's network topology.\r\n\",['Amanda Prorok']\r\nhttp://arxiv.org/abs/2202.08118v1,Smart contracts,2022-02-16T14:53:18Z,2022-02-16T14:53:18Z,\"Smart Cities, Smart Libraries and Smart Knowledge Managers: Ushering in\r\n  the neo-Knowledge Society\",\"  The emergence of smart cities as a specific concept is not very old. In\r\nsimple terms, it refers to cities which are sustainable and driven\r\npredominantly by their Information and Communication Technology (ICT)\r\ninfrastructure. Smart libraries and smart knowledge managers, alongside its\r\nother smart component-entities, are vital for their emergence, sustenance and\r\nprogress. The paper attempts at deducing a symbiosis amongst smart cities,\r\nsmart libraries and smart knowledge managers. It further elaborates on how\r\nthese will usher in the neo-knowledge society, and the opportunities it'll\r\noffer vis-\\`a-vis Library and Information Science (LIS). Finally, it concludes\r\non an optimistic note, mentioning possible future research activities in this\r\nregard.\r\n\",['Mayukh Bagchi']\r\nhttp://arxiv.org/abs/2309.12344v1,Smart contracts,2023-08-25T19:23:33Z,2023-08-25T19:23:33Z,\"Exploring IoT in Smart Cities: Practices, Challenges and Way Forward\",\"  The rise of Internet of things (IoT) technology has revolutionized urban\r\nliving, offering immense potential for smart cities in which smart home, smart\r\ninfrastructure, and smart industry are essential aspects that contribute to the\r\ndevelopment of intelligent urban ecosystems. The integration of smart home\r\ntechnology raises concerns regarding data privacy and security, while smart\r\ninfrastructure implementation demands robust networking and interoperability\r\nsolutions. Simultaneously, deploying IoT in industrial settings faces\r\nchallenges related to scalability, standardization, and data management. This\r\nresearch paper offers a systematic literature review of published research in\r\nthe field of IoT in smart cities including 55 relevant primary studies that\r\nhave been published in reputable journals and conferences. This extensive\r\nliterature review explores and evaluates various aspects of smart home, smart\r\ninfrastructure, and smart industry and the challenges like security and\r\nprivacy, smart sensors, interoperability and standardization. We provide a\r\nunified perspective, as we seek to enhance the efficiency and effectiveness of\r\nsmart cities while overcoming security concerns. It then explores their\r\npotential for collective integration and impact on the development of smart\r\ncities. Furthermore, this study addresses the challenges associated with each\r\ncomponent individually and explores their combined impact on enhancing urban\r\nefficiency and sustainability. Through a comprehensive analysis of security\r\nconcerns, this research successfully integrates these IoT components in a\r\nunified approach, presenting a holistic framework for building smart cities of\r\nthe future. Integrating smart home, smart infrastructure, and smart industry,\r\nthis research highlights the significance of an integrated approach in\r\ndeveloping smart cities.\r\n\",\"['Kashif Ishaq', 'Syed Shah Farooq']\"\r\nhttp://arxiv.org/abs/1807.08165v1,Smart contracts,2018-07-21T14:57:49Z,2018-07-21T14:57:49Z,\"On Computational Infraestruture Requirements to Smart and Autonomic\r\n  Cities Framework\",\"  Smart cities are an actual trend being pursued by research that,\r\nfundamentally, tries to improve city's management on behalf of a better human\r\nquality of live. This paper proposes a new autonomic complementary approach for\r\nsmart cities management. It is argued that smart city management systems with\r\nautonomic characteristics will improve and facilitate management\r\nfunctionalities in general. A framework is also presented as use case\r\nconsidering specific application scenarios like smart-health, smart-grid,\r\nsmart-environment and smart-streets.\r\n\",\"['Romildo Bezerra', 'Flavia Nascimento', 'Joberto Martins']\"\r\nhttp://arxiv.org/abs/1804.01242v1,Smart contracts,2018-04-04T05:27:09Z,2018-04-04T05:27:09Z,A Smart Home Gateway Platform for Data Collection and Awareness,\"  Smart homes have attracted much attention due to the expanding of\r\nInternet-of-Things (IoT) and smart devices. In this paper, we propose a smart\r\ngateway platform for data collection and awareness in smart home networks. A\r\nsmart gateway will replace the traditional network gateway to connect the home\r\nnetwork and the Internet. A smart home network supports different types of\r\nsmart devices, such as in home IoT devices, smart phones, smart electric\r\nappliances, etc. A traditional network gateway is not capable of providing\r\nquality-of-service measurement, user behavioral analytics, or network\r\noptimization. Such tasks are traditionally performed with measurement agents\r\nsuch as optical splitters or network probes deployed in the core network. Our\r\nproposed platform is a lightweight plug-in for the smart gateway to accomplish\r\ndata collection, awareness and reporting. While the smart gateway is able to\r\nadjust the control policy for data collection and awareness locally, a\r\ncloud-based controller is also included for more refined control policy\r\nupdates. Furthermore, we propose a multi-dimensional awareness framework to\r\nachieve accurate data awareness at the smart gateway. The efficiency of data\r\ncollection and accuracy of data awareness of the proposed platform is\r\ndemonstrated based on the tests using actual data traffic from a large number\r\nof smart home users.\r\n\",\"['Pan Wang', 'Feng Ye', 'Xuejiao Chen']\"\r\nhttp://arxiv.org/abs/2209.10300v1,Smart contracts,2022-09-21T12:14:46Z,2022-09-21T12:14:46Z,5G-Enabled Smart Manufacturing -- A booklet by 5G-SMART,\"  In this booklet the most important learnings and key results of 5G-SMART in\r\nthe area of smart manufacturing are summarized.\r\n\",\"['Leefke Grosjean', 'Krister Landerns', 'Berna Sayrac', 'Ognjen Dobrijevic', 'Niels Knig', 'Davit Harutyunyan', 'Dhruvin Patel', 'Jose F. Monserrat', 'Joachim Sachs']\"\r\nhttp://arxiv.org/abs/2109.05581v1,Smart contracts,2021-09-12T18:33:24Z,2021-09-12T18:33:24Z,Data Analytics for Smart cities: Challenges and Promises,\"  The explosion of advancements in artificial intelligence, sensor\r\ntechnologies, and wireless communication activates ubiquitous sensing through\r\ndistributed sensors. These sensors are various domains of networks that lead us\r\nto smart systems in healthcare, transportation, environment, and other relevant\r\nbranches/networks. Having collaborative interaction among the smart systems\r\nconnects end-user devices to each other which enables achieving a new\r\nintegrated entity called Smart Cities. The goal of this study is to provide a\r\ncomprehensive survey of data analytics in smart cities. In this paper, we aim\r\nto focus on one of the smart cities important branches, namely Smart Mobility,\r\nand its positive ample impact on the smart cities decision-making process.\r\nIntelligent decision-making systems in smart mobility offer many advantages\r\nsuch as saving energy, relaying city traffic, and more importantly, reducing\r\nair pollution by offering real-time useful information and imperative\r\nknowledge. Making a decision in smart cities in time is challenging due to\r\nvarious and high dimensional factors and parameters, which are not frequently\r\ncollected. In this paper, we first address current challenges in smart cities\r\nand provide an overview of potential solutions to these challenges. Then, we\r\noffer a framework of these solutions, called universal smart cities decision\r\nmaking, with three main sections of data capturing, data analysis, and decision\r\nmaking to optimize the smart mobility within smart cities. With this framework,\r\nwe elaborate on fundamental concepts of big data, machine learning, and deep\r\nleaning algorithms that have been applied to smart cities and discuss the role\r\nof these algorithms in decision making for smart mobility in smart cities.\r\n\",\"['Farid Ghareh Mohammadi', 'Farzan Shenavarmasouleh', 'M. Hadi Amini', 'Hamid R. Arabnia']\"\r\nhttp://arxiv.org/abs/1711.09184v1,Smart contracts,2017-11-25T04:06:24Z,2017-11-25T04:06:24Z,A Formal Specification Framework for Smart Grid Components,\"  Smart grid can be considered as the next step in the evolution of power\r\nsystems. It comprises of different entities and objects ranging from smart\r\nappliances, smart meters, generators, smart storages, and more. One key problem\r\nin modeling smart grid is that while currently there is a considerable focus on\r\nthe practical aspects of smart grid, there are very few modeling attempts and\r\neven lesser attempts at formalization. To the best of our knowledge, among\r\nother formal methods, formal specification has previously not been applied in\r\nthe domain of smart grid. In this paper, we attempt to bridge this gap by\r\npresenting a novel approach to modeling smart grid components using a formal\r\nspecification approach. We use a state-based formal specification language\r\nnamely Z (pronounced as `Zed') since we believe Z is particularly suited for\r\nmodeling smart grid components.We demonstrate the application of Z on key smart\r\ngrid components. The presented formal specification can be considered as first\r\nsteps towards modeling of smart grid using a Software Engineering formalism. It\r\nalso demonstrates how complex systems, such as the smart grid, can be modeled\r\nelegantly using formal specification.\r\n\",\"['Waseem Akram', 'Muaz A. Niazi']\"\r\nhttp://arxiv.org/abs/1909.02914v1,Smart contracts,2019-09-06T13:55:14Z,2019-09-06T13:55:14Z,\"Blockchain Technologies for Smart Energy Systems: Fundamentals,\r\n  Challenges and Solutions\",\"  In this paper, we discuss the integration of blockchain in smart energy\r\nsystems. We present various blockchain technology solutions, review important\r\nblockchain platforms, and several blockchain based smart energy projects in\r\ndifferent smart energy domains. The majority of blockchain platforms with\r\nembedded combination of blockchain technology solutions are computing- and\r\nresource- intensive, and hence not entirely suitable for smart energy\r\napplications. We consider the requirements of smart energy systems and\r\naccordingly identify appropriate blockchain technology solutions for smart\r\nenergy applications. Our analysis can help in the development of flexible\r\nblockchain platforms for smart energy systems.\r\n\",\"['Naveed UL Hassan', 'Chau Yuen', 'Dusit Niyato']\"\r\nhttp://arxiv.org/abs/2405.06930v1,Smart contracts,2024-05-11T06:26:07Z,2024-05-11T06:26:07Z,\"Extended Reality for Smart Built Environments Design: Smart Lighting\r\n  Design Testbed\",\"  Smart Built Environment is an eco-system of `connected' and `smart' Internet\r\nof Things (IoT) devices that are embedded in a built environment. Smart\r\nlighting is an important category of smart IoT devices that has recently\r\nattracted research interest, particularly for residential areas. In this paper,\r\nwe present an extended reality based smart lighting design testbed that can\r\ngenerate design prototypes based on the functionality of the physical\r\nenvironment. The emphasis is on designing a smart lighting system in a\r\ncontrolled residential environment, with some evaluation of well-being and\r\ncomfort.\r\n\",\"['Elham Mohammadrezaei', 'Denis Gracanin']\"\r\nhttp://arxiv.org/abs/1610.06855v1,Smart contracts,2016-10-21T16:50:11Z,2016-10-21T16:50:11Z,Big Data: Perspektiven fuer Smart Grids und Smart Buildings,\"  This paper gives a short survey of recent trends in the emerging field of big\r\ndata. It explains the definitions and useful methods. In addition, application\r\nfields of smart buildings and smart grids are discussed.\r\n\",['Ralf Mikut']\r\nhttp://arxiv.org/abs/1912.04780v2,Smart contracts,2019-12-10T15:52:52Z,2019-12-24T09:04:27Z,Testing Smart Contracts Gets Smarter,\"  Smart contracts are immutable, verifiable, and autonomous pieces of code that\r\ncan be deployed and ran on blockchain networks like Ethereum. Due to the\r\nimmutability nature of blockchain, no change is possible on a deployed smart\r\ncontract or a verified transaction. On the other hand, there are millions of\r\ndollars carried by smart contracts in Ethereum blockchain, and hence, a faulty\r\nsmart contract can lead to a huge monetary loss. Therefore, it is important for\r\nsmart contract developers to fully test and check the correctness of their code\r\nbefore deploying it on the blockchain. In this paper, we propose a testing\r\nmechanism for smart contracts in Solidity language, based on mutation testing.\r\nWe analyzed a comprehensive list of known bugs in Solidity smart contracts, and\r\ndesigned 10 classes of mutation operators inspired by the real faults. Our\r\nexperimental results show that our proposed mutation operators can regenerate\r\n10 of 15 famous faulty smart contracts, which have resulted in millions of\r\ndollars loss. The results show the effectiveness of our proposed mutation\r\noperators in detecting real faults in Solidity smart contracts. We have also\r\nextended {\\em Universal Mutator } tool with our mutation operators, so that it\r\ncan automatically generate mutants for smart contracts written in Solidity.\r\n\",\"['Erfan Andesta', 'Fathiyeh Faghih', 'Mahdi Fooladgar']\"\r\nhttp://arxiv.org/abs/2206.02760v1,Smart contracts,2022-06-06T17:37:51Z,2022-06-06T17:37:51Z,Blockchain for the Cybersecurity of Smart City Applications,\"  Cybersecurity is an inherent characteristic that should be addressed before\r\nthe large deployment of smart city applications. Recently, Blockchain appears\r\nas a promising technology to provide several cybersecurity aspects of smart\r\ncity applications. This paper provides a comprehensive review of the existing\r\nblockchain-based solutions for the cybersecurity of the main smart city\r\napplications, namely smart healthcare, smart transportation, smart agriculture,\r\nsupply chain management, smart grid, and smart homes. We describe the existing\r\nsolutions and we discuss their merits and limits. Moreover, we define the\r\nsecurity requirements of each smart city application and we give a mapping of\r\nthe studied solutions to these defined requirements. Additionally, future\r\ndirections are given. We believe that the present survey is a good starting\r\npoint for every researcher in the fields of cybersecurity, blockchain, and\r\nsmart cities.\r\n\",\"['Omar Cheikhrouhou', 'Ichrak Amdouni', 'Khaleel Mershad', 'Maryem Ammi', 'Tuan Nguyen Gia']\"\r\nhttp://arxiv.org/abs/2207.04424v1,Smart contracts,2022-07-10T09:22:10Z,2022-07-10T09:22:10Z,\"An Overview of Cyber Threats, Attacks, and Countermeasures on the\r\n  Primary Domains of Smart Cities\",\"  A smart city is a place where existing facilities and services are enhanced\r\nby digital technology to benefit people and companies. The most critical\r\ninfrastructures in this city are interconnected. Increased data exchange across\r\nmunicipal domains aims to manage the essential assets, leading to more\r\nautomation in city governance and optimization of the dynamic offered services.\r\nHowever, no clear guideline or standard exists for modeling these data flows.\r\nAs a result, operators, municipalities, policymakers, manufac-turers, solution\r\nproviders, and vendors are forced to accept systems with limited scalability\r\nand varying needs. Nonetheless, it is critical to raise awareness about smart\r\ncity cybersecurity and implement suitable measures to safeguard citizens'\r\nprivacy and security because the cyber threats seem to be well-organized,\r\ndiverse, and sophisticated. This study aims to present an overview of cyber\r\nthreats, attacks, and countermeasures on the primary domains of smart cities\r\n(smart government, smart mobility, smart environment, smart living, smart\r\nhealthcare, smart economy, and smart people) to present information extracted\r\nfrom state-of-the-art to policymakers to perceive the critical situation and,\r\nat the same time, to be a valuable resource for the scientific community.\r\n\",\"['Vasiliki Demertzi', 'Stavros Demertzis', 'Konstantinos Demertzis']\"\r\nhttp://arxiv.org/abs/2402.00568v1,Smart contracts,2024-02-01T13:01:47Z,2024-02-01T13:01:47Z,Secure Supervised Learning-Based Smart Home Authentication Framework,\"  The Smart home possesses the capability of facilitating home services to\r\ntheir users with the systematic advance in The Internet of Things (IoT) and\r\ninformation and communication technologies (ICT) in recent decades. The home\r\nservice offered by the smart devices helps the users in utilize maximized level\r\nof comfort for the objective of improving life quality. As the user and smart\r\ndevices communicate through an insecure channel, the smart home environment is\r\nprone to security and privacy problems. A secure authentication protocol needs\r\nto be established between the smart devices and the user, such that a situation\r\nfor device authentication can be made feasible in smart home environments. Most\r\nof the existing smart home authentication protocols were identified to fail in\r\nfacilitating a secure mutual authentication and increases the possibility of\r\nlunching the attacks of session key disclosure, impersonation and stolen smart\r\ndevice. In this paper, Secure Supervised Learning-based Smart Home\r\nAuthentication Framework (SSL-SHAF) is proposed as are liable mutual\r\nauthentication that can be contextually imposed for better security. The formal\r\nanalysis of the proposed SSL-SHAF confirmed better resistance against session\r\nkey disclosure, impersonation and stolen smart device attacks. The results of\r\nSSL-SHAF confirmed minimized computational costs and security compared to the\r\nbaseline protocols considered for investigation.\r\n\",\"['K. Swapna Sudha', 'N. Jeyanthi', 'Celestine Iwendi']\"\r\nhttp://arxiv.org/abs/1112.1158v1,Smart contracts,2011-12-06T04:55:32Z,2011-12-06T04:55:32Z,\"Wireless Communications and Networking Technologies for Smart Grid:\r\n  Paradigms and Challenges\",\"  Smart grid, regarded as the next generation power grid, uses two-way flows of\r\nelectricity and information to create a widely distributed automated energy\r\ndelivery network. In this work we present our vision on smart grid from the\r\nperspective of wireless communications and networking technologies. We present\r\nwireless communication and networking paradigms for four typical scenarios in\r\nthe future smart grid and also point out the research challenges of the\r\nwireless communication and networking technologies used in smart grid\r\n\",\"['Xi Fang', 'Dejun Yang', 'Guoliang Xue']\"\r\nhttp://arxiv.org/abs/1807.03111v1,Smart contracts,2018-06-13T10:03:44Z,2018-06-13T10:03:44Z,\"A Framework for Detecting and Translating User Behavior from Smart Meter\r\n  Data\",\"  The European adoption of smart electricity meters triggers the developments\r\nof new value-added service for smart energy and optimal consumption. Recently,\r\nseveral algorithms and tools have been built to analyze smart meter's data.\r\nThis paper introduces an open framework and prototypes for detecting and\r\npresenting user behavior from its smart meter power consumption data. The\r\nframework aims at presenting the detected user behavior in natural language\r\nreports. In order to validate the proposed framework, an experiment has been\r\nperformed and the results have been presented.\r\n\",\"['Egon Kidmose', 'Emad Ebeid', 'Rune Hylsberg Jacobsen']\"\r\nhttp://arxiv.org/abs/2101.06519v1,Smart contracts,2021-01-16T20:46:43Z,2021-01-16T20:46:43Z,\"Intrusion Detection Systems for Smart Home IoT Devices: Experimental\r\n  Comparison Study\",\"  Smart homes are one of the most promising applications of the emerging\r\nInternet of Things (IoT) technology. With the growing number of IoT related\r\ndevices such as smart thermostats, smart fridges, smart speaker, smart light\r\nbulbs and smart locks, smart homes promise to make our lives easier and more\r\ncomfortable. However, the increased deployment of such smart devices brings an\r\nincrease in potential security risks and home privacy breaches. In order to\r\novercome such risks, Intrusion Detection Systems are presented as pertinent\r\ntools that can provide network-level protection for smart devices deployed in\r\nhome environments. These systems monitor the network activities of the smart\r\nhome-connected de-vices and focus on alerting suspicious or malicious activity.\r\nThey also can deal with detected abnormal activities by hindering the impostors\r\nin accessing the victim devices. However, the employment of such systems in the\r\ncontext of a smart home can be challenging due to the devices hardware\r\nlimitations, which may restrict their ability to counter the existing and\r\nemerging attack vectors. Therefore, this paper proposes an experimental\r\ncomparison between the widely used open-source NIDSs namely Snort, Suricata and\r\nBro IDS to find the most appropriate one for smart homes in term of detection\r\naccuracy and resources consumption including CP and memory utilization.\r\nExperimental Results show that Suricata is the best performing NIDS for smart\r\nhomes\r\n\",\"['Faisal Alsakran', 'Gueltoum Bendiab', 'Stavros Shiaeles', 'Nicholas Kolokotronis']\"\r\nhttp://arxiv.org/abs/2304.12851v1,Smart contracts,2023-04-25T14:22:48Z,2023-04-25T14:22:48Z,Towards Smart Education through the Internet of Things: A Review,\"  IoT is a fundamental enabling technology for creating smart spaces, which can\r\nassist the effective face-to-face and online education systems. The transition\r\nto smart education (integrating IoT and AI into the education system) is\r\nappealing, which has a concrete impact on learners' engagement, motivation,\r\nattendance, and deep learning. Traditional education faces many challenges,\r\nincluding administration, pedagogy, assessment, and classroom supervision.\r\nRecent developments in ICT (e.g., IoT, AI and 5G, etc.) have yielded lots of\r\nsmart solutions for various aspects of life; however, smart solutions are not\r\nwell integrated into the education system. In particular, the COVID-19 pandemic\r\nsituation had further emphasized the adoption of new smart solutions in\r\neducation. This study reviews the related studies and addresses the (i)\r\nproblems in the traditional education system with possible solutions, (ii) the\r\ntransition towards smart education, and (iii) research challenges in the\r\ntransition to smart education (i.e, computational and social resistance).\r\nConsidering these studies, smart solutions (e.g., smart pedagogy, smart\r\nassessment, smart classroom, smart administration, etc.) are introduced to the\r\nproblems of the traditional system. This exploratory study opens new trends for\r\nscholars and the market to integrate ICT, IoT, and AI into smart education.\r\n\",\"['Afzal Badshah', 'Anwar Ghani', 'Ali Daud', 'Ateeqa Jalal', 'Muhammad Bilal', 'Jon Crowcroft']\"\r\nhttp://arxiv.org/abs/1109.4474v1,Smart contracts,2011-09-21T04:33:49Z,2011-09-21T04:33:49Z,Smart Grid Information Security (IS) Functional Requirement,\"  It is important to implement safe smart grid environment to enhance people's\r\nlives and livelihoods. This paper provides information on smart grid IS\r\nfunctional requirement by illustrating some discussion points to the sixteen\r\nidentified requirements. This paper introduces the smart grid potential hazards\r\nthat can be referred as a triggering factor to improve the system and security\r\nof the entire grid. The background of smart information infrastructure and the\r\nneeds for smart grid IS is described with the adoption of hermeneutic circle as\r\nmethodology. Grid information technology and security-s session discusses that\r\ngrid provides the chance of a simple and transparent access to different\r\ninformation sources. In addition, the transformation between traditional versus\r\nsmart grid networking trend and the IS importance on the communication field\r\nreflects the criticality of grid IS functional requirement identification is\r\nintroduces. The smart grid IS functional requirements described in this paper\r\nare general and can be adopted or modified to suit any smart grid system. This\r\npaper has tutorial contents where some related backgrounds were provided,\r\nespecially for networking community, covering the cyber security requirement of\r\nsmart grid information infrastructure.\r\n\",\"['Amy Poh Ai Ling', 'Mukaidono Masao']\"\r\nhttp://arxiv.org/abs/1706.07363v1,Smart contracts,2017-06-22T15:19:16Z,2017-06-22T15:19:16Z,Smart Wireless Communication is the Cornerstone of Smart Infrastructures,\"  Emerging smart infrastructures, such as Smart City, Smart Grid, Smart Health,\r\nand Smart Transportation, need smart wireless connectivity. However, the\r\nrequirements of these smart infrastructures cannot be met with today's wireless\r\nnetworks. A new wireless infrastructure is needed to meet unprecedented needs\r\nin terms of agility, reliability, security, scalability, and partnerships.\r\n  We are at the beginning of a revolution in how we live with technology,\r\nresulting from a convergence of machine learning (ML), the Internet-of-Things\r\n(IoT), and robotics. A smart infrastructure monitors and processes a vast\r\namount of data, collected from a dense and wide distribution of heterogeneous\r\nsensors (e.g., the IoT), as well as from web applications like social media. In\r\nreal time, using machine learning, patterns and relationships in the data over\r\nspace, time, and application can be detected and predictions can be made; on\r\nthe basis of these, resources can be managed, decisions can be made, and\r\ndevices can be actuated to optimize metrics, such as cost, health, safety, and\r\nconvenience.\r\n\",\"['Mary Ann Weitnauer', 'Jennifer Rexford', 'Nicholas Laneman', 'Matthieu Bloch', 'Santiago Griljava', 'Catherine Ross', 'Gee-Kung Chang']\"\r\n
===================================================================
diff --git a/data/arxiv.csv b/data/arxiv.csv
--- a/data/arxiv.csv	(revision d23e95cda0a2999cc35eda0b98d90e9f5764f5a3)
+++ b/data/arxiv.csv	(date 1748182763808)
@@ -367,6 +367,7 @@
 for future attempts at this problem. The VR interface, code and datasets are
 available at https://tinyurl.com/3DSketch3DV.
 ","['Ling Luo', 'Yulia Gryaditskaya', 'Yongxin Yang', 'Tao Xiang', 'Yi-Zhe Song']"
+id,technology,published,updated,title,summary,authors
 http://arxiv.org/abs/2005.10488v1,Artificial intelligence,2020-05-21T07:00:31Z,2020-05-21T07:00:31Z,"Does an artificial intelligence perform market manipulation with its own
   discretion? -- A genetic algorithm learns in an artificial market simulation","  Who should be charged with responsibility for an artificial intelligence
 performing market manipulation have been discussed. In this study, I
